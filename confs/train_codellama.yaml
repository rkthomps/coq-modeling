# File Paths
data_path: data/basic-final
model_name: codellama/CodeLlama-7b-hf
output_dir: models/codellama-7b-basic

# Training Args
max_seq_len: 512
max_input_len: 224 
max_num_passages: 2
max_tokens_per_passage: 112
per_device_train_batch_size: 4
learning_rate: 1.0e-3
num_train_epochs: 2
peft_lora_r: 64
peft_lora_alpha: 16
#deepspeed: /home/kthompson/coq-modeling/deepspeed.json

# Evaluation Args
eval_steps: 500 
save_steps: 500
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 10
save_total_limit: 2

# Train from checkpoint
#checkpoint_name: "models/codellama-7b-whole-proof-ret/checkpoint-4500"
