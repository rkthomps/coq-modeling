{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/coq-modeling/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Optional\n",
    "from tactic_gen.lm_example import LmExample\n",
    "from tactic_gen.train_fid import get_model, get_tokenizer, get_datasets\n",
    "from tactic_gen.fid_data import FidDataset\n",
    "from tactic_gen.fid_prime_model import FiDT5 \n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer, CodeLlamaTokenizer\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiDT5(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): EncoderWrapper(\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 8)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (encoder_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "t5.config.n_passages = 8\n",
    "fid_model = FiDT5(t5.config)\n",
    "fid_model.load_t5(t5.state_dict())\n",
    "fid_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = LmExample(\"hi\", \"there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FidDataset(None, tokenizer, 448, 64, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = dataset.collate([test_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 448])\n",
      "shape before view torch.Size([1, 448, 8, 512])\n",
      "proj shape torch.Size([512, 4096])\n",
      "state shape torch.Size([1, 448, 4096])\n",
      "torch.Size([1, 448, 512])\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[0.0368, 0.0000, 0.0193,  ..., 0.0738, 0.0000, 0.0000],\n",
      "         [0.0659, 0.0367, 0.1574,  ..., 0.0651, 0.0000, 0.0000],\n",
      "         [0.0447, 0.0423, 0.1159,  ..., 0.0229, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0986, 0.0945, 0.1009,  ..., 0.0735, 0.0000, 0.0013],\n",
      "         [0.1088, 0.0517, 0.0000,  ..., 0.0878, 0.0000, 0.0000],\n",
      "         [0.1229, 0.0000, 0.1172,  ..., 0.0753, 0.0000, 0.0000]]],\n",
      "       device='cuda:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (56) must match the size of tensor b (448) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/src/tactic_gen/fid_prime_model.py:69\u001b[0m, in \u001b[0;36mFiDT5.generate\u001b[0;34m(self, input_ids, attention_mask, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m n_passages, seq_len)\n\u001b[1;32m     68\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m n_passages, seq_len)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2521\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2521\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/src/tactic_gen/fid_prime_model.py:59\u001b[0m, in \u001b[0;36mFiDT5.forward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         bsz, n_passages, seq_len \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     58\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m n_passages, seq_len)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m--> 560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[1;32m    561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (56) must match the size of tensor b (448) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "outputs = fid_model.generate(\n",
    "                    test_batch[\"input_ids\"].cuda(),\n",
    "                    test_batch[\"attention_mask\"].cuda(),\n",
    "                    64,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig shape torch.Size([1, 8, 448])\n",
      "reshaping torch.Size([8, 448])\n",
      "torch.Size([8, 448])\n",
      "shape before view torch.Size([1, 448, 8, 512])\n",
      "proj shape torch.Size([512, 4096])\n",
      "state shape torch.Size([1, 448, 4096])\n",
      "torch.Size([1, 448, 512])\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0320, 0.0668],\n",
      "         [0.0000, 0.0021, 0.0468,  ..., 0.0222, 0.0287, 0.0000],\n",
      "         [0.1294, 0.0965, 0.0000,  ..., 0.0213, 0.0249, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0864, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2473, 0.0881, 0.0372,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2165, 0.0441, 0.0000,  ..., 0.0097, 0.0000, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 8, 64, 448] doesn't match the broadcast shape [8, 8, 64, 448]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfid_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/src/tactic_gen/fid_prime_model.py:59\u001b[0m, in \u001b[0;36mFiDT5.forward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         bsz, n_passages, seq_len \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     58\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m n_passages, seq_len)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m--> 560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[1;32m    561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 8, 64, 448] doesn't match the broadcast shape [8, 8, 64, 448]"
     ]
    }
   ],
   "source": [
    "outputs = fid_model(\n",
    "                    test_batch[\"input_ids\"].cuda(),\n",
    "                    test_batch[\"attention_mask\"].cuda(),\n",
    "                    test_batch[\"labels\"].cuda(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(1.5929, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-54.2444, -31.2982, -34.3318,  ..., -68.7229, -69.2365, -69.2920],\n",
       "         [-57.3117, -29.2449, -21.7290,  ..., -69.4969, -69.8299, -69.8837],\n",
       "         [-57.6321, -29.4005, -24.8219,  ..., -68.5074, -68.8125, -68.7309],\n",
       "         ...,\n",
       "         [-55.4589, -30.7832, -35.0631,  ..., -70.0534, -70.5770, -70.6369],\n",
       "         [-55.4020, -30.7915, -35.0198,  ..., -69.9774, -70.5003, -70.5602],\n",
       "         [-55.3844, -30.8054, -34.9945,  ..., -69.9370, -70.4597, -70.5195]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-3.4396e-01, -1.7363e+00, -7.6735e-01,  ..., -1.8445e+00,\n",
       "           -1.2066e+00, -2.9358e-01],\n",
       "          [-3.8958e-01,  5.2143e-01, -3.1301e-01,  ..., -1.0632e+00,\n",
       "            7.7356e-01, -1.0901e+00],\n",
       "          [ 7.4578e-01,  4.7222e-01,  1.0388e+00,  ...,  2.3293e-01,\n",
       "           -1.3219e+00, -1.1404e+00],\n",
       "          ...,\n",
       "          [-3.4396e-01, -1.7363e+00, -7.6735e-01,  ..., -1.8445e+00,\n",
       "           -1.2066e+00, -2.9358e-01],\n",
       "          [-3.4396e-01, -1.7363e+00, -7.6735e-01,  ..., -1.8445e+00,\n",
       "           -1.2066e+00, -2.9358e-01],\n",
       "          [-3.4396e-01, -1.7363e+00, -7.6735e-01,  ..., -1.8445e+00,\n",
       "           -1.2066e+00, -2.9358e-01]],\n",
       "\n",
       "         [[-6.8959e-01, -1.7366e+00, -1.5478e-01,  ...,  7.9841e-01,\n",
       "            4.9856e-01,  3.9638e-01],\n",
       "          [-4.1962e-01, -8.3590e-01, -2.2803e+00,  ..., -1.0977e+00,\n",
       "            1.7350e+00,  1.3602e+00],\n",
       "          [ 9.2895e-01, -5.2099e-01, -2.5260e-02,  ...,  4.3533e-01,\n",
       "            2.4255e-01,  3.7554e-02],\n",
       "          ...,\n",
       "          [-6.8959e-01, -1.7366e+00, -1.5478e-01,  ...,  7.9841e-01,\n",
       "            4.9856e-01,  3.9638e-01],\n",
       "          [-6.8959e-01, -1.7366e+00, -1.5478e-01,  ...,  7.9841e-01,\n",
       "            4.9856e-01,  3.9638e-01],\n",
       "          [-6.8959e-01, -1.7366e+00, -1.5478e-01,  ...,  7.9841e-01,\n",
       "            4.9856e-01,  3.9638e-01]],\n",
       "\n",
       "         [[ 8.1598e-01, -1.7342e+00,  4.7446e-01,  ..., -8.6764e-01,\n",
       "            2.6179e-01,  4.6825e-01],\n",
       "          [-3.7167e-01,  3.4860e-01, -6.1055e-01,  ...,  2.9404e-01,\n",
       "           -1.9165e-01,  4.8307e-02],\n",
       "          [ 1.0661e+00,  3.0124e-03, -1.5016e+00,  ...,  2.1615e+00,\n",
       "           -1.6281e-01,  1.1740e+00],\n",
       "          ...,\n",
       "          [ 8.1598e-01, -1.7342e+00,  4.7446e-01,  ..., -8.6764e-01,\n",
       "            2.6179e-01,  4.6825e-01],\n",
       "          [ 8.1598e-01, -1.7342e+00,  4.7446e-01,  ..., -8.6764e-01,\n",
       "            2.6179e-01,  4.6825e-01],\n",
       "          [ 8.1598e-01, -1.7342e+00,  4.7446e-01,  ..., -8.6764e-01,\n",
       "            2.6179e-01,  4.6825e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.2530e-01,  1.6227e+00,  2.3211e-01,  ...,  9.7718e-01,\n",
       "           -2.2502e+00, -8.9343e-01],\n",
       "          [-1.3295e+00,  5.7928e-01, -5.2452e-02,  ...,  8.7634e-01,\n",
       "            1.1796e-01,  9.4124e-01],\n",
       "          [ 2.3413e-01,  1.7796e+00, -5.6645e-01,  ...,  6.8412e-01,\n",
       "            2.8203e-01,  2.4032e+00],\n",
       "          ...,\n",
       "          [-5.2530e-01,  1.6227e+00,  2.3211e-01,  ...,  9.7718e-01,\n",
       "           -2.2502e+00, -8.9343e-01],\n",
       "          [-5.2530e-01,  1.6227e+00,  2.3211e-01,  ...,  9.7718e-01,\n",
       "           -2.2502e+00, -8.9343e-01],\n",
       "          [-5.2530e-01,  1.6227e+00,  2.3211e-01,  ...,  9.7718e-01,\n",
       "           -2.2502e+00, -8.9343e-01]],\n",
       "\n",
       "         [[-4.4023e+00, -9.2770e-01, -1.2116e+00,  ..., -8.7074e-01,\n",
       "           -1.1427e-01,  1.3266e+00],\n",
       "          [ 1.0860e+00, -4.1964e-01, -7.7362e-01,  ..., -4.7265e-01,\n",
       "           -3.8159e-02, -1.2308e-01],\n",
       "          [ 5.3402e-01, -5.0030e-01, -4.7041e-01,  ...,  1.9157e-01,\n",
       "           -1.4961e-01,  4.8875e-01],\n",
       "          ...,\n",
       "          [-4.4023e+00, -9.2770e-01, -1.2116e+00,  ..., -8.7074e-01,\n",
       "           -1.1427e-01,  1.3266e+00],\n",
       "          [-4.4023e+00, -9.2770e-01, -1.2116e+00,  ..., -8.7074e-01,\n",
       "           -1.1427e-01,  1.3266e+00],\n",
       "          [-4.4023e+00, -9.2770e-01, -1.2116e+00,  ..., -8.7074e-01,\n",
       "           -1.1427e-01,  1.3266e+00]],\n",
       "\n",
       "         [[ 1.0371e-01,  6.0698e-02, -5.7466e-02,  ...,  5.9837e-01,\n",
       "           -1.3986e-01, -2.7639e-01],\n",
       "          [ 4.0517e-02,  6.3054e-01,  1.4681e-01,  ..., -1.8684e-01,\n",
       "            2.8101e-01,  3.2981e-01],\n",
       "          [ 9.5114e-02,  1.0244e+00,  1.4824e+00,  ..., -3.2022e-01,\n",
       "           -4.7784e-01, -1.9858e-01],\n",
       "          ...,\n",
       "          [ 1.0371e-01,  6.0698e-02, -5.7466e-02,  ...,  5.9837e-01,\n",
       "           -1.3986e-01, -2.7639e-01],\n",
       "          [ 1.0371e-01,  6.0698e-02, -5.7466e-02,  ...,  5.9837e-01,\n",
       "           -1.3986e-01, -2.7639e-01],\n",
       "          [ 1.0371e-01,  6.0698e-02, -5.7466e-02,  ...,  5.9837e-01,\n",
       "           -1.3986e-01, -2.7639e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.2420,  0.3202, -0.1908,  ...,  0.5061,  0.3611,  0.1487],\n",
       "          [-0.0190, -1.6247, -0.7873,  ..., -0.4696, -0.7734, -1.0287],\n",
       "          [-0.1368,  0.7169,  0.2310,  ..., -0.7658, -0.2438,  0.8381],\n",
       "          ...,\n",
       "          [ 0.2420,  0.3202, -0.1908,  ...,  0.5061,  0.3611,  0.1487],\n",
       "          [ 0.2420,  0.3202, -0.1908,  ...,  0.5061,  0.3611,  0.1487],\n",
       "          [ 0.2420,  0.3202, -0.1908,  ...,  0.5061,  0.3611,  0.1487]],\n",
       "\n",
       "         [[-0.1162,  0.6529, -0.4111,  ..., -0.0721, -0.0051, -0.2105],\n",
       "          [-0.6081, -1.7334,  1.4189,  ...,  0.8316,  2.6635, -1.1717],\n",
       "          [ 0.5906, -0.4185, -0.3417,  ..., -0.4336, -0.3311, -0.3333],\n",
       "          ...,\n",
       "          [-0.1162,  0.6529, -0.4111,  ..., -0.0721, -0.0051, -0.2105],\n",
       "          [-0.1162,  0.6529, -0.4111,  ..., -0.0721, -0.0051, -0.2105],\n",
       "          [-0.1162,  0.6529, -0.4111,  ..., -0.0721, -0.0051, -0.2105]],\n",
       "\n",
       "         [[-0.0257,  0.1164, -0.0520,  ..., -0.2275, -0.8600, -0.0494],\n",
       "          [ 0.3898,  0.0775, -0.4481,  ...,  0.3606, -1.2184, -0.5856],\n",
       "          [ 0.0253,  0.4046,  0.2349,  ...,  0.2693,  0.0983,  0.1573],\n",
       "          ...,\n",
       "          [-0.0257,  0.1164, -0.0520,  ..., -0.2275, -0.8600, -0.0494],\n",
       "          [-0.0257,  0.1164, -0.0520,  ..., -0.2275, -0.8600, -0.0494],\n",
       "          [-0.0257,  0.1164, -0.0520,  ..., -0.2275, -0.8600, -0.0494]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2647, -0.1674, -0.0331,  ..., -0.1900,  0.0695,  0.0880],\n",
       "          [ 1.0055, -1.7179,  0.0519,  ..., -1.4271,  0.2011, -0.7969],\n",
       "          [-1.1102,  0.8096,  0.2592,  ..., -0.6442,  1.6448, -0.0464],\n",
       "          ...,\n",
       "          [ 0.2647, -0.1674, -0.0331,  ..., -0.1900,  0.0695,  0.0880],\n",
       "          [ 0.2647, -0.1674, -0.0331,  ..., -0.1900,  0.0695,  0.0880],\n",
       "          [ 0.2647, -0.1674, -0.0331,  ..., -0.1900,  0.0695,  0.0880]],\n",
       "\n",
       "         [[-0.1654, -0.0780, -0.0207,  ...,  0.0890, -0.0396, -0.1227],\n",
       "          [ 0.3390,  0.5832,  0.6511,  ...,  0.4995, -0.7829,  0.7094],\n",
       "          [ 2.3622,  0.9666,  1.5835,  ..., -1.2890, -0.6603, -0.6708],\n",
       "          ...,\n",
       "          [-0.1654, -0.0780, -0.0207,  ...,  0.0890, -0.0396, -0.1227],\n",
       "          [-0.1654, -0.0780, -0.0207,  ...,  0.0890, -0.0396, -0.1227],\n",
       "          [-0.1654, -0.0780, -0.0207,  ...,  0.0890, -0.0396, -0.1227]],\n",
       "\n",
       "         [[-0.0603, -0.6576,  0.2875,  ...,  0.1888, -0.1296,  0.0929],\n",
       "          [ 1.6775,  2.6168,  0.1932,  ...,  0.5408,  0.9790, -0.7238],\n",
       "          [-0.1754, -0.9108, -0.8495,  ..., -0.3042,  0.7591, -1.2195],\n",
       "          ...,\n",
       "          [-0.0603, -0.6576,  0.2875,  ...,  0.1888, -0.1296,  0.0929],\n",
       "          [-0.0603, -0.6576,  0.2875,  ...,  0.1888, -0.1296,  0.0929],\n",
       "          [-0.0603, -0.6576,  0.2875,  ...,  0.1888, -0.1296,  0.0929]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 0.8387, -0.8197, -1.3434,  ..., -0.6468, -1.0808, -0.3033],\n",
       "          [ 0.7306,  0.3908, -2.1140,  ..., -1.6813,  0.0727,  2.0226],\n",
       "          [ 0.8220, -0.1168,  0.4073,  ..., -0.3476,  0.8951,  0.1827],\n",
       "          ...,\n",
       "          [ 1.0383, -1.7601, -1.1946,  ..., -2.1230,  2.3487, -2.1171],\n",
       "          [ 1.0383, -1.7601, -1.1946,  ..., -2.1230,  2.3487, -2.1171],\n",
       "          [ 1.0383, -1.7601, -1.1946,  ..., -2.1230,  2.3487, -2.1171]],\n",
       "\n",
       "         [[-0.4099, -0.5728, -0.8261,  ..., -3.3370, -0.5792,  0.5685],\n",
       "          [ 2.1259, -1.5045,  1.1722,  ..., -2.6375,  1.3952, -0.1588],\n",
       "          [-1.8606,  2.0334,  1.2015,  ..., -2.5936,  1.0559, -0.8789],\n",
       "          ...,\n",
       "          [-1.3407,  0.2332,  3.1186,  ..., -1.8877,  2.4868, -0.0387],\n",
       "          [-1.3407,  0.2332,  3.1186,  ..., -1.8877,  2.4868, -0.0387],\n",
       "          [-1.3407,  0.2332,  3.1186,  ..., -1.8877,  2.4868, -0.0387]],\n",
       "\n",
       "         [[ 0.0387,  0.0331, -0.5611,  ..., -1.1306, -1.1089, -1.4028],\n",
       "          [ 1.7213,  1.2631, -0.1963,  ..., -0.3078,  0.7113, -1.4205],\n",
       "          [ 0.7545,  0.1778, -1.0974,  ..., -0.5338,  1.0542, -0.2381],\n",
       "          ...,\n",
       "          [-0.0987, -0.2540, -0.2204,  ..., -0.4527, -0.7543, -0.2000],\n",
       "          [-0.0987, -0.2540, -0.2204,  ..., -0.4527, -0.7543, -0.2000],\n",
       "          [-0.0987, -0.2540, -0.2204,  ..., -0.4527, -0.7543, -0.2000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0706,  0.8719,  0.5168,  ...,  0.5086,  0.7364, -1.1845],\n",
       "          [-1.5334, -0.2587,  0.4809,  ...,  0.3607,  0.6861, -0.4272],\n",
       "          [ 0.4416,  0.7397, -1.2170,  ...,  1.6861,  0.4094, -0.1240],\n",
       "          ...,\n",
       "          [-0.8794,  0.6067,  0.0968,  ...,  0.9863, -0.8503, -1.0490],\n",
       "          [-0.8794,  0.6067,  0.0968,  ...,  0.9863, -0.8503, -1.0490],\n",
       "          [-0.8794,  0.6067,  0.0968,  ...,  0.9863, -0.8503, -1.0490]],\n",
       "\n",
       "         [[ 0.7137, -0.0820,  0.7059,  ...,  0.7059,  0.9205,  0.0648],\n",
       "          [ 0.9367,  0.5884,  0.2169,  ...,  0.3858, -0.3820,  1.3659],\n",
       "          [-0.7341,  1.2071,  0.1599,  ...,  0.5845,  0.2145,  1.0616],\n",
       "          ...,\n",
       "          [ 1.8829,  1.1042, -0.2431,  ...,  1.3962, -0.4978,  1.3311],\n",
       "          [ 1.8829,  1.1042, -0.2431,  ...,  1.3962, -0.4978,  1.3311],\n",
       "          [ 1.8829,  1.1042, -0.2431,  ...,  1.3962, -0.4978,  1.3311]],\n",
       "\n",
       "         [[ 0.1714, -0.9837, -0.2553,  ..., -0.8904, -0.7983, -1.7839],\n",
       "          [-0.8470,  0.7840,  0.0823,  ...,  1.9844,  1.7884, -0.8803],\n",
       "          [ 0.0507, -0.6813,  0.4086,  ...,  0.2790, -0.3062, -1.3561],\n",
       "          ...,\n",
       "          [ 0.4908,  0.2036, -1.5690,  ...,  1.5875,  2.5462,  0.6180],\n",
       "          [ 0.4908,  0.2036, -1.5690,  ...,  1.5875,  2.5462,  0.6180],\n",
       "          [ 0.4908,  0.2036, -1.5690,  ...,  1.5875,  2.5462,  0.6180]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 1.1419,  2.2394,  2.5504,  ..., -2.0076, -1.2087,  0.4902],\n",
       "          [-1.8974, -1.0678, -0.1778,  ..., -1.5800, -0.9251,  2.5156],\n",
       "          [ 0.0415, -0.7664, -1.1827,  ..., -0.7680, -0.5445,  0.5465],\n",
       "          ...,\n",
       "          [ 2.4420,  0.3731,  1.4927,  ..., -1.0600, -1.7765,  1.4960],\n",
       "          [ 2.4420,  0.3731,  1.4927,  ..., -1.0600, -1.7765,  1.4960],\n",
       "          [ 2.4420,  0.3731,  1.4927,  ..., -1.0600, -1.7765,  1.4960]],\n",
       "\n",
       "         [[ 0.5583,  3.0471,  0.2971,  ...,  0.0644, -2.2395, -2.7517],\n",
       "          [ 1.7048,  1.6972,  1.0587,  ..., -1.8221,  1.0523,  0.4533],\n",
       "          [ 1.3861,  0.5480,  0.7634,  ...,  0.2962,  3.3576, -0.3415],\n",
       "          ...,\n",
       "          [-2.0745, -2.5771,  0.6548,  ...,  2.1898,  0.8062,  1.1303],\n",
       "          [-2.0745, -2.5771,  0.6548,  ...,  2.1898,  0.8062,  1.1303],\n",
       "          [-2.0745, -2.5771,  0.6548,  ...,  2.1898,  0.8062,  1.1303]],\n",
       "\n",
       "         [[ 1.0455,  0.7474,  3.0835,  ..., -1.7908,  0.0750, -0.2776],\n",
       "          [-0.7635,  2.7696,  1.2680,  ..., -0.3398, -4.4265,  0.2384],\n",
       "          [-0.8491,  1.0057,  4.3393,  ...,  0.9932, -4.7707, -1.6566],\n",
       "          ...,\n",
       "          [ 0.1977,  1.6798, -1.3327,  ..., -1.7262, -2.0461, -0.7529],\n",
       "          [ 0.1977,  1.6798, -1.3327,  ..., -1.7262, -2.0461, -0.7529],\n",
       "          [ 0.1977,  1.6798, -1.3327,  ..., -1.7262, -2.0461, -0.7529]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9727, -2.4795, -0.8617,  ...,  1.0892,  0.8654,  0.8822],\n",
       "          [-0.2058,  0.2323,  0.6821,  ...,  0.5311,  1.8130, -0.3257],\n",
       "          [ 0.1693,  1.6251, -0.7402,  ...,  2.0736,  0.6478, -1.8328],\n",
       "          ...,\n",
       "          [-0.9220,  0.8834,  1.8561,  ...,  1.5761,  0.8780, -0.6925],\n",
       "          [-0.9220,  0.8834,  1.8561,  ...,  1.5761,  0.8780, -0.6925],\n",
       "          [-0.9220,  0.8834,  1.8561,  ...,  1.5761,  0.8780, -0.6925]],\n",
       "\n",
       "         [[ 1.6130,  0.6624,  1.1119,  ..., -1.4764, -0.4415,  1.3660],\n",
       "          [-3.9230, -1.0101,  0.6256,  ...,  0.1955,  3.2721,  0.0705],\n",
       "          [ 0.2902,  1.8308,  0.7384,  ...,  0.4442, -0.5367, -0.2796],\n",
       "          ...,\n",
       "          [-0.3298,  1.7643,  1.2210,  ...,  2.8232,  0.3288,  0.0630],\n",
       "          [-0.3298,  1.7643,  1.2210,  ...,  2.8232,  0.3288,  0.0630],\n",
       "          [-0.3298,  1.7643,  1.2210,  ...,  2.8232,  0.3288,  0.0630]],\n",
       "\n",
       "         [[ 2.6232, -3.6504,  1.0336,  ..., -2.5178,  1.4067, -0.3872],\n",
       "          [-1.6161,  0.2560,  0.7733,  ..., -2.5539,  1.3491, -0.4423],\n",
       "          [-0.6440,  4.0680,  0.7698,  ..., -0.0313, -1.9767,  1.3475],\n",
       "          ...,\n",
       "          [-0.3536,  0.3694,  0.5065,  ...,  0.2553,  2.0433, -0.7614],\n",
       "          [-0.3536,  0.3694,  0.5065,  ...,  0.2553,  2.0433, -0.7614],\n",
       "          [-0.3536,  0.3694,  0.5065,  ...,  0.2553,  2.0433, -0.7614]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.6452,  0.5477,  1.1199,  ...,  0.6010, -0.6019,  0.9516],\n",
       "          [ 0.1352, -0.1480, -0.5817,  ...,  0.3230, -0.3360, -0.3410],\n",
       "          [ 1.1246,  0.1329,  0.4634,  ..., -0.7204, -0.5790,  0.3450],\n",
       "          ...,\n",
       "          [ 0.6439,  0.5510,  1.1276,  ...,  0.6045, -0.5933,  0.9528],\n",
       "          [ 0.6437,  0.5508,  1.1273,  ...,  0.6046, -0.5938,  0.9526],\n",
       "          [ 0.6436,  0.5508,  1.1271,  ...,  0.6047, -0.5943,  0.9525]],\n",
       "\n",
       "         [[-0.5265, -0.5862, -2.3141,  ...,  1.7814,  0.3989, -0.0985],\n",
       "          [-0.3372, -0.1834,  0.2083,  ..., -0.4759, -0.0105,  0.1264],\n",
       "          [ 0.1546, -0.1966,  0.8806,  ..., -1.1981, -0.0556, -0.0426],\n",
       "          ...,\n",
       "          [-0.5264, -0.5868, -2.3319,  ...,  1.7922,  0.3992, -0.1038],\n",
       "          [-0.5266, -0.5867, -2.3309,  ...,  1.7916,  0.3988, -0.1037],\n",
       "          [-0.5267, -0.5867, -2.3297,  ...,  1.7911,  0.3985, -0.1034]],\n",
       "\n",
       "         [[-1.6360, -0.3451, -0.2417,  ..., -0.3382,  0.5827,  0.8689],\n",
       "          [-0.6541,  0.9764,  0.3249,  ..., -0.6929,  0.4336,  0.3866],\n",
       "          [ 0.0257,  1.4185, -0.1876,  ...,  0.8480,  0.3190,  0.1604],\n",
       "          ...,\n",
       "          [-1.6274, -0.3291, -0.2375,  ..., -0.3478,  0.5975,  0.8801],\n",
       "          [-1.6283, -0.3303, -0.2377,  ..., -0.3468,  0.5969,  0.8792],\n",
       "          [-1.6291, -0.3316, -0.2377,  ..., -0.3458,  0.5961,  0.8783]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5691,  0.2191, -0.1901,  ..., -0.2319,  0.8924, -0.6757],\n",
       "          [ 0.7886, -1.2040,  0.2100,  ..., -0.0559, -0.4360, -0.6725],\n",
       "          [ 1.1245,  0.0355,  1.7837,  ..., -1.1579,  0.5097, -0.1220],\n",
       "          ...,\n",
       "          [ 0.5621,  0.2275, -0.1881,  ..., -0.2485,  0.8771, -0.6842],\n",
       "          [ 0.5627,  0.2269, -0.1882,  ..., -0.2475,  0.8782, -0.6836],\n",
       "          [ 0.5637,  0.2264, -0.1884,  ..., -0.2463,  0.8795, -0.6832]],\n",
       "\n",
       "         [[-0.2018,  0.8769, -0.2997,  ..., -0.3374, -0.0064,  0.0930],\n",
       "          [ 0.3159, -0.0185, -1.0216,  ..., -0.6688, -0.4846,  0.5775],\n",
       "          [-0.8394,  0.8505, -0.7666,  ..., -0.5913,  1.2865,  0.2459],\n",
       "          ...,\n",
       "          [-0.2046,  0.8906, -0.3052,  ..., -0.3494,  0.0090,  0.0825],\n",
       "          [-0.2042,  0.8900, -0.3049,  ..., -0.3488,  0.0079,  0.0830],\n",
       "          [-0.2040,  0.8894, -0.3046,  ..., -0.3481,  0.0067,  0.0835]],\n",
       "\n",
       "         [[ 0.1580, -0.2007,  0.0425,  ..., -0.2940,  0.1667, -0.0650],\n",
       "          [ 0.2680, -0.2080, -0.0282,  ..., -0.1885,  0.0581, -0.3057],\n",
       "          [ 0.0192,  0.5241,  1.5886,  ..., -0.3700,  0.0607, -0.1393],\n",
       "          ...,\n",
       "          [ 0.1472, -0.1922,  0.0289,  ..., -0.3009,  0.1568, -0.0539],\n",
       "          [ 0.1478, -0.1926,  0.0295,  ..., -0.3004,  0.1573, -0.0542],\n",
       "          [ 0.1486, -0.1933,  0.0304,  ..., -0.3001,  0.1578, -0.0544]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 1.6009e-01,  1.6030e-01, -1.6402e-01,  ..., -6.7273e-02,\n",
       "           -9.5205e-02, -7.7403e-02],\n",
       "          [ 1.0201e+00,  8.4407e-01, -1.2281e+00,  ..., -1.0614e+00,\n",
       "            8.4964e-01, -5.7193e-01],\n",
       "          [-5.1208e-02, -1.2078e+00, -3.6430e-01,  ..., -1.7117e+00,\n",
       "            7.7508e-01, -1.8924e+00],\n",
       "          ...,\n",
       "          [ 1.6461e-01,  1.5722e-01, -1.6423e-01,  ..., -7.3516e-02,\n",
       "           -9.0114e-02, -6.9736e-02],\n",
       "          [ 1.6428e-01,  1.5723e-01, -1.6439e-01,  ..., -7.3013e-02,\n",
       "           -9.0377e-02, -6.9888e-02],\n",
       "          [ 1.6392e-01,  1.5790e-01, -1.6410e-01,  ..., -7.2302e-02,\n",
       "           -9.0125e-02, -7.0027e-02]],\n",
       "\n",
       "         [[-7.0532e-01, -4.1690e-01,  4.6775e-01,  ..., -1.1500e-01,\n",
       "            1.6956e-01, -7.0884e-01],\n",
       "          [-1.5148e-01,  9.0890e-01, -3.3528e-01,  ...,  3.1477e-01,\n",
       "            3.1195e-02, -2.2305e-01],\n",
       "          [ 7.5677e-01,  2.9153e-01,  1.7133e-01,  ...,  1.1585e-01,\n",
       "            7.0953e-01,  9.2971e-02],\n",
       "          ...,\n",
       "          [-6.9851e-01, -4.3575e-01,  4.6662e-01,  ..., -1.1926e-01,\n",
       "            1.7230e-01, -7.1058e-01],\n",
       "          [-6.9869e-01, -4.3426e-01,  4.6669e-01,  ..., -1.1869e-01,\n",
       "            1.7208e-01, -7.1009e-01],\n",
       "          [-6.9937e-01, -4.3212e-01,  4.6646e-01,  ..., -1.1855e-01,\n",
       "            1.7154e-01, -7.1011e-01]],\n",
       "\n",
       "         [[ 1.6723e-01,  6.4501e-02,  6.3455e-02,  ...,  1.2975e-01,\n",
       "           -8.8034e-03, -3.2251e-02],\n",
       "          [ 1.0370e+00,  6.7880e-01,  3.9121e-01,  ..., -1.3569e+00,\n",
       "            4.7313e-01, -1.7590e-01],\n",
       "          [ 4.8634e-01, -1.5081e+00,  2.4142e-01,  ..., -3.1261e-01,\n",
       "           -1.1192e+00,  7.7273e-01],\n",
       "          ...,\n",
       "          [ 1.5841e-01,  6.1262e-02,  5.5384e-02,  ...,  1.4037e-01,\n",
       "            1.0864e-03, -2.4940e-02],\n",
       "          [ 1.5875e-01,  6.0959e-02,  5.5649e-02,  ...,  1.4001e-01,\n",
       "            4.9390e-04, -2.5639e-02],\n",
       "          [ 1.5898e-01,  6.0684e-02,  5.5947e-02,  ...,  1.3939e-01,\n",
       "           -8.7988e-05, -2.6047e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3705e-01,  2.5584e-02, -6.9769e-02,  ...,  2.7804e-01,\n",
       "            1.3078e-01,  1.6178e-01],\n",
       "          [ 7.0244e-01,  5.7540e-01, -4.7123e-01,  ...,  6.7561e-01,\n",
       "           -3.6620e-01, -6.6593e-01],\n",
       "          [-8.8795e-01, -1.4225e+00,  4.0808e-01,  ..., -5.4282e-01,\n",
       "           -1.5338e+00,  1.1410e-01],\n",
       "          ...,\n",
       "          [ 1.3381e-01,  3.0186e-02, -6.5669e-02,  ...,  2.7961e-01,\n",
       "            1.3048e-01,  1.5678e-01],\n",
       "          [ 1.3398e-01,  3.0067e-02, -6.5854e-02,  ...,  2.7965e-01,\n",
       "            1.3050e-01,  1.5700e-01],\n",
       "          [ 1.3417e-01,  2.9992e-02, -6.6361e-02,  ...,  2.7977e-01,\n",
       "            1.3049e-01,  1.5719e-01]],\n",
       "\n",
       "         [[-6.0305e-02,  9.7010e-03,  7.6238e-03,  ...,  9.6321e-03,\n",
       "           -1.2780e-01, -2.3119e-01],\n",
       "          [-8.9508e-01,  7.5874e-01,  7.7308e-01,  ...,  2.7007e-01,\n",
       "           -2.4325e-01, -1.1757e+00],\n",
       "          [-2.8625e-01, -2.7510e+00,  4.5525e+00,  ...,  1.8596e+00,\n",
       "            6.3740e-01,  4.1612e+00],\n",
       "          ...,\n",
       "          [-6.2879e-02,  5.0378e-03,  8.4599e-03,  ...,  3.4854e-03,\n",
       "           -1.2620e-01, -2.3141e-01],\n",
       "          [-6.2869e-02,  5.3461e-03,  8.6513e-03,  ...,  3.6384e-03,\n",
       "           -1.2640e-01, -2.3142e-01],\n",
       "          [-6.2753e-02,  5.6540e-03,  8.8493e-03,  ...,  3.9895e-03,\n",
       "           -1.2661e-01, -2.3134e-01]],\n",
       "\n",
       "         [[ 1.0315e-01, -9.7178e-03,  2.1980e-01,  ..., -3.9204e-01,\n",
       "           -2.3050e-01,  1.1320e-01],\n",
       "          [-2.4595e-01,  3.0638e-01, -5.6280e-02,  ..., -1.3904e+00,\n",
       "           -8.4080e-01,  1.8055e-01],\n",
       "          [-1.4111e+00, -7.7706e-01,  4.4571e-01,  ...,  1.3763e+00,\n",
       "            1.0909e+00,  9.3889e-02],\n",
       "          ...,\n",
       "          [ 1.1030e-01, -7.0106e-03,  2.2546e-01,  ..., -3.9304e-01,\n",
       "           -2.1974e-01,  1.2644e-01],\n",
       "          [ 1.1035e-01, -6.8525e-03,  2.2517e-01,  ..., -3.9314e-01,\n",
       "           -2.1995e-01,  1.2614e-01],\n",
       "          [ 1.0976e-01, -6.5374e-03,  2.2494e-01,  ..., -3.9253e-01,\n",
       "           -2.2073e-01,  1.2513e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.2186, -0.9794, -0.3502,  ...,  1.5283,  2.0895,  0.4455],\n",
       "          [ 1.1791,  0.3502, -0.8746,  ...,  0.2314, -0.0800,  0.9733],\n",
       "          [ 0.1225, -0.1998, -0.6057,  ..., -1.2276, -0.1649, -1.1064],\n",
       "          ...,\n",
       "          [ 1.6535,  0.0174,  0.4297,  ...,  0.7344,  0.8510,  0.4003],\n",
       "          [ 1.6535,  0.0174,  0.4297,  ...,  0.7344,  0.8510,  0.4003],\n",
       "          [ 1.6535,  0.0174,  0.4297,  ...,  0.7344,  0.8510,  0.4003]],\n",
       "\n",
       "         [[ 0.9978, -1.0692, -0.1059,  ...,  1.4841, -1.0340, -0.6098],\n",
       "          [ 0.4144, -0.3176, -0.6844,  ..., -0.7274,  1.2186, -0.3728],\n",
       "          [-1.3042,  0.1779,  0.4536,  ...,  0.6548,  0.8710, -0.7366],\n",
       "          ...,\n",
       "          [ 0.0108,  0.8185,  0.4476,  ..., -0.0865,  1.5459, -2.0623],\n",
       "          [ 0.0108,  0.8185,  0.4476,  ..., -0.0865,  1.5459, -2.0623],\n",
       "          [ 0.0108,  0.8185,  0.4476,  ..., -0.0865,  1.5459, -2.0623]],\n",
       "\n",
       "         [[ 1.0467,  1.5441, -1.3410,  ..., -0.4684,  0.0809,  0.4659],\n",
       "          [ 0.9981,  0.2220,  0.9031,  ..., -0.5960,  1.0995, -0.5380],\n",
       "          [ 0.0408,  0.5346,  0.6803,  ..., -0.2311,  1.7244,  0.4549],\n",
       "          ...,\n",
       "          [-0.3548,  0.5485, -1.4936,  ...,  1.4400,  0.5113,  0.8755],\n",
       "          [-0.3548,  0.5485, -1.4936,  ...,  1.4400,  0.5113,  0.8755],\n",
       "          [-0.3548,  0.5485, -1.4936,  ...,  1.4400,  0.5113,  0.8755]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7550,  0.7924, -0.7425,  ...,  0.4894, -1.7704, -1.7516],\n",
       "          [ 1.2816, -2.0594, -0.0893,  ...,  0.1822, -0.4123,  0.2575],\n",
       "          [ 1.2696, -1.5467,  0.0756,  ..., -0.3753, -0.4177, -0.4481],\n",
       "          ...,\n",
       "          [-1.0557, -0.7049, -1.7620,  ..., -0.0304,  0.8322,  0.3933],\n",
       "          [-1.0557, -0.7049, -1.7620,  ..., -0.0304,  0.8322,  0.3933],\n",
       "          [-1.0557, -0.7049, -1.7620,  ..., -0.0304,  0.8322,  0.3933]],\n",
       "\n",
       "         [[-1.3214, -2.4707, -1.4566,  ..., -1.3993, -3.5627,  1.4588],\n",
       "          [ 0.8752,  0.2085,  1.4504,  ...,  1.1396,  0.3387, -0.0670],\n",
       "          [-0.2598, -2.0862,  0.5436,  ..., -2.3275,  0.7087,  2.1219],\n",
       "          ...,\n",
       "          [ 1.3335,  0.1196,  1.0265,  ...,  0.6133,  0.6172,  0.7640],\n",
       "          [ 1.3335,  0.1196,  1.0265,  ...,  0.6133,  0.6172,  0.7640],\n",
       "          [ 1.3335,  0.1196,  1.0265,  ...,  0.6133,  0.6172,  0.7640]],\n",
       "\n",
       "         [[-1.0117, -0.6728,  2.8443,  ...,  0.8852,  0.3500,  3.4017],\n",
       "          [-1.9801, -0.4381, -3.4063,  ...,  0.2336,  1.8025,  1.2539],\n",
       "          [-2.5312, -0.1698,  0.5914,  ..., -0.3788, -0.8816,  2.1368],\n",
       "          ...,\n",
       "          [ 0.7450, -1.1631,  0.2265,  ...,  1.0412, -1.1223,  0.8685],\n",
       "          [ 0.7450, -1.1631,  0.2265,  ...,  1.0412, -1.1223,  0.8685],\n",
       "          [ 0.7450, -1.1631,  0.2265,  ...,  1.0412, -1.1223,  0.8685]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[-3.0983,  1.1654,  2.7545,  ...,  1.6623, -0.0993,  2.2322],\n",
       "          [ 0.6707, -0.0181, -1.9318,  ...,  0.7768,  4.3710,  3.0974],\n",
       "          [ 1.6835, -1.6815,  0.3223,  ...,  1.9184, -0.5172,  1.7011],\n",
       "          ...,\n",
       "          [ 6.0062, -0.6851, -2.2859,  ..., -1.4523,  3.2304, -0.7990],\n",
       "          [ 6.0062, -0.6851, -2.2859,  ..., -1.4523,  3.2304, -0.7990],\n",
       "          [ 6.0062, -0.6851, -2.2859,  ..., -1.4523,  3.2304, -0.7990]],\n",
       "\n",
       "         [[-2.9911,  0.3404,  0.4680,  ...,  0.0213, -3.4457,  2.3947],\n",
       "          [-4.3744, -1.4945, -0.3943,  ...,  0.9338,  1.0033,  1.7582],\n",
       "          [-0.0571, -0.6824,  1.0755,  ...,  1.4132, -0.9432,  0.6721],\n",
       "          ...,\n",
       "          [ 0.0727, -4.6851, -2.3885,  ..., -0.1776,  1.2010,  1.5115],\n",
       "          [ 0.0727, -4.6851, -2.3885,  ..., -0.1776,  1.2010,  1.5115],\n",
       "          [ 0.0727, -4.6851, -2.3885,  ..., -0.1776,  1.2010,  1.5115]],\n",
       "\n",
       "         [[-3.9201, -2.2197, -1.3420,  ..., -2.4130,  3.0029, -0.6425],\n",
       "          [-1.3442, -0.1016,  1.7463,  ...,  0.2046,  1.1538, -1.4497],\n",
       "          [-1.3898, -0.0810,  0.4027,  ...,  1.7894, -1.2249, -0.9156],\n",
       "          ...,\n",
       "          [-2.3345,  0.3003,  0.6589,  ..., -3.4705, -2.0471, -4.6802],\n",
       "          [-2.3345,  0.3003,  0.6589,  ..., -3.4705, -2.0471, -4.6802],\n",
       "          [-2.3345,  0.3003,  0.6589,  ..., -3.4705, -2.0471, -4.6802]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0445, -0.6840, -3.5710,  ..., -0.1662, -2.0965,  1.5357],\n",
       "          [-1.6701,  0.3498,  2.0110,  ...,  1.9945, -1.4530,  0.4055],\n",
       "          [ 1.7039, -3.6110,  1.6566,  ..., -0.3059,  1.8562, -1.8068],\n",
       "          ...,\n",
       "          [ 1.9990,  1.6477,  1.7146,  ..., -1.0097, -4.7657,  2.2612],\n",
       "          [ 1.9990,  1.6477,  1.7146,  ..., -1.0097, -4.7657,  2.2612],\n",
       "          [ 1.9990,  1.6477,  1.7146,  ..., -1.0097, -4.7657,  2.2612]],\n",
       "\n",
       "         [[ 0.1949, -4.6007, -0.1229,  ...,  2.0686, -0.1796,  1.0755],\n",
       "          [ 3.9601, -2.2976,  0.7537,  ..., -1.2502,  2.4901,  0.6190],\n",
       "          [ 1.0872, -4.1901,  1.3972,  ...,  0.6112,  1.2337,  0.3507],\n",
       "          ...,\n",
       "          [ 0.5871,  1.8648,  2.6208,  ...,  0.7268,  1.9942,  2.8131],\n",
       "          [ 0.5871,  1.8648,  2.6208,  ...,  0.7268,  1.9942,  2.8131],\n",
       "          [ 0.5871,  1.8648,  2.6208,  ...,  0.7268,  1.9942,  2.8131]],\n",
       "\n",
       "         [[ 0.4591,  1.7150, -3.1907,  ...,  1.5695, -0.6391, -2.6402],\n",
       "          [ 2.5842, -0.3424,  0.9790,  ..., -2.1868, -1.6401, -0.8268],\n",
       "          [ 1.7368,  0.7441,  3.6199,  ..., -1.1967, -1.2578, -0.3118],\n",
       "          ...,\n",
       "          [ 0.5611, -0.0426, -0.6805,  ...,  2.6567,  1.1775, -0.7982],\n",
       "          [ 0.5611, -0.0426, -0.6805,  ...,  2.6567,  1.1775, -0.7982],\n",
       "          [ 0.5611, -0.0426, -0.6805,  ...,  2.6567,  1.1775, -0.7982]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-8.4905e-01, -4.4057e-01, -4.7358e-01,  ...,  1.1211e-01,\n",
       "           -3.6568e-02, -5.7411e-01],\n",
       "          [-8.3050e-01, -1.5184e-01, -3.5618e-01,  ..., -1.6724e+00,\n",
       "            9.4132e-01,  1.7635e-01],\n",
       "          [ 6.9534e-01,  6.7771e-01,  8.3596e-01,  ..., -3.6029e-01,\n",
       "           -1.2756e+00, -2.0686e-01],\n",
       "          ...,\n",
       "          [-8.6312e-01, -4.1481e-01, -4.4726e-01,  ...,  1.0615e-01,\n",
       "           -7.2986e-02, -6.0124e-01],\n",
       "          [-8.6296e-01, -4.1663e-01, -4.4793e-01,  ...,  1.0643e-01,\n",
       "           -7.1369e-02, -6.0008e-01],\n",
       "          [-8.6222e-01, -4.1749e-01, -4.4881e-01,  ...,  1.0704e-01,\n",
       "           -6.9262e-02, -5.9884e-01]],\n",
       "\n",
       "         [[-1.7262e-01,  2.4756e-01,  7.9413e-02,  ...,  4.4243e-01,\n",
       "            6.4308e-02,  4.2508e-01],\n",
       "          [-2.9860e-01,  1.2891e+00,  2.8545e-02,  ...,  2.3963e-01,\n",
       "            4.8824e-01, -1.1098e-01],\n",
       "          [-2.0008e-01,  4.3154e-01, -2.4298e-02,  ...,  1.0949e+00,\n",
       "            1.5842e-01, -6.2554e-01],\n",
       "          ...,\n",
       "          [-1.6934e-01,  1.8884e-01,  1.9586e-02,  ...,  4.0442e-01,\n",
       "            3.4306e-02,  4.7114e-01],\n",
       "          [-1.6897e-01,  1.9147e-01,  2.2757e-02,  ...,  4.0575e-01,\n",
       "            3.5694e-02,  4.6969e-01],\n",
       "          [-1.6930e-01,  1.9409e-01,  2.5457e-02,  ...,  4.0730e-01,\n",
       "            3.7229e-02,  4.6820e-01]],\n",
       "\n",
       "         [[-7.8989e-01,  1.3512e-01,  8.8764e-01,  ...,  5.1540e-01,\n",
       "            4.8670e-01, -4.6958e-01],\n",
       "          [-1.0306e+00,  7.2090e-01, -1.5543e-01,  ...,  5.2616e-01,\n",
       "            3.3397e-01, -4.4466e-01],\n",
       "          [-1.7604e+00,  1.0563e+00, -5.5188e-01,  ...,  1.6776e-01,\n",
       "           -3.6591e-01,  1.3019e-01],\n",
       "          ...,\n",
       "          [-7.6612e-01,  1.8315e-01,  9.1758e-01,  ...,  5.2951e-01,\n",
       "            5.0483e-01, -4.4899e-01],\n",
       "          [-7.6706e-01,  1.8073e-01,  9.1582e-01,  ...,  5.2847e-01,\n",
       "            5.0414e-01, -4.4970e-01],\n",
       "          [-7.6774e-01,  1.7922e-01,  9.1443e-01,  ...,  5.2742e-01,\n",
       "            5.0332e-01, -4.5033e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0480e-03, -1.3140e+00, -3.3368e-01,  ...,  1.2589e+00,\n",
       "            3.0162e-01,  7.1606e-01],\n",
       "          [ 1.1784e+00, -8.3536e-01,  5.8294e-01,  ..., -7.7009e-01,\n",
       "            1.5500e+00,  5.1347e-01],\n",
       "          [ 1.4418e+00, -1.4571e+00, -2.2823e-01,  ...,  1.8120e+00,\n",
       "            3.2449e-01,  3.9660e-01],\n",
       "          ...,\n",
       "          [-7.2937e-03, -1.3315e+00, -3.5156e-01,  ...,  1.2858e+00,\n",
       "            2.8353e-01,  7.2966e-01],\n",
       "          [-7.6825e-03, -1.3305e+00, -3.5022e-01,  ...,  1.2850e+00,\n",
       "            2.8482e-01,  7.2882e-01],\n",
       "          [-7.5711e-03, -1.3298e+00, -3.4933e-01,  ...,  1.2839e+00,\n",
       "            2.8562e-01,  7.2718e-01]],\n",
       "\n",
       "         [[ 6.0094e-01, -5.0896e-02, -5.9354e-02,  ..., -7.9606e-01,\n",
       "            4.6008e-01, -3.1412e+00],\n",
       "          [ 1.5614e+00, -7.0171e-01,  7.7129e-02,  ..., -5.9095e-01,\n",
       "            1.0551e+00,  7.3896e-01],\n",
       "          [ 3.0836e+00,  1.0811e-03,  8.2064e-01,  ...,  1.1242e+00,\n",
       "            9.8059e-01,  1.2078e+00],\n",
       "          ...,\n",
       "          [ 5.7252e-01, -1.5335e-02, -3.2867e-02,  ..., -7.9806e-01,\n",
       "            4.8613e-01, -3.1802e+00],\n",
       "          [ 5.7368e-01, -1.6146e-02, -3.4014e-02,  ..., -7.9780e-01,\n",
       "            4.8417e-01, -3.1785e+00],\n",
       "          [ 5.7410e-01, -1.8450e-02, -3.5447e-02,  ..., -7.9794e-01,\n",
       "            4.8274e-01, -3.1765e+00]],\n",
       "\n",
       "         [[-2.2254e-01,  4.0189e-01,  2.6541e-01,  ..., -3.3664e-01,\n",
       "           -4.3890e-01,  2.7936e-01],\n",
       "          [-2.3456e-01, -3.5947e-01,  5.4555e-01,  ..., -1.2996e+00,\n",
       "           -9.0702e-01,  6.3016e-01],\n",
       "          [ 1.7180e-01, -9.6689e-01,  2.6933e-01,  ..., -2.2323e-01,\n",
       "           -1.2325e+00,  1.9141e-01],\n",
       "          ...,\n",
       "          [-2.2457e-01,  4.7288e-01,  1.9825e-01,  ..., -3.2047e-01,\n",
       "           -3.7326e-01,  2.6924e-01],\n",
       "          [-2.2426e-01,  4.6927e-01,  2.0156e-01,  ..., -3.2212e-01,\n",
       "           -3.7565e-01,  2.6965e-01],\n",
       "          [-2.2355e-01,  4.6627e-01,  2.0493e-01,  ..., -3.2310e-01,\n",
       "           -3.7894e-01,  2.7055e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-2.5383e-01, -1.0896e-01, -1.6815e-01,  ...,  6.0397e-02,\n",
       "            1.0188e-01, -1.7095e-01],\n",
       "          [-9.7546e-01,  2.5270e-01, -1.2855e+00,  ..., -1.2682e+00,\n",
       "           -3.7854e-01, -2.8762e-01],\n",
       "          [ 1.9225e+00,  1.0605e+00,  3.2969e+00,  ..., -2.2961e+00,\n",
       "            1.6388e+00,  1.1466e-01],\n",
       "          ...,\n",
       "          [-2.6793e-01, -8.3976e-02, -1.8087e-01,  ...,  3.6093e-02,\n",
       "            1.1433e-01, -1.7761e-01],\n",
       "          [-2.6801e-01, -8.5449e-02, -1.8060e-01,  ...,  3.7011e-02,\n",
       "            1.1318e-01, -1.7752e-01],\n",
       "          [-2.6714e-01, -8.6771e-02, -1.7890e-01,  ...,  3.8242e-02,\n",
       "            1.1198e-01, -1.7698e-01]],\n",
       "\n",
       "         [[ 2.1474e-01,  2.1021e-01, -1.4374e-01,  ...,  1.4974e-01,\n",
       "            1.8089e-01,  3.1123e-01],\n",
       "          [-2.7313e-01,  1.2871e+00,  8.2896e-01,  ..., -4.2194e-01,\n",
       "            6.5019e-01, -3.7851e-01],\n",
       "          [ 6.1629e-01,  6.9900e-01, -1.1430e-01,  ...,  1.3289e-01,\n",
       "           -7.6487e-01, -1.1834e+00],\n",
       "          ...,\n",
       "          [ 2.0882e-01,  2.6464e-01, -1.2829e-01,  ...,  1.8562e-01,\n",
       "            2.0719e-01,  3.9192e-01],\n",
       "          [ 2.0972e-01,  2.6312e-01, -1.2783e-01,  ...,  1.8448e-01,\n",
       "            2.0545e-01,  3.8907e-01],\n",
       "          [ 2.0943e-01,  2.6183e-01, -1.2794e-01,  ...,  1.8323e-01,\n",
       "            2.0348e-01,  3.8711e-01]],\n",
       "\n",
       "         [[ 5.0955e-02, -3.8469e-02, -9.2572e-02,  ...,  2.3165e-01,\n",
       "           -6.4634e-02,  3.6601e-02],\n",
       "          [ 7.7513e-01,  1.9598e+00, -6.0666e-01,  ...,  2.9412e+00,\n",
       "            2.2151e+00,  6.9622e-01],\n",
       "          [ 7.3504e-02, -7.1283e-01,  2.8837e-01,  ...,  2.1765e+00,\n",
       "            2.0908e+00,  3.4635e-01],\n",
       "          ...,\n",
       "          [ 7.3043e-02, -7.3239e-02, -7.7268e-02,  ...,  1.9331e-01,\n",
       "           -5.9871e-02,  5.3237e-02],\n",
       "          [ 7.1290e-02, -7.1814e-02, -7.7717e-02,  ...,  1.9530e-01,\n",
       "           -6.0162e-02,  5.1348e-02],\n",
       "          [ 7.0684e-02, -7.0766e-02, -7.8773e-02,  ...,  1.9644e-01,\n",
       "           -6.0589e-02,  5.0265e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.0632e-01, -1.7240e-02, -2.6324e-01,  ..., -2.4402e-01,\n",
       "           -1.1415e-01, -4.6559e-02],\n",
       "          [-1.4593e+00,  4.9139e-02, -2.3528e+00,  ..., -3.6795e-01,\n",
       "            5.4708e-01, -5.8439e-01],\n",
       "          [-2.5157e+00, -1.3365e+00, -2.2864e+00,  ...,  2.7283e+00,\n",
       "            1.5446e-01,  1.5011e+00],\n",
       "          ...,\n",
       "          [ 2.1967e-01,  6.1473e-04, -2.6638e-01,  ..., -2.3457e-01,\n",
       "           -1.3743e-01, -6.6850e-02],\n",
       "          [ 2.1907e-01, -1.8720e-04, -2.6572e-01,  ..., -2.3345e-01,\n",
       "           -1.3622e-01, -6.6015e-02],\n",
       "          [ 2.1896e-01, -1.6152e-03, -2.6583e-01,  ..., -2.3463e-01,\n",
       "           -1.3497e-01, -6.4365e-02]],\n",
       "\n",
       "         [[ 2.1757e-01,  5.3588e-02,  8.8009e-02,  ..., -1.3477e-01,\n",
       "           -1.1498e-01,  5.8604e-02],\n",
       "          [ 2.8477e-01, -2.8817e-02,  3.5140e-01,  ..., -7.1524e-02,\n",
       "            9.2120e-01,  4.6701e-01],\n",
       "          [-3.7854e+00,  5.8223e-03,  1.5665e+00,  ..., -2.2417e+00,\n",
       "           -2.3152e+00,  5.6359e-01],\n",
       "          ...,\n",
       "          [ 2.2113e-01,  7.3814e-02,  6.4008e-02,  ..., -1.1211e-01,\n",
       "           -1.3865e-01,  5.1946e-02],\n",
       "          [ 2.2136e-01,  7.3646e-02,  6.4865e-02,  ..., -1.1335e-01,\n",
       "           -1.3764e-01,  5.2249e-02],\n",
       "          [ 2.2131e-01,  7.1724e-02,  6.5839e-02,  ..., -1.1405e-01,\n",
       "           -1.3557e-01,  5.2299e-02]],\n",
       "\n",
       "         [[-1.4121e-01, -4.5263e-01, -1.1757e-01,  ..., -1.6569e-01,\n",
       "           -4.3990e-01,  4.3897e-01],\n",
       "          [-5.4121e-01, -2.2191e+00,  1.0735e+00,  ...,  3.8391e-01,\n",
       "           -1.5870e+00,  2.2276e+00],\n",
       "          [-4.1742e-01, -1.5494e+00,  4.4419e-01,  ..., -8.7525e-01,\n",
       "           -2.9828e+00,  1.1266e+00],\n",
       "          ...,\n",
       "          [-2.0897e-01, -4.4391e-01, -9.2556e-02,  ..., -1.3544e-01,\n",
       "           -3.9366e-01,  4.0569e-01],\n",
       "          [-2.0633e-01, -4.4392e-01, -9.4507e-02,  ..., -1.3689e-01,\n",
       "           -3.9483e-01,  4.0766e-01],\n",
       "          [-2.0333e-01, -4.4410e-01, -9.5475e-02,  ..., -1.3781e-01,\n",
       "           -3.9744e-01,  4.0874e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-0.4270,  1.0286, -0.4180,  ..., -0.0734,  0.8448,  2.2440],\n",
       "          [ 1.2167,  0.2620,  0.5194,  ..., -0.3553, -0.0589,  0.7804],\n",
       "          [ 0.7843,  0.7926, -0.2445,  ...,  0.5937,  0.0848,  2.2570],\n",
       "          ...,\n",
       "          [-0.0075,  1.6378,  1.7653,  ...,  0.0542,  0.0425, -1.0655],\n",
       "          [-0.0075,  1.6378,  1.7653,  ...,  0.0542,  0.0425, -1.0655],\n",
       "          [-0.0075,  1.6378,  1.7653,  ...,  0.0542,  0.0425, -1.0655]],\n",
       "\n",
       "         [[ 1.3642, -0.1006,  0.0254,  ...,  0.5456,  0.3521,  1.1184],\n",
       "          [-0.5193, -1.2786, -1.4937,  ..., -0.7220,  0.3554, -0.5431],\n",
       "          [-0.9370, -0.4902, -0.9434,  ..., -0.0668,  0.5321,  0.0159],\n",
       "          ...,\n",
       "          [ 2.1132,  1.4870,  0.8992,  ...,  1.2795, -0.1370, -0.0504],\n",
       "          [ 2.1132,  1.4870,  0.8992,  ...,  1.2795, -0.1370, -0.0504],\n",
       "          [ 2.1132,  1.4870,  0.8992,  ...,  1.2795, -0.1370, -0.0504]],\n",
       "\n",
       "         [[-0.6672,  0.2557,  0.9776,  ...,  0.7437,  1.4196, -1.6191],\n",
       "          [ 1.4768,  0.4689,  0.9255,  ..., -0.1269,  0.8441,  0.0340],\n",
       "          [ 1.0464,  1.3286,  1.8783,  ..., -0.4923, -0.1801, -0.7810],\n",
       "          ...,\n",
       "          [ 0.2259,  0.0913,  1.6601,  ..., -0.3721,  2.5601, -1.2318],\n",
       "          [ 0.2259,  0.0913,  1.6601,  ..., -0.3721,  2.5601, -1.2318],\n",
       "          [ 0.2259,  0.0913,  1.6601,  ..., -0.3721,  2.5601, -1.2318]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4393,  0.1612,  0.2952,  ...,  0.1695,  1.6909,  1.3941],\n",
       "          [-0.3609, -1.7899, -0.4188,  ..., -1.4588,  2.2965,  1.2120],\n",
       "          [-1.2827, -0.4054, -0.1445,  ..., -1.0032,  0.9196,  0.1205],\n",
       "          ...,\n",
       "          [-0.2379,  3.3623, -2.5430,  ...,  0.6684,  4.0210,  1.7221],\n",
       "          [-0.2379,  3.3623, -2.5430,  ...,  0.6684,  4.0210,  1.7221],\n",
       "          [-0.2379,  3.3623, -2.5430,  ...,  0.6684,  4.0210,  1.7221]],\n",
       "\n",
       "         [[ 0.8614,  0.7406,  0.7391,  ..., -0.3236, -0.0958, -1.0990],\n",
       "          [ 1.0541,  5.7540,  0.1182,  ..., -2.2819,  2.0524, -1.0047],\n",
       "          [ 0.7967,  2.8298, -0.4221,  ...,  3.1995, -1.8070, -0.9022],\n",
       "          ...,\n",
       "          [-1.3262,  3.4231,  0.9662,  ..., -0.2684,  1.1423, -1.2166],\n",
       "          [-1.3262,  3.4231,  0.9662,  ..., -0.2684,  1.1423, -1.2166],\n",
       "          [-1.3262,  3.4231,  0.9662,  ..., -0.2684,  1.1423, -1.2166]],\n",
       "\n",
       "         [[ 0.6843, -0.6500, -0.3197,  ..., -0.4650, -0.5301,  2.3100],\n",
       "          [-0.2163, -1.6362,  2.5556,  ..., -0.4505,  0.3121,  0.9815],\n",
       "          [ 0.1120, -1.3113,  1.4555,  ...,  0.7122,  0.1632,  0.8436],\n",
       "          ...,\n",
       "          [-2.1456,  0.6829,  1.0422,  ..., -0.1409,  3.1450,  1.3836],\n",
       "          [-2.1456,  0.6829,  1.0422,  ..., -0.1409,  3.1450,  1.3836],\n",
       "          [-2.1456,  0.6829,  1.0422,  ..., -0.1409,  3.1450,  1.3836]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[-3.6994, -3.0383, -3.3281,  ...,  0.0561, -1.3515, -1.8485],\n",
       "          [-1.8423, -4.4128, -3.3520,  ..., -1.5633,  1.4728,  0.8695],\n",
       "          [ 0.7378, -1.7113, -1.0500,  ..., -5.4286, -1.1542,  1.9157],\n",
       "          ...,\n",
       "          [ 3.2102, -3.3695,  2.0932,  ...,  0.6182,  1.6308, -1.2712],\n",
       "          [ 3.2102, -3.3695,  2.0932,  ...,  0.6182,  1.6308, -1.2712],\n",
       "          [ 3.2102, -3.3695,  2.0932,  ...,  0.6182,  1.6308, -1.2712]],\n",
       "\n",
       "         [[ 2.3479,  1.4663,  3.1151,  ...,  4.0290, -1.1257,  0.2051],\n",
       "          [-0.5545,  0.8604,  1.7889,  ...,  4.2215, -4.8367, -0.6953],\n",
       "          [-2.5191,  0.2831, -2.5597,  ..., -0.3146, -0.5181,  1.3298],\n",
       "          ...,\n",
       "          [-1.0180,  0.9062,  0.9671,  ..., -0.2661,  0.7344, -3.0825],\n",
       "          [-1.0180,  0.9062,  0.9671,  ..., -0.2661,  0.7344, -3.0825],\n",
       "          [-1.0180,  0.9062,  0.9671,  ..., -0.2661,  0.7344, -3.0825]],\n",
       "\n",
       "         [[ 0.2451,  2.8239,  2.6284,  ...,  3.7894, -3.7253, -6.0479],\n",
       "          [ 3.8584,  0.6901, -1.3543,  ...,  1.5118,  1.7972,  4.8048],\n",
       "          [ 0.9997, -2.1993,  0.6726,  ...,  0.8992,  2.1849, -1.6272],\n",
       "          ...,\n",
       "          [-1.5228,  2.0785,  0.0162,  ...,  2.2375,  3.6786,  2.6598],\n",
       "          [-1.5228,  2.0785,  0.0162,  ...,  2.2375,  3.6786,  2.6598],\n",
       "          [-1.5228,  2.0785,  0.0162,  ...,  2.2375,  3.6786,  2.6598]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3746,  1.3000, -0.2895,  ...,  2.0482,  0.7962,  3.2170],\n",
       "          [ 0.5430,  1.9725, -5.2252,  ...,  3.0297,  2.1658,  0.0063],\n",
       "          [-1.2203,  2.5227, -2.2826,  ..., -2.6479,  2.5277,  1.1927],\n",
       "          ...,\n",
       "          [ 2.6978,  1.5184, -1.0575,  ...,  1.2847, -3.9576, -3.0950],\n",
       "          [ 2.6978,  1.5184, -1.0575,  ...,  1.2847, -3.9576, -3.0950],\n",
       "          [ 2.6978,  1.5184, -1.0575,  ...,  1.2847, -3.9576, -3.0950]],\n",
       "\n",
       "         [[ 0.6142, -0.1460, -2.6696,  ...,  2.5438,  2.5871, -1.2309],\n",
       "          [-0.3136,  0.4845, -1.4112,  ...,  0.0126, -0.7111, -4.6146],\n",
       "          [ 2.5957, -1.1853, -1.8815,  ...,  1.6448, -2.0869, -2.4393],\n",
       "          ...,\n",
       "          [ 5.5986, -0.2700,  1.0556,  ...,  1.1769, -3.0857, -0.7198],\n",
       "          [ 5.5986, -0.2700,  1.0556,  ...,  1.1769, -3.0857, -0.7198],\n",
       "          [ 5.5986, -0.2700,  1.0556,  ...,  1.1769, -3.0857, -0.7198]],\n",
       "\n",
       "         [[-5.3772,  3.6991,  4.0044,  ..., -2.8442, -0.0201,  1.9202],\n",
       "          [-1.0587,  2.7619, -0.0325,  ...,  0.0434,  0.1186, -0.0887],\n",
       "          [ 0.4516,  2.5652,  0.9154,  ...,  1.0185, -1.7879, -2.2596],\n",
       "          ...,\n",
       "          [-0.4159,  0.4678, -0.5458,  ..., -0.7311,  2.6932,  4.4179],\n",
       "          [-0.4159,  0.4678, -0.5458,  ..., -0.7311,  2.6932,  4.4179],\n",
       "          [-0.4159,  0.4678, -0.5458,  ..., -0.7311,  2.6932,  4.4179]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.9464e+00,  1.7833e+00,  1.4901e+00,  ...,  1.4255e+00,\n",
       "           -5.2125e-01,  2.4211e-02],\n",
       "          [ 2.7243e+00,  6.5414e-01, -1.4531e+00,  ..., -7.5349e-01,\n",
       "            2.9195e-01,  3.9119e-01],\n",
       "          [ 6.9059e-01,  1.0521e+00,  1.0825e+00,  ..., -5.3340e-01,\n",
       "            6.0762e-01,  5.5224e-01],\n",
       "          ...,\n",
       "          [ 1.9312e+00,  1.9174e+00,  1.4671e+00,  ...,  1.5520e+00,\n",
       "           -4.8780e-01,  5.9616e-02],\n",
       "          [ 1.9319e+00,  1.9108e+00,  1.4679e+00,  ...,  1.5446e+00,\n",
       "           -4.8945e-01,  5.6861e-02],\n",
       "          [ 1.9324e+00,  1.9030e+00,  1.4694e+00,  ...,  1.5370e+00,\n",
       "           -4.9111e-01,  5.4128e-02]],\n",
       "\n",
       "         [[-4.9721e-01,  8.3616e-03, -3.9549e-01,  ...,  7.9894e-02,\n",
       "           -6.0210e-01,  4.0520e-01],\n",
       "          [-9.2117e-01, -2.1854e-01, -5.2114e-02,  ..., -1.9221e-01,\n",
       "           -8.2039e-01,  8.2749e-01],\n",
       "          [-5.2745e-01,  1.1810e+00, -3.9533e-01,  ..., -3.1404e-01,\n",
       "           -8.2848e-01,  1.2401e+00],\n",
       "          ...,\n",
       "          [-5.2928e-01,  6.0268e-02, -4.1837e-01,  ...,  1.3476e-01,\n",
       "           -5.7417e-01,  4.1696e-01],\n",
       "          [-5.2751e-01,  5.7685e-02, -4.1734e-01,  ...,  1.3222e-01,\n",
       "           -5.7653e-01,  4.1582e-01],\n",
       "          [-5.2620e-01,  5.4914e-02, -4.1685e-01,  ...,  1.2861e-01,\n",
       "           -5.7850e-01,  4.1475e-01]],\n",
       "\n",
       "         [[ 9.0197e-01, -2.3592e+00, -3.5254e-01,  ...,  1.0660e+00,\n",
       "            4.9789e-01,  7.3708e-01],\n",
       "          [ 3.7927e-01,  1.2444e-01,  6.3340e-01,  ...,  1.0412e+00,\n",
       "            5.5564e-01, -5.2042e-02],\n",
       "          [ 1.0994e+00, -3.5373e-01, -1.1731e-01,  ..., -6.4997e-01,\n",
       "           -7.4626e-01,  9.2446e-01],\n",
       "          ...,\n",
       "          [ 9.6488e-01, -2.3702e+00, -4.3634e-01,  ...,  1.1137e+00,\n",
       "            4.4551e-01,  7.6151e-01],\n",
       "          [ 9.6131e-01, -2.3690e+00, -4.3199e-01,  ...,  1.1115e+00,\n",
       "            4.4755e-01,  7.6090e-01],\n",
       "          [ 9.5776e-01, -2.3681e+00, -4.2807e-01,  ...,  1.1083e+00,\n",
       "            4.4945e-01,  7.5883e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.9706e-01,  5.2024e-02, -7.0406e-02,  ..., -1.2503e+00,\n",
       "            1.2847e+00, -1.1111e+00],\n",
       "          [ 4.5006e-01, -2.1170e-01, -2.0708e+00,  ..., -4.3606e-01,\n",
       "            1.1622e+00, -1.6392e+00],\n",
       "          [-5.0685e-01, -1.4703e+00, -1.4374e+00,  ..., -1.0864e+00,\n",
       "            8.1211e-01, -1.6857e+00],\n",
       "          ...,\n",
       "          [-8.8512e-01,  9.0126e-02, -8.1111e-03,  ..., -1.3428e+00,\n",
       "            1.2195e+00, -1.1619e+00],\n",
       "          [-8.8569e-01,  8.7753e-02, -1.1571e-02,  ..., -1.3380e+00,\n",
       "            1.2228e+00, -1.1587e+00],\n",
       "          [-8.8639e-01,  8.5666e-02, -1.3959e-02,  ..., -1.3338e+00,\n",
       "            1.2256e+00, -1.1557e+00]],\n",
       "\n",
       "         [[ 4.9548e-01, -1.2111e-01, -2.2496e-01,  ..., -4.3528e-01,\n",
       "           -1.2707e-01, -5.7299e-03],\n",
       "          [ 1.0004e+00,  3.6307e-01, -2.9545e-01,  ..., -1.1123e+00,\n",
       "            1.3391e-01,  1.6116e+00],\n",
       "          [ 3.2893e-01,  1.1246e+00, -1.4912e+00,  ..., -3.4039e-01,\n",
       "            1.1994e+00,  6.5492e-01],\n",
       "          ...,\n",
       "          [ 4.8401e-01, -1.6327e-01, -1.9235e-01,  ..., -4.5561e-01,\n",
       "           -1.1996e-01, -1.8037e-03],\n",
       "          [ 4.8425e-01, -1.6110e-01, -1.9384e-01,  ..., -4.5487e-01,\n",
       "           -1.2094e-01, -2.3712e-03],\n",
       "          [ 4.8461e-01, -1.5957e-01, -1.9535e-01,  ..., -4.5293e-01,\n",
       "           -1.2089e-01, -2.3413e-03]],\n",
       "\n",
       "         [[-5.7858e-02,  1.8646e-01,  1.3509e+00,  ...,  4.5514e-01,\n",
       "            2.4584e-01, -2.4501e-01],\n",
       "          [-9.8454e-02, -1.9465e-01,  4.5910e-01,  ...,  7.2537e-02,\n",
       "           -7.4237e-01, -1.0572e+00],\n",
       "          [ 2.2191e-01,  4.4412e-02,  9.2638e-02,  ...,  1.0442e+00,\n",
       "           -1.2814e+00, -1.2856e+00],\n",
       "          ...,\n",
       "          [-6.6464e-02,  1.1971e-01,  1.2339e+00,  ...,  5.0620e-01,\n",
       "            2.3522e-01, -1.8233e-01],\n",
       "          [-6.5160e-02,  1.2386e-01,  1.2401e+00,  ...,  5.0405e-01,\n",
       "            2.3669e-01, -1.8536e-01],\n",
       "          [-6.4781e-02,  1.2783e-01,  1.2462e+00,  ...,  5.0164e-01,\n",
       "            2.3769e-01, -1.8855e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-3.2128e-01, -5.6880e-01, -9.0911e-01,  ...,  2.7208e-01,\n",
       "           -5.7862e-01,  5.0261e-01],\n",
       "          [-4.9479e-01, -2.3342e-01,  1.9155e-01,  ...,  4.8276e-01,\n",
       "           -4.5667e-01, -2.5136e-02],\n",
       "          [ 7.3619e-01,  4.4430e-01,  2.5266e-03,  ...,  8.8434e-02,\n",
       "            3.4365e-01,  1.4277e-01],\n",
       "          ...,\n",
       "          [-2.3182e-01, -4.8710e-01, -9.6042e-01,  ...,  2.8937e-01,\n",
       "           -5.6196e-01,  5.3094e-01],\n",
       "          [-2.3595e-01, -4.8981e-01, -9.5757e-01,  ...,  2.8917e-01,\n",
       "           -5.6286e-01,  5.2889e-01],\n",
       "          [-2.4050e-01, -4.9326e-01, -9.5601e-01,  ...,  2.8925e-01,\n",
       "           -5.6433e-01,  5.2701e-01]],\n",
       "\n",
       "         [[ 4.5614e-01, -5.0360e-01, -3.2205e-01,  ..., -2.3719e-01,\n",
       "           -5.1091e-01, -4.5513e-01],\n",
       "          [-9.0892e-03, -1.5301e+00,  1.5810e+00,  ..., -2.5365e+00,\n",
       "            7.0852e-01,  3.9030e-01],\n",
       "          [ 7.5976e-01,  2.7004e-01,  3.7953e+00,  ..., -1.4344e-02,\n",
       "            5.1938e-02, -2.0943e+00],\n",
       "          ...,\n",
       "          [ 4.0215e-01, -5.7980e-01, -2.6431e-01,  ..., -1.5210e-01,\n",
       "           -5.3879e-01, -4.1012e-01],\n",
       "          [ 4.0572e-01, -5.7729e-01, -2.6844e-01,  ..., -1.5530e-01,\n",
       "           -5.3751e-01, -4.1386e-01],\n",
       "          [ 4.0978e-01, -5.7390e-01, -2.7183e-01,  ..., -1.6048e-01,\n",
       "           -5.3732e-01, -4.1639e-01]],\n",
       "\n",
       "         [[ 4.9745e-01,  5.7937e-01, -1.5889e-03,  ..., -7.2163e-02,\n",
       "           -1.3478e-02,  3.2029e-01],\n",
       "          [ 3.1514e+00, -2.6830e-01,  1.5268e-01,  ...,  9.2351e-01,\n",
       "            4.4914e-01,  1.4092e+00],\n",
       "          [ 4.4004e+00, -8.2510e-01, -2.9997e+00,  ..., -2.2746e+00,\n",
       "           -6.6156e-01, -5.6520e-01],\n",
       "          ...,\n",
       "          [ 4.7768e-01,  5.8236e-01,  5.6309e-04,  ..., -7.9540e-02,\n",
       "           -3.4144e-02,  3.2919e-01],\n",
       "          [ 4.7859e-01,  5.8174e-01,  7.1139e-04,  ..., -7.8184e-02,\n",
       "           -3.2038e-02,  3.3042e-01],\n",
       "          [ 4.7778e-01,  5.8156e-01,  1.5854e-03,  ..., -7.7433e-02,\n",
       "           -3.1130e-02,  3.3009e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.6581e-02, -3.2223e-02,  1.8911e-03,  ..., -6.3580e-02,\n",
       "           -4.9741e-01,  3.4811e-01],\n",
       "          [ 1.0731e+00, -8.7130e-02, -2.2320e-01,  ...,  6.0005e-01,\n",
       "           -1.7216e+00, -6.8761e-01],\n",
       "          [ 8.5599e-01, -2.0304e+00,  4.0690e+00,  ..., -3.9943e-01,\n",
       "            4.5221e-01,  1.2137e+00],\n",
       "          ...,\n",
       "          [ 4.3685e-02, -3.6115e-02,  2.1191e-02,  ..., -4.7631e-02,\n",
       "           -5.1507e-01,  3.3871e-01],\n",
       "          [ 4.5216e-02, -3.5984e-02,  1.9397e-02,  ..., -4.8512e-02,\n",
       "           -5.1421e-01,  3.3895e-01],\n",
       "          [ 4.7255e-02, -3.6343e-02,  1.7732e-02,  ..., -4.8737e-02,\n",
       "           -5.1186e-01,  3.3995e-01]],\n",
       "\n",
       "         [[ 7.2126e-02,  1.1730e-01, -3.3614e-01,  ...,  2.0412e-01,\n",
       "           -1.1919e-01, -1.2739e-01],\n",
       "          [ 8.2907e-01, -8.2895e-01, -1.6283e+00,  ...,  3.5876e+00,\n",
       "           -1.0921e-01, -1.2501e+00],\n",
       "          [ 1.9838e+00, -2.7915e-01, -1.0237e+00,  ...,  3.4886e+00,\n",
       "           -8.9763e-01, -1.2714e+00],\n",
       "          ...,\n",
       "          [ 8.7667e-02,  1.1147e-01, -3.2518e-01,  ...,  2.2005e-01,\n",
       "           -9.0615e-02, -1.4908e-01],\n",
       "          [ 8.6775e-02,  1.1095e-01, -3.2653e-01,  ...,  2.1906e-01,\n",
       "           -9.2966e-02, -1.4673e-01],\n",
       "          [ 8.5702e-02,  1.1065e-01, -3.2773e-01,  ...,  2.1821e-01,\n",
       "           -9.4729e-02, -1.4599e-01]],\n",
       "\n",
       "         [[-3.6415e-01, -6.8299e-02, -2.0956e-01,  ..., -4.9543e-01,\n",
       "           -1.6121e-02, -2.9768e-01],\n",
       "          [-5.8445e-01,  1.8256e+00, -2.1877e+00,  ..., -2.5716e+00,\n",
       "           -2.0289e-01, -8.8233e-03],\n",
       "          [-1.6487e-01, -3.3462e+00,  2.4671e+00,  ..., -2.6388e+00,\n",
       "            1.0529e+00,  2.4569e-01],\n",
       "          ...,\n",
       "          [-3.3883e-01, -1.0736e-01, -2.2851e-01,  ..., -4.2936e-01,\n",
       "            2.2318e-03, -3.4790e-01],\n",
       "          [-3.4058e-01, -1.0698e-01, -2.2690e-01,  ..., -4.3155e-01,\n",
       "            1.4361e-03, -3.4665e-01],\n",
       "          [-3.4157e-01, -1.0616e-01, -2.2615e-01,  ..., -4.3353e-01,\n",
       "            7.8022e-04, -3.4384e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.6747, -1.2268, -3.7210,  ...,  0.5600, -0.2157,  0.3639],\n",
       "          [-0.8053, -0.8561, -0.1863,  ..., -1.4279,  0.0455,  0.3802],\n",
       "          [ 0.0658, -1.3406, -2.6270,  ..., -1.9995, -0.6617,  2.8912],\n",
       "          ...,\n",
       "          [ 0.5336,  0.0398, -0.7547,  ...,  1.4767, -2.0541, -0.9166],\n",
       "          [ 0.5336,  0.0398, -0.7547,  ...,  1.4767, -2.0541, -0.9166],\n",
       "          [ 0.5336,  0.0398, -0.7547,  ...,  1.4767, -2.0541, -0.9166]],\n",
       "\n",
       "         [[ 1.8554, -0.0154, -1.6741,  ..., -1.4489, -0.3490,  0.2346],\n",
       "          [ 3.0105, -0.2207,  0.0871,  ...,  1.1123,  0.2408, -0.2417],\n",
       "          [ 2.3352,  1.1673,  1.1291,  ..., -0.1396,  0.5275,  2.3027],\n",
       "          ...,\n",
       "          [-1.3182,  0.4851,  0.8399,  ...,  0.7095,  0.4464,  1.7640],\n",
       "          [-1.3182,  0.4851,  0.8399,  ...,  0.7095,  0.4464,  1.7640],\n",
       "          [-1.3182,  0.4851,  0.8399,  ...,  0.7095,  0.4464,  1.7640]],\n",
       "\n",
       "         [[-1.7334, -2.5143, -0.0836,  ..., -0.4389,  0.2208,  0.3702],\n",
       "          [-0.4513, -0.5869, -0.2085,  ...,  0.7424, -0.0152,  0.1104],\n",
       "          [ 0.2043,  0.0722, -1.4949,  ...,  1.3477, -0.1384, -0.3400],\n",
       "          ...,\n",
       "          [ 1.7172,  1.9916, -0.7273,  ...,  0.0064,  0.1964, -0.4912],\n",
       "          [ 1.7172,  1.9916, -0.7273,  ...,  0.0064,  0.1964, -0.4912],\n",
       "          [ 1.7172,  1.9916, -0.7273,  ...,  0.0064,  0.1964, -0.4912]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5509, -0.2259, -1.9225,  ..., -0.7056,  0.5570, -0.4162],\n",
       "          [-0.0338,  0.0406, -1.5544,  ...,  0.4774,  0.7425,  1.0244],\n",
       "          [ 0.2832,  1.6574, -1.0255,  ..., -0.1984,  0.0262,  0.8761],\n",
       "          ...,\n",
       "          [ 0.7091, -1.9619, -2.4214,  ...,  1.1984,  0.2152, -0.5144],\n",
       "          [ 0.7091, -1.9619, -2.4214,  ...,  1.1984,  0.2152, -0.5144],\n",
       "          [ 0.7091, -1.9619, -2.4214,  ...,  1.1984,  0.2152, -0.5144]],\n",
       "\n",
       "         [[-3.7721, -0.6074, -1.4274,  ...,  2.6721,  2.2845, -0.7991],\n",
       "          [-2.3804,  0.9457, -0.2732,  ..., -0.6580,  2.3598, -0.8530],\n",
       "          [-0.8228,  0.4115,  0.8349,  ...,  1.3655,  1.9495, -0.0911],\n",
       "          ...,\n",
       "          [-1.1839, -0.2362, -1.5985,  ..., -0.3336,  1.3927,  1.0389],\n",
       "          [-1.1839, -0.2362, -1.5985,  ..., -0.3336,  1.3927,  1.0389],\n",
       "          [-1.1839, -0.2362, -1.5985,  ..., -0.3336,  1.3927,  1.0389]],\n",
       "\n",
       "         [[-0.8523,  1.4589,  0.5036,  ...,  1.3527, -1.7126, -0.2709],\n",
       "          [-0.4895,  1.1670, -1.2019,  ..., -0.5495, -0.0510,  1.2568],\n",
       "          [ 0.0124,  1.6053, -1.7345,  ...,  0.8770,  2.6405,  0.6722],\n",
       "          ...,\n",
       "          [-2.4617, -0.1726,  4.0838,  ...,  0.6485,  0.1093, -0.4616],\n",
       "          [-2.4617, -0.1726,  4.0838,  ...,  0.6485,  0.1093, -0.4616],\n",
       "          [-2.4617, -0.1726,  4.0838,  ...,  0.6485,  0.1093, -0.4616]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[-1.0321,  2.3425, -2.8192,  ...,  5.4571, -2.2640,  2.6973],\n",
       "          [ 3.1662,  1.9664, -4.8378,  ...,  0.1885,  3.7500, -0.1410],\n",
       "          [ 3.3316, -5.0677,  1.0481,  ..., -2.7811,  1.5388,  0.8910],\n",
       "          ...,\n",
       "          [-0.9019,  3.5135, -5.2643,  ..., -1.2543, 12.4478,  1.7684],\n",
       "          [-0.9019,  3.5135, -5.2643,  ..., -1.2543, 12.4478,  1.7684],\n",
       "          [-0.9019,  3.5135, -5.2643,  ..., -1.2543, 12.4478,  1.7684]],\n",
       "\n",
       "         [[ 4.8448, -0.0533,  1.5124,  ...,  0.9124,  0.0777,  1.2737],\n",
       "          [ 2.7523, -0.2906,  2.2695,  ...,  0.8946, -1.2339,  2.0079],\n",
       "          [ 1.1257,  4.0522, -1.1988,  ...,  1.6290,  1.4551, -1.0160],\n",
       "          ...,\n",
       "          [-1.3297,  4.2171, -0.8534,  ..., -0.2826,  2.0163,  0.4557],\n",
       "          [-1.3297,  4.2171, -0.8534,  ..., -0.2826,  2.0163,  0.4557],\n",
       "          [-1.3297,  4.2171, -0.8534,  ..., -0.2826,  2.0163,  0.4557]],\n",
       "\n",
       "         [[ 2.3711, -6.1271, -1.4259,  ..., -2.6564,  0.0838,  1.7427],\n",
       "          [ 0.5042,  1.4103,  0.9832,  ...,  0.5919, -0.0700,  3.9565],\n",
       "          [ 0.9857,  0.5198, -0.7932,  ...,  0.3751, -2.4306,  3.2331],\n",
       "          ...,\n",
       "          [-1.9827,  1.0246,  1.6501,  ...,  0.5093, -2.7031, -0.8263],\n",
       "          [-1.9827,  1.0246,  1.6501,  ...,  0.5093, -2.7031, -0.8263],\n",
       "          [-1.9827,  1.0246,  1.6501,  ...,  0.5093, -2.7031, -0.8263]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.3776,  0.2340, -2.6472,  ...,  1.2334, -5.6730, -2.1913],\n",
       "          [-0.0685, -1.4819, -1.4575,  ..., -3.1315,  0.9091, -1.2031],\n",
       "          [-0.1183,  0.8543,  0.4304,  ..., -0.8403, -1.6091, -0.3171],\n",
       "          ...,\n",
       "          [ 1.2091, -2.8032, -2.2553,  ..., -3.0233,  1.0581, -4.8000],\n",
       "          [ 1.2091, -2.8032, -2.2553,  ..., -3.0233,  1.0581, -4.8000],\n",
       "          [ 1.2091, -2.8032, -2.2553,  ..., -3.0233,  1.0581, -4.8000]],\n",
       "\n",
       "         [[ 2.9312,  3.4721, -2.5932,  ...,  5.4569,  2.5920, -1.4850],\n",
       "          [-4.2043,  2.3809,  0.2109,  ...,  3.1093, -3.5423,  0.4619],\n",
       "          [-0.8163,  0.0697,  3.5890,  ...,  0.7988, -0.2213,  0.5760],\n",
       "          ...,\n",
       "          [ 4.1223,  4.7097, -2.2233,  ...,  5.8082, -0.4880, -9.9744],\n",
       "          [ 4.1223,  4.7097, -2.2233,  ...,  5.8082, -0.4880, -9.9744],\n",
       "          [ 4.1223,  4.7097, -2.2233,  ...,  5.8082, -0.4880, -9.9744]],\n",
       "\n",
       "         [[ 4.0364,  4.4474,  0.9382,  ..., -1.9657, -6.1033, -6.8793],\n",
       "          [ 7.3442, -1.8925,  0.5641,  ..., -0.9731, -1.9378,  1.6153],\n",
       "          [ 2.4678, -4.1538, -0.4270,  ..., -2.8831,  2.8164,  4.8148],\n",
       "          ...,\n",
       "          [ 3.3441, -4.2881,  0.3147,  ...,  1.2991, -0.3377, -2.9891],\n",
       "          [ 3.3441, -4.2881,  0.3147,  ...,  1.2991, -0.3377, -2.9891],\n",
       "          [ 3.3441, -4.2881,  0.3147,  ...,  1.2991, -0.3377, -2.9891]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.3276,  0.9372, -0.7828,  ..., -0.7798, -0.3358,  0.3796],\n",
       "          [-1.1105,  0.1264,  0.3185,  ..., -0.5316, -0.0175, -0.1758],\n",
       "          [-1.7352,  0.3560, -0.4734,  ...,  1.0589, -0.0309,  0.5174],\n",
       "          ...,\n",
       "          [-0.3008,  0.9880, -0.8391,  ..., -0.7626, -0.2800,  0.3745],\n",
       "          [-0.3020,  0.9853, -0.8355,  ..., -0.7632, -0.2818,  0.3741],\n",
       "          [-0.3039,  0.9829, -0.8312,  ..., -0.7635, -0.2840,  0.3742]],\n",
       "\n",
       "         [[-2.1515, -0.2124,  0.6646,  ..., -0.2171,  0.5482,  1.2906],\n",
       "          [-0.9514,  1.6369, -0.4747,  ..., -0.9458, -0.5809,  2.0539],\n",
       "          [-1.2924,  1.6105, -0.0842,  ...,  1.3525, -0.1152,  1.6718],\n",
       "          ...,\n",
       "          [-2.0292, -0.3720,  0.6371,  ..., -0.2953,  0.5770,  1.2272],\n",
       "          [-2.0351, -0.3648,  0.6384,  ..., -0.2925,  0.5750,  1.2303],\n",
       "          [-2.0393, -0.3580,  0.6391,  ..., -0.2901,  0.5729,  1.2336]],\n",
       "\n",
       "         [[ 0.4069,  1.4358, -1.2423,  ..., -0.1084, -1.2711, -0.8480],\n",
       "          [ 0.2723,  1.9239, -0.6624,  ..., -0.3813,  0.1049, -2.3791],\n",
       "          [ 0.2884,  1.2730, -2.7138,  ...,  0.1730,  0.3561, -1.6194],\n",
       "          ...,\n",
       "          [ 0.4828,  1.4137, -1.2788,  ..., -0.0687, -1.1661, -0.7317],\n",
       "          [ 0.4802,  1.4150, -1.2762,  ..., -0.0693, -1.1717, -0.7382],\n",
       "          [ 0.4773,  1.4176, -1.2726,  ..., -0.0699, -1.1771, -0.7444]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4103,  0.1090, -0.0826,  ...,  0.3275, -0.5136, -1.3939],\n",
       "          [-1.0968,  0.3214,  0.5815,  ...,  0.0693, -1.3707, -0.8846],\n",
       "          [ 0.1947, -0.7123, -0.7110,  ..., -1.2310, -0.9572,  1.0747],\n",
       "          ...,\n",
       "          [ 0.4550,  0.1514, -0.1336,  ...,  0.2977, -0.5233, -1.3983],\n",
       "          [ 0.4530,  0.1478, -0.1301,  ...,  0.2995, -0.5220, -1.3979],\n",
       "          [ 0.4504,  0.1444, -0.1278,  ...,  0.3009, -0.5206, -1.3970]],\n",
       "\n",
       "         [[-0.3930,  0.7910,  0.3490,  ..., -0.0887,  0.2115, -0.8684],\n",
       "          [-0.5559,  0.5175, -0.4127,  ..., -0.3228,  0.6456, -0.4023],\n",
       "          [ 0.2685, -1.3069, -1.4628,  ..., -1.2085,  0.0375,  0.1107],\n",
       "          ...,\n",
       "          [-0.2812,  0.8192,  0.3765,  ..., -0.1384,  0.1646, -0.8386],\n",
       "          [-0.2856,  0.8178,  0.3764,  ..., -0.1362,  0.1652, -0.8408],\n",
       "          [-0.2899,  0.8160,  0.3749,  ..., -0.1342,  0.1657, -0.8434]],\n",
       "\n",
       "         [[-0.2858,  0.9264, -0.4540,  ..., -0.0028, -0.2313,  0.3330],\n",
       "          [-0.5781,  0.2875, -0.0079,  ..., -1.0122, -0.4707, -1.0750],\n",
       "          [-1.5235, -0.2232, -0.3797,  ...,  0.4919,  0.0476, -0.7867],\n",
       "          ...,\n",
       "          [-0.2462,  0.9194, -0.4096,  ..., -0.0588, -0.2032,  0.3975],\n",
       "          [-0.2487,  0.9205, -0.4120,  ..., -0.0568, -0.2040,  0.3944],\n",
       "          [-0.2517,  0.9209, -0.4150,  ..., -0.0541, -0.2049,  0.3917]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 6.3323e-01,  5.4870e-01, -2.3419e-01,  ...,  3.1800e-01,\n",
       "           -8.1853e-01, -9.2269e-02],\n",
       "          [-5.8394e-01, -7.5003e-01,  3.1346e-01,  ...,  1.5538e+00,\n",
       "           -3.3634e-01, -6.5627e-02],\n",
       "          [ 1.0392e+00,  5.1526e-01, -7.9542e-01,  ...,  1.8675e+00,\n",
       "            7.8752e-01,  1.1466e+00],\n",
       "          ...,\n",
       "          [ 6.1502e-01,  4.7834e-01, -2.2376e-01,  ...,  2.2250e-01,\n",
       "           -7.7802e-01, -1.2753e-01],\n",
       "          [ 6.1465e-01,  4.8345e-01, -2.2113e-01,  ...,  2.2848e-01,\n",
       "           -7.8115e-01, -1.2701e-01],\n",
       "          [ 6.1600e-01,  4.8624e-01, -2.1944e-01,  ...,  2.3140e-01,\n",
       "           -7.8590e-01, -1.2628e-01]],\n",
       "\n",
       "         [[-2.5315e-01, -5.2978e-01,  2.5118e-01,  ..., -1.1853e-02,\n",
       "            5.9598e-01, -6.4250e-01],\n",
       "          [-1.1733e-01, -4.3406e-01, -1.0395e+00,  ...,  9.3439e-01,\n",
       "            7.0899e-01,  5.8874e-02],\n",
       "          [ 2.3487e+00, -1.5810e+00, -2.5455e+00,  ...,  2.2123e+00,\n",
       "           -2.1465e+00,  3.8933e-01],\n",
       "          ...,\n",
       "          [-2.3592e-01, -4.6201e-01,  3.6729e-01,  ...,  6.2564e-02,\n",
       "            5.8641e-01, -6.7440e-01],\n",
       "          [-2.3862e-01, -4.6366e-01,  3.6110e-01,  ...,  6.0681e-02,\n",
       "            5.8522e-01, -6.7382e-01],\n",
       "          [-2.3800e-01, -4.6728e-01,  3.5604e-01,  ...,  5.8310e-02,\n",
       "            5.8321e-01, -6.7055e-01]],\n",
       "\n",
       "         [[-1.8870e-02,  9.9256e-01,  8.6632e-02,  ...,  3.1709e-01,\n",
       "           -2.0927e-01,  3.5401e-01],\n",
       "          [-2.0244e-01,  2.1822e+00, -1.5117e+00,  ...,  8.7493e-01,\n",
       "           -1.8837e+00,  2.0737e-01],\n",
       "          [-9.3498e-01,  3.5088e+00, -3.0538e+00,  ..., -2.8777e+00,\n",
       "           -1.7981e+00,  4.7546e-01],\n",
       "          ...,\n",
       "          [-1.1214e-03,  8.9913e-01,  6.8090e-02,  ...,  2.9936e-01,\n",
       "           -1.9236e-01,  2.5135e-01],\n",
       "          [-8.3661e-04,  9.0321e-01,  6.7139e-02,  ...,  3.0141e-01,\n",
       "           -1.9392e-01,  2.5511e-01],\n",
       "          [-3.7754e-04,  9.0666e-01,  6.8279e-02,  ...,  3.0419e-01,\n",
       "           -1.9377e-01,  2.5889e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0100e-01, -7.1933e-01, -5.0378e-01,  ..., -9.3561e-02,\n",
       "            2.9588e-01,  1.6386e-01],\n",
       "          [ 9.6015e-02, -1.1109e+00, -1.6083e+00,  ..., -1.5716e+00,\n",
       "            5.4559e-01,  7.6802e-01],\n",
       "          [-1.5634e+00,  1.0859e+00, -1.0401e+01,  ...,  1.7711e+00,\n",
       "           -2.2654e+00, -2.2721e+00],\n",
       "          ...,\n",
       "          [ 1.3034e-01, -6.7184e-01, -4.7932e-01,  ..., -1.3785e-01,\n",
       "            3.9128e-01,  2.2679e-01],\n",
       "          [ 1.2916e-01, -6.7338e-01, -4.8150e-01,  ..., -1.3307e-01,\n",
       "            3.8800e-01,  2.2641e-01],\n",
       "          [ 1.2878e-01, -6.7766e-01, -4.8343e-01,  ..., -1.2979e-01,\n",
       "            3.8445e-01,  2.2326e-01]],\n",
       "\n",
       "         [[ 2.9239e-02,  1.9048e-01,  4.9017e-01,  ..., -1.0056e-01,\n",
       "            6.8762e-01,  5.6534e-01],\n",
       "          [ 1.3956e+00, -1.0223e+00,  9.2414e-01,  ..., -6.1051e-01,\n",
       "           -2.0532e-01, -1.1164e+00],\n",
       "          [-3.0040e+00, -1.8575e+00,  1.8086e+00,  ..., -6.6132e-01,\n",
       "           -6.9496e-01, -1.7919e+00],\n",
       "          ...,\n",
       "          [ 8.4105e-02,  8.7733e-02,  4.8151e-01,  ..., -1.6831e-01,\n",
       "            7.8534e-01,  4.6683e-01],\n",
       "          [ 8.0403e-02,  9.2854e-02,  4.8227e-01,  ..., -1.6816e-01,\n",
       "            7.8270e-01,  4.7035e-01],\n",
       "          [ 7.6533e-02,  9.6415e-02,  4.8206e-01,  ..., -1.6460e-01,\n",
       "            7.7961e-01,  4.7454e-01]],\n",
       "\n",
       "         [[ 4.6513e-01, -1.1309e-01, -6.3310e-02,  ...,  4.8010e-01,\n",
       "           -3.3202e-01, -1.9592e-01],\n",
       "          [ 5.4952e-01, -9.1508e-01, -1.7493e+00,  ...,  8.1371e-01,\n",
       "           -4.1347e-01,  1.0962e-01],\n",
       "          [-1.4229e+00,  2.5276e+00,  1.2126e+00,  ..., -2.8554e+00,\n",
       "            8.1522e-01, -1.6413e+00],\n",
       "          ...,\n",
       "          [ 3.6161e-01, -8.8053e-02, -3.9737e-02,  ...,  4.0162e-01,\n",
       "           -3.0344e-01, -1.1805e-01],\n",
       "          [ 3.6638e-01, -8.7962e-02, -4.0925e-02,  ...,  4.0389e-01,\n",
       "           -3.0458e-01, -1.2176e-01],\n",
       "          [ 3.7076e-01, -8.8270e-02, -4.0423e-02,  ...,  4.0805e-01,\n",
       "           -3.0517e-01, -1.2276e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 9.8790e-01, -1.0484e+00, -1.5799e+00,  ..., -2.9385e-01,\n",
       "           -2.5167e+00,  3.0891e-01],\n",
       "          [ 1.7409e+00,  1.0734e+00,  4.2327e-01,  ...,  2.9228e-01,\n",
       "           -1.0538e+00, -1.3067e+00],\n",
       "          [ 9.9961e-01,  2.4989e+00, -8.7484e-01,  ...,  1.1934e+00,\n",
       "           -2.3004e+00,  5.6391e-01],\n",
       "          ...,\n",
       "          [ 3.7176e+00,  1.2889e+00,  1.6301e+00,  ..., -5.8465e-04,\n",
       "           -3.3626e+00, -8.6803e-01],\n",
       "          [ 3.7176e+00,  1.2889e+00,  1.6301e+00,  ..., -5.8465e-04,\n",
       "           -3.3626e+00, -8.6803e-01],\n",
       "          [ 3.7176e+00,  1.2889e+00,  1.6301e+00,  ..., -5.8465e-04,\n",
       "           -3.3626e+00, -8.6803e-01]],\n",
       "\n",
       "         [[-1.2778e+00, -5.2318e-01, -7.4076e-01,  ..., -1.4746e-01,\n",
       "            1.3143e+00, -1.9455e+00],\n",
       "          [ 3.2878e-01,  2.0793e+00, -3.3648e+00,  ...,  1.2744e+00,\n",
       "            1.7155e+00,  2.5089e+00],\n",
       "          [-2.8882e-01, -5.6122e-01,  2.3251e+00,  ...,  2.3815e+00,\n",
       "            5.3442e-02,  2.3384e-01],\n",
       "          ...,\n",
       "          [-6.5208e-01, -2.7649e+00, -1.1846e+00,  ..., -3.9158e-01,\n",
       "            2.8634e-01,  5.4457e-01],\n",
       "          [-6.5208e-01, -2.7649e+00, -1.1846e+00,  ..., -3.9158e-01,\n",
       "            2.8634e-01,  5.4457e-01],\n",
       "          [-6.5208e-01, -2.7649e+00, -1.1846e+00,  ..., -3.9158e-01,\n",
       "            2.8634e-01,  5.4457e-01]],\n",
       "\n",
       "         [[ 3.6874e-02,  3.5219e-01,  1.0468e+00,  ...,  4.9502e-02,\n",
       "            2.7112e+00, -3.7034e-01],\n",
       "          [ 9.9998e-01, -1.0319e+00, -1.9225e-01,  ..., -6.2748e-01,\n",
       "           -1.1485e+00, -8.7363e-01],\n",
       "          [ 4.1518e-01, -2.3408e+00,  2.1776e-01,  ...,  6.2347e-03,\n",
       "            1.6476e+00, -8.7019e-01],\n",
       "          ...,\n",
       "          [ 1.5503e+00, -6.9153e-01,  7.3573e-01,  ..., -3.4709e+00,\n",
       "           -8.1506e-01, -1.1039e+00],\n",
       "          [ 1.5503e+00, -6.9153e-01,  7.3573e-01,  ..., -3.4709e+00,\n",
       "           -8.1506e-01, -1.1039e+00],\n",
       "          [ 1.5503e+00, -6.9153e-01,  7.3573e-01,  ..., -3.4709e+00,\n",
       "           -8.1506e-01, -1.1039e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.1392e+00, -8.0397e-01, -2.5697e+00,  ..., -1.9336e+00,\n",
       "           -2.0587e+00, -6.0919e-01],\n",
       "          [-2.0301e+00, -3.0309e-01, -1.4114e-01,  ..., -2.0391e+00,\n",
       "            2.7943e+00, -1.2829e+00],\n",
       "          [-1.0289e+00,  4.2850e-01, -1.3670e+00,  ..., -1.9002e+00,\n",
       "            2.0544e-01,  9.6090e-01],\n",
       "          ...,\n",
       "          [-1.6267e-01,  4.1146e-01, -2.0026e+00,  ..., -4.0780e-01,\n",
       "            2.7681e+00,  5.5887e-01],\n",
       "          [-1.6267e-01,  4.1146e-01, -2.0026e+00,  ..., -4.0780e-01,\n",
       "            2.7681e+00,  5.5887e-01],\n",
       "          [-1.6267e-01,  4.1146e-01, -2.0026e+00,  ..., -4.0780e-01,\n",
       "            2.7681e+00,  5.5887e-01]],\n",
       "\n",
       "         [[ 7.1554e-01, -1.8620e+00,  1.6600e+00,  ...,  2.1659e-01,\n",
       "            2.4271e+00, -1.0255e+00],\n",
       "          [-3.8306e-01,  1.5610e+00,  2.5032e+00,  ...,  1.4216e+00,\n",
       "            1.2157e+00, -4.8603e-02],\n",
       "          [-1.5535e+00, -5.0306e-01, -1.7077e+00,  ...,  8.9100e-01,\n",
       "           -9.6490e-01, -7.5562e-01],\n",
       "          ...,\n",
       "          [ 1.4076e+00, -2.1589e-01,  1.0704e-01,  ...,  4.2370e-01,\n",
       "           -2.4726e-01,  5.8753e-01],\n",
       "          [ 1.4076e+00, -2.1589e-01,  1.0704e-01,  ...,  4.2370e-01,\n",
       "           -2.4726e-01,  5.8753e-01],\n",
       "          [ 1.4076e+00, -2.1589e-01,  1.0704e-01,  ...,  4.2370e-01,\n",
       "           -2.4726e-01,  5.8753e-01]],\n",
       "\n",
       "         [[-6.9124e-01,  5.2344e-01, -8.6219e-01,  ...,  1.3128e+00,\n",
       "            5.3493e-01, -1.5872e-01],\n",
       "          [ 6.4754e-01,  1.3694e+00, -1.7099e+00,  ..., -3.2077e-01,\n",
       "            6.1959e-01, -1.2511e+00],\n",
       "          [ 9.5361e-01,  1.3370e+00, -6.9783e-01,  ..., -7.6089e-01,\n",
       "            1.7401e+00,  6.3318e-01],\n",
       "          ...,\n",
       "          [ 1.6391e+00,  1.0388e+00,  3.1271e+00,  ...,  3.2121e-01,\n",
       "            1.3036e+00, -3.7399e+00],\n",
       "          [ 1.6391e+00,  1.0388e+00,  3.1271e+00,  ...,  3.2121e-01,\n",
       "            1.3036e+00, -3.7399e+00],\n",
       "          [ 1.6391e+00,  1.0388e+00,  3.1271e+00,  ...,  3.2121e-01,\n",
       "            1.3036e+00, -3.7399e+00]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.0335e+00,  2.3987e+00,  5.0182e-01,  ..., -2.8025e+00,\n",
       "            2.4735e-01,  9.0754e+00],\n",
       "          [ 6.6036e-01,  4.2076e+00,  1.7145e+00,  ...,  7.6332e-01,\n",
       "            1.3083e-01, -1.1241e+00],\n",
       "          [ 1.5144e+00, -7.8639e-04,  4.3208e+00,  ...,  2.2834e+00,\n",
       "           -2.6512e+00, -2.9045e+00],\n",
       "          ...,\n",
       "          [-1.0723e+01,  3.5180e+00,  1.2270e+00,  ...,  2.0469e+00,\n",
       "            8.5516e+00,  5.3093e-01],\n",
       "          [-1.0723e+01,  3.5180e+00,  1.2270e+00,  ...,  2.0469e+00,\n",
       "            8.5516e+00,  5.3093e-01],\n",
       "          [-1.0723e+01,  3.5180e+00,  1.2270e+00,  ...,  2.0469e+00,\n",
       "            8.5516e+00,  5.3093e-01]],\n",
       "\n",
       "         [[-1.0807e+00,  3.0364e+00, -3.5398e+00,  ..., -4.3627e-01,\n",
       "           -1.2386e+01,  1.6324e+00],\n",
       "          [ 3.3347e+00,  1.5573e+00,  1.3621e+00,  ...,  1.7493e+00,\n",
       "           -3.1250e+00,  5.0796e+00],\n",
       "          [-2.0413e+00, -2.4668e+00, -4.4043e+00,  ..., -2.2412e+00,\n",
       "            8.2112e+00, -2.6040e+00],\n",
       "          ...,\n",
       "          [-7.8964e+00,  5.5659e+00, -2.6832e+00,  ...,  2.9779e+00,\n",
       "           -6.7243e+00, -3.3321e+00],\n",
       "          [-7.8964e+00,  5.5659e+00, -2.6832e+00,  ...,  2.9779e+00,\n",
       "           -6.7243e+00, -3.3321e+00],\n",
       "          [-7.8964e+00,  5.5659e+00, -2.6832e+00,  ...,  2.9779e+00,\n",
       "           -6.7243e+00, -3.3321e+00]],\n",
       "\n",
       "         [[ 6.1343e-01,  3.1074e+00,  9.3900e-01,  ..., -7.4625e+00,\n",
       "           -8.0546e+00, -4.5179e+00],\n",
       "          [-1.9562e+00,  1.0400e+00,  1.4236e+00,  ..., -1.6521e+00,\n",
       "           -6.4979e+00,  6.2507e-01],\n",
       "          [-2.7128e+00,  1.4162e+00,  6.4359e+00,  ..., -4.4398e+00,\n",
       "           -1.0812e+01, -3.5777e+00],\n",
       "          ...,\n",
       "          [-1.3702e+01, -1.1375e+01, -8.7379e+00,  ..., -7.3465e-01,\n",
       "           -1.4343e+01, -5.2782e-02],\n",
       "          [-1.3702e+01, -1.1375e+01, -8.7379e+00,  ..., -7.3465e-01,\n",
       "           -1.4343e+01, -5.2782e-02],\n",
       "          [-1.3702e+01, -1.1375e+01, -8.7379e+00,  ..., -7.3465e-01,\n",
       "           -1.4343e+01, -5.2782e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.8753e+00, -1.0353e+01, -7.2677e+00,  ..., -6.8928e+00,\n",
       "           -7.1667e+00, -2.2024e+00],\n",
       "          [ 7.5076e+00, -3.0006e+00,  5.5351e+00,  ..., -2.3646e+00,\n",
       "           -3.7507e+00, -5.0902e+00],\n",
       "          [-5.8150e-01,  3.6523e-01, -2.4106e+00,  ..., -3.8925e+00,\n",
       "            8.5450e-02,  3.2496e+00],\n",
       "          ...,\n",
       "          [-2.8229e+00, -6.8957e+00,  1.2839e+00,  ..., -4.3069e+00,\n",
       "            2.8566e+00, -1.6604e+00],\n",
       "          [-2.8229e+00, -6.8957e+00,  1.2839e+00,  ..., -4.3069e+00,\n",
       "            2.8566e+00, -1.6604e+00],\n",
       "          [-2.8229e+00, -6.8957e+00,  1.2839e+00,  ..., -4.3069e+00,\n",
       "            2.8566e+00, -1.6604e+00]],\n",
       "\n",
       "         [[-7.4887e+00,  2.8040e+00,  4.7810e+00,  ..., -4.4250e-01,\n",
       "            2.4669e+00,  1.5152e+00],\n",
       "          [-4.3051e+00, -2.4437e-01,  5.9024e+00,  ..., -1.2314e+00,\n",
       "            2.9662e-01, -7.3475e+00],\n",
       "          [-5.8483e+00, -1.7181e-01, -2.8861e+00,  ...,  3.8609e+00,\n",
       "           -1.3035e+00, -6.2693e+00],\n",
       "          ...,\n",
       "          [ 1.8668e+00, -4.7516e+00, -6.5139e+00,  ...,  7.6262e+00,\n",
       "            3.5604e+00,  3.9812e+00],\n",
       "          [ 1.8668e+00, -4.7516e+00, -6.5139e+00,  ...,  7.6262e+00,\n",
       "            3.5604e+00,  3.9812e+00],\n",
       "          [ 1.8668e+00, -4.7516e+00, -6.5139e+00,  ...,  7.6262e+00,\n",
       "            3.5604e+00,  3.9812e+00]],\n",
       "\n",
       "         [[ 7.6095e+00, -2.6786e+00,  2.3664e+00,  ...,  3.3909e+00,\n",
       "           -5.3746e+00, -1.5768e+00],\n",
       "          [-5.8369e+00,  8.4666e-01,  3.8247e-01,  ...,  6.4642e+00,\n",
       "            2.5516e+00, -3.1160e+00],\n",
       "          [ 2.1525e+00, -7.0811e-01, -1.2576e+00,  ...,  7.6923e-01,\n",
       "            5.2252e+00, -2.1315e+00],\n",
       "          ...,\n",
       "          [ 3.9426e+00, -6.7135e+00,  4.6262e+00,  ...,  5.3263e+00,\n",
       "            8.5238e-01,  2.3371e+00],\n",
       "          [ 3.9426e+00, -6.7135e+00,  4.6262e+00,  ...,  5.3263e+00,\n",
       "            8.5238e-01,  2.3371e+00],\n",
       "          [ 3.9426e+00, -6.7135e+00,  4.6262e+00,  ...,  5.3263e+00,\n",
       "            8.5238e-01,  2.3371e+00]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.1556,  0.2373,  2.3078,  ...,  0.7451, -1.3783, -0.4447],\n",
       "          [-1.3372,  0.2927,  1.3436,  ...,  1.1513, -1.1578, -0.0767],\n",
       "          [-0.3286, -2.5723, -0.4493,  ...,  3.0072,  2.3119, -0.9367],\n",
       "          ...,\n",
       "          [ 0.1519,  0.1351,  2.3651,  ...,  0.6933, -1.4817, -0.4118],\n",
       "          [ 0.1495,  0.1397,  2.3628,  ...,  0.6968, -1.4791, -0.4143],\n",
       "          [ 0.1478,  0.1436,  2.3604,  ...,  0.6983, -1.4768, -0.4149]],\n",
       "\n",
       "         [[-0.1810,  0.3186,  2.2527,  ..., -0.3022,  0.7700, -0.9794],\n",
       "          [-0.5187, -0.2648,  1.2329,  ...,  0.7086, -0.8459, -0.8133],\n",
       "          [-0.4360, -2.2234,  1.4193,  ...,  0.2220,  0.2271, -0.4259],\n",
       "          ...,\n",
       "          [-0.1062,  0.2791,  2.1992,  ..., -0.1908,  0.7208, -0.9495],\n",
       "          [-0.1108,  0.2794,  2.2021,  ..., -0.1943,  0.7215, -0.9499],\n",
       "          [-0.1126,  0.2832,  2.2044,  ..., -0.2001,  0.7231, -0.9508]],\n",
       "\n",
       "         [[-0.7793,  0.2268, -0.4340,  ..., -0.1160,  1.4548,  1.2194],\n",
       "          [-1.1908, -0.6602, -0.7322,  ...,  0.3397,  1.0975,  1.6165],\n",
       "          [ 1.3526, -0.5918, -0.9575,  ...,  0.5975,  1.0611,  1.0236],\n",
       "          ...,\n",
       "          [-0.7846,  0.1938, -0.2857,  ..., -0.1087,  1.4884,  1.1548],\n",
       "          [-0.7846,  0.1946, -0.2958,  ..., -0.1108,  1.4884,  1.1586],\n",
       "          [-0.7838,  0.1967, -0.3051,  ..., -0.1130,  1.4848,  1.1618]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8468,  0.2346,  0.2312,  ..., -0.5832, -0.3255,  1.1487],\n",
       "          [-0.3118, -0.3087, -0.3270,  ..., -0.5462, -1.0666, -0.8463],\n",
       "          [ 3.2189, -0.3854, -0.5164,  ..., -1.0495, -1.5430,  1.1321],\n",
       "          ...,\n",
       "          [ 0.9781,  0.2135,  0.2197,  ..., -0.5304, -0.4459,  1.1890],\n",
       "          [ 0.9702,  0.2157,  0.2200,  ..., -0.5334, -0.4402,  1.1860],\n",
       "          [ 0.9624,  0.2168,  0.2205,  ..., -0.5355, -0.4341,  1.1839]],\n",
       "\n",
       "         [[-1.7499,  0.6403,  0.1738,  ..., -0.4209,  1.3698,  0.3620],\n",
       "          [-2.2783,  0.6852, -0.7597,  ...,  0.7010,  0.4773,  0.1027],\n",
       "          [ 0.0132,  1.3961,  0.0363,  ..., -0.1644,  1.5928, -1.5093],\n",
       "          ...,\n",
       "          [-1.5829,  0.7206,  0.2292,  ..., -0.3429,  1.3433,  0.4800],\n",
       "          [-1.5916,  0.7179,  0.2253,  ..., -0.3456,  1.3447,  0.4759],\n",
       "          [-1.5999,  0.7135,  0.2231,  ..., -0.3490,  1.3447,  0.4709]],\n",
       "\n",
       "         [[ 1.0646,  1.0067,  0.5492,  ..., -0.2942,  1.6345, -0.1048],\n",
       "          [ 0.3641,  0.0059,  0.4842,  ...,  0.4251,  2.3381,  0.0770],\n",
       "          [-0.7622,  1.6608,  1.0014,  ...,  0.1076,  1.5111, -0.0748],\n",
       "          ...,\n",
       "          [ 1.0363,  0.9206,  0.4627,  ..., -0.2237,  1.4886, -0.0305],\n",
       "          [ 1.0389,  0.9243,  0.4674,  ..., -0.2269,  1.5011, -0.0333],\n",
       "          [ 1.0411,  0.9286,  0.4710,  ..., -0.2288,  1.5122, -0.0368]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 4.1846e-01,  5.0260e-01, -3.2760e-01,  ..., -2.5146e-01,\n",
       "            2.6307e-01, -3.4172e-01],\n",
       "          [ 1.6597e+00,  1.3930e+00,  5.4152e-01,  ..., -3.9795e-01,\n",
       "            2.0529e+00, -3.4253e-01],\n",
       "          [ 2.5519e+00,  3.5061e-01, -3.7620e+00,  ...,  1.1593e+00,\n",
       "            3.6351e+00, -5.4918e-01],\n",
       "          ...,\n",
       "          [ 3.6444e-01,  5.2567e-01, -2.2175e-01,  ..., -3.8896e-01,\n",
       "            2.4574e-01, -2.9439e-01],\n",
       "          [ 3.6898e-01,  5.2252e-01, -2.2529e-01,  ..., -3.8738e-01,\n",
       "            2.4712e-01, -2.9604e-01],\n",
       "          [ 3.7113e-01,  5.2278e-01, -2.3109e-01,  ..., -3.8560e-01,\n",
       "            2.4856e-01, -2.9960e-01]],\n",
       "\n",
       "         [[-9.0542e-01, -1.1891e+00,  9.4788e-01,  ..., -1.5614e+00,\n",
       "            9.7289e-01, -1.0965e+00],\n",
       "          [-2.2251e-01, -3.2102e+00,  1.4620e+00,  ..., -1.3098e+00,\n",
       "            1.4633e+00, -2.8386e+00],\n",
       "          [ 1.4139e+00, -4.3367e+00,  6.3955e+00,  ...,  1.6549e+00,\n",
       "            2.0473e+00, -1.5789e+00],\n",
       "          ...,\n",
       "          [-7.6272e-01, -1.0992e+00,  1.0143e+00,  ..., -1.2307e+00,\n",
       "            9.0549e-01, -8.9766e-01],\n",
       "          [-7.6929e-01, -1.1067e+00,  1.0189e+00,  ..., -1.2437e+00,\n",
       "            9.0934e-01, -9.0467e-01],\n",
       "          [-7.7425e-01, -1.1168e+00,  1.0130e+00,  ..., -1.2573e+00,\n",
       "            9.1086e-01, -9.1411e-01]],\n",
       "\n",
       "         [[ 2.4491e+00,  1.3212e+00, -1.7264e+00,  ..., -5.8796e-01,\n",
       "           -1.5159e-01, -4.3858e-01],\n",
       "          [ 1.4104e+00, -1.3312e+00, -1.4049e+00,  ..., -1.8108e+00,\n",
       "           -1.4399e+00, -1.7141e+00],\n",
       "          [ 2.6155e+00,  7.6315e-01, -4.5504e+00,  ..., -2.5734e+00,\n",
       "           -4.0195e+00, -3.3971e+00],\n",
       "          ...,\n",
       "          [ 2.5779e+00,  1.1824e+00, -1.7524e+00,  ..., -6.7369e-01,\n",
       "           -1.0022e-01, -7.0148e-01],\n",
       "          [ 2.5744e+00,  1.1882e+00, -1.7509e+00,  ..., -6.6675e-01,\n",
       "           -1.0491e-01, -6.8871e-01],\n",
       "          [ 2.5689e+00,  1.1959e+00, -1.7492e+00,  ..., -6.6020e-01,\n",
       "           -1.0848e-01, -6.7413e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8503e-01, -2.2626e-01,  1.6299e-01,  ...,  2.9335e-01,\n",
       "            8.2159e-01,  1.5710e+00],\n",
       "          [-8.3046e-01, -8.3063e-01, -1.1288e+00,  ...,  1.9254e+00,\n",
       "            1.1194e+00,  2.3563e+00],\n",
       "          [ 3.3859e+00, -7.4894e+00,  5.1807e+00,  ..., -1.2170e+00,\n",
       "           -2.4170e+00,  1.6113e+00],\n",
       "          ...,\n",
       "          [ 3.7164e-01, -3.4868e-01, -1.9227e-03,  ...,  2.1009e-01,\n",
       "            8.0645e-01,  1.6129e+00],\n",
       "          [ 3.7218e-01, -3.4005e-01,  4.8868e-03,  ...,  2.1917e-01,\n",
       "            8.0841e-01,  1.6100e+00],\n",
       "          [ 3.7081e-01, -3.3278e-01,  1.3911e-02,  ...,  2.2792e-01,\n",
       "            8.0859e-01,  1.6085e+00]],\n",
       "\n",
       "         [[-1.9053e-01, -1.6214e-01, -1.3204e+00,  ..., -5.1204e-01,\n",
       "            7.0385e-01,  1.5747e-01],\n",
       "          [ 1.2058e+00, -1.9145e+00, -2.5944e+00,  ..., -1.5778e+00,\n",
       "            4.2622e-02,  1.8101e+00],\n",
       "          [ 2.7616e+00, -1.2769e+00, -3.8002e+00,  ..., -7.9401e-01,\n",
       "           -4.2589e+00,  2.1492e+00],\n",
       "          ...,\n",
       "          [-2.0662e-01, -2.1102e-03, -1.2609e+00,  ..., -4.4227e-01,\n",
       "            7.2319e-01,  5.7376e-02],\n",
       "          [-2.0617e-01, -1.2558e-02, -1.2642e+00,  ..., -4.4434e-01,\n",
       "            7.2713e-01,  6.2201e-02],\n",
       "          [-2.0802e-01, -1.6222e-02, -1.2683e+00,  ..., -4.4726e-01,\n",
       "            7.2767e-01,  6.5191e-02]],\n",
       "\n",
       "         [[ 2.4613e-01, -9.4898e-01, -1.2056e+00,  ...,  1.3092e+00,\n",
       "            1.6011e+00,  2.6374e-01],\n",
       "          [-8.5101e-01, -1.9749e+00,  1.6852e-01,  ...,  1.8606e+00,\n",
       "            3.1005e+00, -2.0532e+00],\n",
       "          [-2.7279e-01,  9.3485e-01, -7.6542e-01,  ..., -1.4424e+00,\n",
       "            4.3966e-01, -5.9217e-01],\n",
       "          ...,\n",
       "          [ 3.0309e-01, -7.4855e-01, -1.3415e+00,  ...,  1.2902e+00,\n",
       "            1.5832e+00,  1.8658e-01],\n",
       "          [ 3.0705e-01, -7.5699e-01, -1.3317e+00,  ...,  1.2932e+00,\n",
       "            1.5901e+00,  1.8877e-01],\n",
       "          [ 3.1131e-01, -7.6813e-01, -1.3268e+00,  ...,  1.2937e+00,\n",
       "            1.5917e+00,  1.9650e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.7708,  0.7708,  1.5353,  ...,  2.1278, -0.9400, -2.0667],\n",
       "          [ 1.9804, -1.1511,  1.3387,  ...,  0.7644, -0.5766, -0.3592],\n",
       "          [-0.0820, -2.3522, -0.8170,  ..., -1.0936,  1.2622,  1.3281],\n",
       "          ...,\n",
       "          [ 1.1051, -0.6634, -2.0440,  ...,  2.8984, -1.0867,  1.7273],\n",
       "          [ 1.1051, -0.6634, -2.0440,  ...,  2.8984, -1.0867,  1.7273],\n",
       "          [ 1.1051, -0.6634, -2.0440,  ...,  2.8984, -1.0867,  1.7273]],\n",
       "\n",
       "         [[ 0.9467,  0.3817, -1.7287,  ...,  1.5690,  0.1236, -1.7690],\n",
       "          [ 0.2779,  0.1599,  0.4587,  ...,  0.6131,  1.0842, -0.4364],\n",
       "          [ 0.0584,  0.6163, -0.9161,  ..., -0.3089,  0.4885, -1.9600],\n",
       "          ...,\n",
       "          [-1.3374,  2.1939, -2.3232,  ...,  1.5605, -0.0042, -1.2707],\n",
       "          [-1.3374,  2.1939, -2.3232,  ...,  1.5605, -0.0042, -1.2707],\n",
       "          [-1.3374,  2.1939, -2.3232,  ...,  1.5605, -0.0042, -1.2707]],\n",
       "\n",
       "         [[-1.5349,  2.2043, -0.3699,  ...,  2.4596,  1.8237, -0.3932],\n",
       "          [ 1.4730,  2.2420, -1.1422,  ..., -0.8931, -0.8208,  0.3325],\n",
       "          [-1.1342,  0.2145, -1.3438,  ...,  0.6204, -0.9742, -0.0961],\n",
       "          ...,\n",
       "          [ 0.8284, -0.0485,  0.4055,  ..., -2.9185, -1.2771, -1.6688],\n",
       "          [ 0.8284, -0.0485,  0.4055,  ..., -2.9185, -1.2771, -1.6688],\n",
       "          [ 0.8284, -0.0485,  0.4055,  ..., -2.9185, -1.2771, -1.6688]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8220,  1.7010,  0.7013,  ...,  1.9675,  0.8415, -1.5467],\n",
       "          [ 0.6225,  1.0114, -0.6646,  ..., -0.5645,  1.6966, -2.8573],\n",
       "          [ 1.2818,  0.3618, -1.9958,  ...,  1.7823,  1.7257, -0.9889],\n",
       "          ...,\n",
       "          [ 0.0854, -2.3329,  0.7770,  ..., -2.0186,  0.2542,  1.7271],\n",
       "          [ 0.0854, -2.3329,  0.7770,  ..., -2.0186,  0.2542,  1.7271],\n",
       "          [ 0.0854, -2.3329,  0.7770,  ..., -2.0186,  0.2542,  1.7271]],\n",
       "\n",
       "         [[ 0.5924, -0.0374, -0.0919,  ...,  1.2161,  0.2236,  0.8307],\n",
       "          [ 0.6344, -0.1190, -2.6556,  ..., -0.0568,  0.4000, -0.9984],\n",
       "          [-0.6917,  0.0815, -1.3579,  ..., -1.2578, -1.9956, -2.1532],\n",
       "          ...,\n",
       "          [-1.8405,  0.8667,  0.3445,  ..., -0.1043,  0.7947,  0.0035],\n",
       "          [-1.8405,  0.8667,  0.3445,  ..., -0.1043,  0.7947,  0.0035],\n",
       "          [-1.8405,  0.8667,  0.3445,  ..., -0.1043,  0.7947,  0.0035]],\n",
       "\n",
       "         [[ 1.1167,  0.7529, -0.5238,  ..., -0.1088, -0.8775, -0.7258],\n",
       "          [ 1.2083, -0.5250, -2.0728,  ..., -0.5209, -1.1971,  1.4535],\n",
       "          [ 0.8639,  0.8696, -1.4571,  ..., -0.6656, -1.9856, -0.0958],\n",
       "          ...,\n",
       "          [-0.9903, -2.5163, -0.1162,  ..., -1.0086,  1.5541, -0.0814],\n",
       "          [-0.9903, -2.5163, -0.1162,  ..., -1.0086,  1.5541, -0.0814],\n",
       "          [-0.9903, -2.5163, -0.1162,  ..., -1.0086,  1.5541, -0.0814]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[  0.4574,   3.7006,   7.1258,  ...,  -5.3154, -10.2106,   1.5350],\n",
       "          [  2.8131,   0.1075,   3.3762,  ...,  -6.7424,   8.4522,  -4.6122],\n",
       "          [ -0.2228,   2.6157,   4.0730,  ...,   0.1121,  -2.4647,  -1.8079],\n",
       "          ...,\n",
       "          [  7.4581,  -3.0322,   2.4019,  ...,  10.3900,  -6.3525,  -4.1618],\n",
       "          [  7.4581,  -3.0322,   2.4019,  ...,  10.3900,  -6.3525,  -4.1618],\n",
       "          [  7.4581,  -3.0322,   2.4019,  ...,  10.3900,  -6.3525,  -4.1618]],\n",
       "\n",
       "         [[  3.0967,  -4.9672,  11.8593,  ...,   6.5858,   0.9473,  -3.4717],\n",
       "          [ -0.8096,  -3.3542,   5.3232,  ...,  -1.8424,   3.1854,   1.8876],\n",
       "          [ -1.5716,  -6.5769,   6.0471,  ...,  -4.2105,   3.1682,   2.8754],\n",
       "          ...,\n",
       "          [  6.5704,   2.6160,  -8.7626,  ...,   3.3844,   9.1574,  11.3323],\n",
       "          [  6.5704,   2.6160,  -8.7626,  ...,   3.3844,   9.1574,  11.3323],\n",
       "          [  6.5704,   2.6160,  -8.7626,  ...,   3.3844,   9.1574,  11.3323]],\n",
       "\n",
       "         [[  1.4954, -22.6312,   5.9055,  ...,  -0.1790,   4.6768,  -2.4046],\n",
       "          [ -4.7870,  -0.4937,   7.3151,  ...,  -1.9616,   2.9213,  -8.5903],\n",
       "          [ -0.8480,   7.1570,  -6.0517,  ...,  -1.6780,  -2.3460,  -1.0727],\n",
       "          ...,\n",
       "          [  9.3178,  -7.5161,   3.5195,  ...,  11.3209,  -6.4557,  15.1093],\n",
       "          [  9.3178,  -7.5161,   3.5195,  ...,  11.3209,  -6.4557,  15.1093],\n",
       "          [  9.3178,  -7.5161,   3.5195,  ...,  11.3209,  -6.4557,  15.1093]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -0.5479,  -6.8697,  -8.5288,  ...,  15.5036,   7.6897,  -2.6049],\n",
       "          [ 10.7219,   0.5208,  -7.5017,  ...,   2.3853,  -5.0525,   3.2709],\n",
       "          [  4.7287,   1.5326,   4.2757,  ...,  -0.4238,   1.4899,   5.7983],\n",
       "          ...,\n",
       "          [  5.7311,   4.7209,   8.0670,  ...,  16.0696,   0.8948,   1.1407],\n",
       "          [  5.7311,   4.7209,   8.0670,  ...,  16.0696,   0.8948,   1.1407],\n",
       "          [  5.7311,   4.7209,   8.0670,  ...,  16.0696,   0.8948,   1.1407]],\n",
       "\n",
       "         [[  0.9691,  -6.7368,  -4.3377,  ...,   0.8435,  -1.7670,   2.1254],\n",
       "          [  7.4499,   9.6458,  -5.0655,  ...,   5.6921,   2.7223,  -7.6475],\n",
       "          [ -3.9628,   1.3543,   7.0163,  ...,  -5.9696,   1.1464,  -4.6833],\n",
       "          ...,\n",
       "          [-10.4726,   1.4343,  17.3871,  ...,   2.7958,  10.7699,   7.6744],\n",
       "          [-10.4726,   1.4343,  17.3871,  ...,   2.7958,  10.7699,   7.6744],\n",
       "          [-10.4726,   1.4343,  17.3871,  ...,   2.7958,  10.7699,   7.6744]],\n",
       "\n",
       "         [[ -4.6699,  -1.1807,   7.5083,  ...,  16.4630,   0.0459,  -0.4406],\n",
       "          [  3.3299,  -4.4375,   2.8584,  ...,  -0.8068,  -4.9254,  -5.5661],\n",
       "          [  3.4286,  -1.8248,   1.3359,  ...,   8.9020,  -4.8898,   0.3777],\n",
       "          ...,\n",
       "          [ 10.0918,  -7.8622,  -2.3160,  ..., -18.9946,   7.8585,   3.0769],\n",
       "          [ 10.0918,  -7.8622,  -2.3160,  ..., -18.9946,   7.8585,   3.0769],\n",
       "          [ 10.0918,  -7.8622,  -2.3160,  ..., -18.9946,   7.8585,   3.0769]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.1670, -0.0549, -0.0678,  ..., -0.0046, -0.1266, -0.0658],\n",
       "         [ 0.1413, -0.0544,  0.1294,  ...,  0.0837, -0.0265,  0.0722],\n",
       "         [-0.1379,  0.1142,  0.0339,  ...,  0.0827,  0.1169, -0.0465],\n",
       "         ...,\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025],\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025],\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_ids = test_batch[\"input_ids\"].view(test_batch[\"input_ids\"].size(0), -1).cuda()\n",
    "input_ids = test_batch[\"input_ids\"].cuda()\n",
    "attention_mask = test_batch[\"attention_mask\"].cuda()\n",
    "labels = test_batch[\"labels\"].cuda()\n",
    "model(input_ids, attention_mask, labels=labels)\n",
    "#model.encoder(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 448])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = test_batch[\"input_ids\"].view(1 * 1, 448).cuda()\n",
    "attention_mask = test_batch[\"attention_mask\"].view(1 * 1, 448).cuda()\n",
    "result = model.encoder.encoder(input_ids, attention_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.1670, -0.0549, -0.0678,  ..., -0.0046, -0.1266, -0.0658],\n",
       "         [ 0.1413, -0.0544,  0.1294,  ...,  0.0837, -0.0265,  0.0722],\n",
       "         [-0.1379,  0.1142,  0.0339,  ...,  0.0827,  0.1169, -0.0465],\n",
       "         ...,\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025],\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025],\n",
       "         [-0.0466,  0.0035,  0.0482,  ..., -0.1274, -0.0180, -0.0025]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (loss, decoder_logits, decoder hidden states, encoder state)\n",
    "result[2][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = test_batch[\"input_ids\"].cuda()\n",
    "attn_mask = test_batch[\"attention_mask\"].cuda()\n",
    "with torch.no_grad():\n",
    "    transformers.set_seed(1)\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        attn_mask,\n",
    "        64,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        #num_beams=10,\n",
    "        length_penalty=0,\n",
    "        num_return_sequences=10,\n",
    "    )\n",
    "    #tokens = model.generate(test_batch[\"input_ids\"].cuda(), test_batch[\"attention_mask\"].cuda(), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SampleEncoderDecoderOutput(sequences=tensor([[   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1],\n",
       "        [   0,  879, 1737, 8976,  276,    5,    1]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        ...,\n",
       "        [     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -157.7079,  ...,      -inf,      -inf,\n",
       "              -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        ...,\n",
       "        [     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,      -inf, -154.7156,  ...,      -inf,      -inf,\n",
       "              -inf]], device='cuda:0'), tensor([[     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        ...,\n",
       "        [     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf],\n",
       "        [     -inf,  -16.1576, -205.6667,  ...,      -inf,      -inf,\n",
       "              -inf]], device='cuda:0')), encoder_attentions=None, encoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, decoder_hidden_states=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>',\n",
       " '<pad> generalize dependent P.</s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [182,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/coq-modeling/venv/lib/python3.10/site-packages/torch/_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "outputs.scores[0][outputs.sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576],\n",
       "        [-135.2088,   49.1707,   20.9431,  -10.5498,  -59.5263,  -16.1576]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_transition_scores(outputs.sequences, outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "where() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor condition, Tensor other)\n      didn't match because some of the arguments have invalid types: (!int!, !int!)\n * (Tensor condition, Number other)\n      didn't match because some of the arguments have invalid types: (!int!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m the_bools \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m((outputs\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id) \u001b[38;5;241m+\u001b[39m (outputs\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#torch.where(the_bools, 1, 0).sum(axis=1)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mthe_bools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: where() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor condition, Tensor other)\n      didn't match because some of the arguments have invalid types: (!int!, !int!)\n * (Tensor condition, Number other)\n      didn't match because some of the arguments have invalid types: (!int!, !int!)\n"
     ]
    }
   ],
   "source": [
    "the_bools = ~((outputs.sequences == tokenizer.pad_token_id) + (outputs.sequences == tokenizer.eos_token_id))\n",
    "#torch.where(the_bools, 1, 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8814, -2.6457, -2.8369, -3.1251, -3.9642, -4.0253, -4.5893, -4.9442,\n",
       "        -5.1175, -5.3550], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"beam_indices\" in outputs.keys():\n",
    "    seq_scores = output.sequences_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0924e+00, -5.3725e-04, -2.0845e-01, -3.4957e-01, -4.6140e-02,\n",
       "         -1.5571e-03, -3.8826e-02, -1.6582e-01, -7.4209e-01, -2.8463e-04,\n",
       "         -4.6652e+01, -4.4954e+01, -3.9539e+01, -4.7255e+01, -4.5628e+01,\n",
       "         -3.8009e+01, -3.0419e+01, -3.6212e+01, -4.0046e+01, -4.7286e+01,\n",
       "         -4.5628e+01, -3.9329e+01, -4.7004e+01, -4.0645e+01, -3.8275e+01,\n",
       "         -3.0197e+01, -3.6068e+01, -3.9653e+01, -4.6780e+01, -4.5773e+01,\n",
       "         -3.8774e+01, -4.1612e+01, -4.1023e+01, -3.8042e+01, -2.9946e+01,\n",
       "         -3.6146e+01, -3.9438e+01, -4.6120e+01, -4.5308e+01, -3.8505e+01,\n",
       "         -4.1189e+01, -4.1466e+01, -3.7899e+01, -2.9849e+01, -3.6151e+01,\n",
       "         -3.9236e+01, -4.1129e+01, -4.4634e+01, -3.8401e+01, -4.0940e+01,\n",
       "         -4.1558e+01, -3.7693e+01, -2.9866e+01, -3.6126e+01, -3.8958e+01,\n",
       "         -4.0932e+01, -4.3907e+01, -3.8285e+01, -4.0571e+01, -4.2507e+01,\n",
       "         -3.7361e+01, -3.0210e+01, -3.6002e+01],\n",
       "        [-1.0924e+00, -1.1111e+01, -1.9473e+01, -1.3342e+01, -1.5176e+01,\n",
       "         -1.6490e+01, -6.9915e+00, -1.0181e+01, -9.1928e+00, -1.1674e+00,\n",
       "         -8.5519e+00, -1.9711e+01, -1.0445e+01, -9.7668e+00, -4.0242e+01,\n",
       "         -4.5499e+01, -4.6634e+01, -3.6907e+01, -4.0086e+01, -4.1955e+01,\n",
       "         -4.5082e+01, -3.8946e+01, -4.2152e+01, -4.2381e+01, -4.4482e+01,\n",
       "         -4.6099e+01, -3.1922e+01, -3.9669e+01, -4.1567e+01, -4.4825e+01,\n",
       "         -3.9058e+01, -3.3959e+01, -4.1496e+01, -4.4365e+01, -4.5703e+01,\n",
       "         -3.2471e+01, -3.9456e+01, -4.1287e+01, -4.1264e+01, -3.8946e+01,\n",
       "         -3.3911e+01, -4.0749e+01, -4.4506e+01, -4.5485e+01, -3.2817e+01,\n",
       "         -3.9241e+01, -4.5841e+01, -4.0389e+01, -3.8806e+01, -3.3919e+01,\n",
       "         -4.0150e+01, -4.4803e+01, -4.5591e+01, -3.3450e+01, -3.8967e+01,\n",
       "         -4.5374e+01, -3.9806e+01, -3.8732e+01, -3.4113e+01, -3.9967e+01,\n",
       "         -4.7068e+01, -2.9687e+01, -3.7913e+01],\n",
       "        [-1.0924e+00, -1.3500e+01, -4.1466e+00, -1.0608e-02, -6.1835e+00,\n",
       "         -1.6132e+01, -8.5768e+00, -1.1907e+01, -1.2902e+01, -7.3657e+00,\n",
       "         -1.6044e+01, -9.2132e+00, -1.9033e+01, -6.3315e+00, -1.1280e+01,\n",
       "         -1.2927e+01, -2.1541e+01, -7.9809e+00, -8.6432e+00, -1.5900e+01,\n",
       "         -8.3181e+00, -1.8190e+01, -4.2338e-02, -1.5797e+01, -1.1760e+01,\n",
       "         -2.1183e+01, -1.1849e-01, -9.1739e+00, -1.5998e+01, -2.0977e-02,\n",
       "         -1.8468e+01, -1.4074e+01, -1.5566e+01, -1.0625e+01, -2.0734e+01,\n",
       "         -1.1775e-01, -9.2115e+00, -1.5999e+01, -1.3643e+01, -1.8508e+01,\n",
       "         -1.3319e+01, -1.5355e+01, -9.6956e+00, -4.6674e-01, -1.2714e-01,\n",
       "         -9.2041e+00, -1.8070e-04, -1.3239e+01, -1.7810e+01, -1.2823e+01,\n",
       "         -1.1312e+01, -2.2109e-01, -4.8931e-01, -1.2988e-01, -9.3395e+00,\n",
       "         -2.6997e-04, -1.2972e+01, -1.7141e+01, -1.2409e+01, -1.0365e+01,\n",
       "         -4.3774e-01, -2.0004e+01, -6.4060e+00],\n",
       "        [-1.0924e+00, -1.5305e+01, -1.5956e+01, -1.1466e-02, -5.7228e+00,\n",
       "         -1.5411e+01, -1.1168e+01, -1.8631e+01, -1.0152e+01, -5.8190e+00,\n",
       "         -2.5670e+01, -8.6552e+00, -3.5645e-01, -6.2145e-02, -1.5887e+01,\n",
       "         -1.1300e+01, -1.6546e+01, -1.9836e+01, -1.8649e+01, -2.5278e+01,\n",
       "         -2.0772e-02, -1.8866e+01, -1.3759e+01, -2.6757e+01, -1.0516e+01,\n",
       "         -1.5883e+01, -1.8294e+01, -1.8172e+01, -1.3851e-04, -7.9346e+00,\n",
       "         -1.9277e+01, -1.3070e+01, -1.1694e+01, -9.8386e+00, -1.5270e+01,\n",
       "         -1.7005e+01, -1.7715e+01, -1.5007e-04, -2.2251e+01, -1.8004e+01,\n",
       "         -1.2257e+01, -1.1475e+01, -2.0466e-01, -2.0237e+01, -1.5872e+01,\n",
       "         -2.0730e+00, -1.5988e+01, -2.1074e+01, -1.7720e+01, -1.1900e+01,\n",
       "         -1.5123e+01, -8.9800e+00, -1.4441e+01, -1.4702e+01, -1.5457e+00,\n",
       "         -5.9845e+00, -2.8781e+01, -4.3904e-01, -1.3219e-04, -3.9391e+01,\n",
       "         -3.9152e+01, -3.9250e+01, -4.6189e+01],\n",
       "        [-1.0924e+00, -1.8360e+01, -5.3760e+00, -1.2830e+01, -1.0697e+01,\n",
       "         -1.6585e+01, -1.1995e+01, -2.8958e+01, -1.3046e+01, -9.7815e+00,\n",
       "         -2.0714e+01, -1.0791e+01, -1.3851e+01, -1.4576e+01, -2.7351e+01,\n",
       "         -1.8119e+01, -1.2698e+01, -1.9905e+01, -1.3025e+01, -1.4578e-04,\n",
       "         -9.9619e+00, -1.9849e+01, -5.7848e+00, -1.1950e+01, -1.1254e+01,\n",
       "         -2.0700e+01, -1.9559e+01, -2.2042e+00, -1.1881e+01, -9.2990e+00,\n",
       "         -1.8032e+01, -5.5323e+00, -2.5780e+01, -1.0927e+01, -1.2714e+01,\n",
       "         -1.8681e+01, -6.9886e+00, -1.1757e+01, -8.8267e+00, -1.7777e+01,\n",
       "         -5.3205e+00, -9.6585e+00, -9.2500e+00, -1.4831e+01, -1.0629e-01,\n",
       "         -1.7363e+01, -1.1763e+01, -8.3037e+00, -1.5461e+01, -5.0886e+00,\n",
       "         -9.5717e+00, -8.7835e+00, -1.9743e+01, -1.0770e-01, -1.2878e+01,\n",
       "         -1.5680e+01, -1.7806e+01, -1.0276e+01, -6.0179e+00, -4.5221e+01,\n",
       "         -4.0486e+01, -3.5138e+01, -3.5508e+01],\n",
       "        [-1.0924e+00, -1.7718e+01, -5.9517e+00, -1.0626e+01, -3.6170e+00,\n",
       "         -5.1467e+00, -1.5388e+01, -1.4600e+01, -9.0396e-01, -9.7414e+00,\n",
       "         -2.1363e+01, -2.7249e-02, -2.0170e+01, -3.6730e+00, -1.1424e+01,\n",
       "         -1.7893e+01, -1.2928e+01, -2.0646e+01, -2.4278e+00, -1.2151e+01,\n",
       "         -2.3626e+01, -1.1742e+01, -1.2793e+01, -1.8647e+01, -2.6936e+01,\n",
       "         -1.2814e+01, -8.1013e-02, -6.8856e+00, -7.6355e+00, -2.2956e+01,\n",
       "         -1.1742e+01, -1.1948e+01, -9.7516e+00, -2.5920e+01, -1.7756e+01,\n",
       "         -7.8063e-02, -2.1066e+00, -7.6420e+00, -7.6405e+00, -1.5663e+01,\n",
       "         -1.1206e+01, -1.2939e-01, -2.4838e+01, -1.2653e+01, -8.1851e-02,\n",
       "         -7.0709e+00, -7.7338e+00, -7.4090e+00, -1.7506e+01, -1.0641e+01,\n",
       "         -1.4713e-01, -2.3707e+01, -1.2083e+01, -8.7491e-02, -1.3047e+01,\n",
       "         -3.6205e+00, -2.4493e+01, -3.3090e-01, -1.4938e+01, -8.8416e+00,\n",
       "         -8.4817e-01, -3.6253e+01, -4.3312e+01],\n",
       "        [-1.3093e+00, -1.4324e+01, -9.1180e+00, -1.2249e+01, -1.6928e-05,\n",
       "         -3.9581e+01, -3.9023e+01, -4.4805e+01, -3.8262e+01, -3.6638e+01,\n",
       "         -5.7185e+01, -4.2010e+01, -3.7517e+01, -3.5929e+01, -4.0739e+01,\n",
       "         -4.3404e+01, -3.6721e+01, -4.9525e+01, -4.2736e+01, -5.7223e+01,\n",
       "         -5.7791e+01, -4.9776e+01, -4.4271e+01, -4.0667e+01, -4.5277e+01,\n",
       "         -3.6326e+01, -4.8965e+01, -4.3459e+01, -5.7537e+01, -5.8008e+01,\n",
       "         -3.6982e+01, -4.3772e+01, -5.6350e+01, -4.0054e+01, -4.0336e+01,\n",
       "         -3.4110e+01, -3.9254e+01, -5.4700e+01, -5.9279e+01, -3.6881e+01,\n",
       "         -4.3409e+01, -5.0076e+01, -4.1242e+01, -3.9777e+01, -4.3831e+01,\n",
       "         -3.9282e+01, -5.3254e+01, -5.8788e+01, -3.7081e+01, -4.3095e+01,\n",
       "         -4.3281e+01, -4.0881e+01, -4.2027e+01, -4.3972e+01, -3.9025e+01,\n",
       "         -5.1740e+01, -4.3468e+01, -4.8663e+01, -4.2750e+01, -4.6127e+01,\n",
       "         -4.0185e+01, -4.1224e+01, -4.0193e+01],\n",
       "        [-1.0924e+00, -2.3120e+01, -1.3117e+01, -2.0541e+01, -1.3966e+01,\n",
       "         -6.5948e+00, -1.0721e+01, -1.2352e+00, -1.3775e+01, -1.8982e+01,\n",
       "         -1.6044e-04, -1.5368e+01, -3.1885e+00, -1.7978e+01, -1.9056e+01,\n",
       "         -1.1224e+01, -2.8704e-01, -1.0132e-01, -2.4460e+00, -7.6626e+00,\n",
       "         -1.4983e+01, -1.6092e+01, -5.3605e+00, -9.3315e+00, -9.8873e+00,\n",
       "         -1.1933e+01, -1.2431e-01, -2.1746e+00, -3.5563e+01, -1.8116e+01,\n",
       "         -1.4298e+01, -1.4521e+01, -1.2542e+01, -1.0141e+01, -3.0139e-01,\n",
       "         -1.2033e-01, -1.1787e+01, -2.4654e+01, -1.8760e+01, -1.4354e+01,\n",
       "         -1.4716e-01, -1.2702e+01, -8.7710e+00, -3.0851e-01, -1.2984e-01,\n",
       "         -1.5464e+00, -1.6726e+01, -2.3263e+01, -1.4119e+01, -3.8766e+00,\n",
       "         -4.8993e+01, -3.9283e+01, -3.0064e+01, -3.4356e+01, -3.8702e+01,\n",
       "         -5.4740e+01, -5.6916e+01, -4.3942e+01, -3.9128e+01, -4.0564e+01,\n",
       "         -4.7324e+01, -3.5192e+01, -4.2155e+01],\n",
       "        [-1.0924e+00, -1.8841e+01, -1.3087e+01, -9.2460e+00, -1.8814e+00,\n",
       "         -9.2148e+00, -1.3788e+01, -7.2602e+00, -1.9601e+01, -1.9460e+01,\n",
       "         -8.0683e+00, -2.3930e+01, -1.4813e+00, -4.5042e+00, -1.0072e+01,\n",
       "         -2.7894e+01, -1.1532e+01, -1.6504e-01, -2.3923e+00, -2.5329e+01,\n",
       "         -1.8516e+01, -1.4703e+01, -1.5090e+01, -1.2180e+01, -1.0431e+01,\n",
       "         -2.9748e-01, -1.2575e-01, -2.2203e+00, -2.5179e+01, -1.4679e+01,\n",
       "         -1.3166e+01, -2.7118e+00, -7.2501e+00, -1.7361e+01, -1.2212e+01,\n",
       "         -2.1427e+01, -2.1234e+00, -2.4831e+01, -1.4426e+01, -8.6356e+00,\n",
       "         -1.3959e+01, -1.8206e+01, -9.9372e+00, -1.7662e+01, -1.2645e-01,\n",
       "         -1.5938e+00, -1.7166e-05, -1.1031e+01, -6.4363e+00, -2.4572e+00,\n",
       "         -1.3486e+00, -6.8933e+00, -4.1377e+01, -3.4985e+01, -3.8975e+01,\n",
       "         -4.8284e+01, -4.2895e+01, -3.8014e+01, -2.9149e+01, -3.7592e+01,\n",
       "         -5.5874e+01, -3.2022e+01, -3.4985e+01],\n",
       "        [-1.0924e+00, -4.2769e+01, -1.3869e+01, -1.0743e+01, -9.9497e+00,\n",
       "         -1.0119e+01, -1.0142e+01, -1.4065e+01, -1.8649e+01, -1.0515e+01,\n",
       "         -1.6978e+01, -1.3514e+01, -7.2171e+00, -1.4865e+01, -1.2265e+01,\n",
       "         -1.0900e+01, -9.5324e-01, -1.6528e-01, -1.8213e+00, -2.4010e+01,\n",
       "         -1.6054e+01, -8.7305e+00, -6.3835e+00, -1.6121e+01, -9.2488e+00,\n",
       "         -1.2259e+01, -6.8334e-02, -1.5985e+00, -1.1774e+01, -2.1425e+01,\n",
       "         -8.6985e+00, -6.4452e+00, -1.1284e+01, -1.0568e+01, -1.2623e+01,\n",
       "         -6.5390e-02, -2.1558e+00, -1.1410e+01, -2.2869e+00, -2.5340e+01,\n",
       "         -6.3709e+00, -6.9112e+00, -1.0368e+01, -1.2580e+01, -7.5148e+00,\n",
       "         -1.8940e+00, -1.9705e+01, -2.0075e+01, -1.9807e+01, -6.8404e+00,\n",
       "         -5.4924e+01, -3.2782e+01, -4.0925e+01, -3.2514e+01, -3.9196e+01,\n",
       "         -4.0972e+01, -4.1007e+01, -5.0059e+01, -3.3679e+01, -4.7172e+01,\n",
       "         -5.7779e+01, -4.1799e+01, -3.5341e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_transition_scores(outputs.sequences, outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SampleEncoderDecoderOutput(sequences=tensor([[    0,    20,  7593,    41,     9,     3,     2,    58,   305,   137,\n",
       "             1,     0,     0,     0,     0],\n",
       "        [    0,  1581, 23966,   834,   994,     9,  9208,  4416,     1,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,    20,  7593,    41,     9,     3,     2,    58,   431,  3670,\n",
       "             3,     7, 10296,     5,     1],\n",
       "        [    0,    20,  7593,    41,     9,     3,     2,    58,   305,  3670,\n",
       "           653, 22112, 10696,     5,     1],\n",
       "        [    0,    20,  7593,    41,     9,     3,     2,    58,   305,   137,\n",
       "             1,     0,     0,     0,     0],\n",
       "        [    0,     3,     7, 10296,     5,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,     3,     7, 10296,     5,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,    20,  7593,    41,     9,     3,     2,    58,   305,  3670,\n",
       "             3,     7, 10296,     5,     1],\n",
       "        [    0,  1581,  3548,   834,   994,     9,  9208,  4416,     1,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,     3,     7, 10296,     5,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -36.2150,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -36.5866, -62.7301,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -31.4300,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -35.9882, -61.4466,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -36.2846, -62.5405,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf, -33.7223,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -34.6087,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -32.6582,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -33.7223,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -34.0223,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf, -35.3661, -62.3617,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf, -25.6777,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -42.7428,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -42.8935,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf, -41.5163,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf,     -inf, -41.7211,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf, -38.0824, -64.7982,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [    -inf, -38.1445, -64.8947,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "       device='cuda:0')), encoder_attentions=None, encoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, decoder_hidden_states=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -38.5619,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -38.5619,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -45.5157,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -40.4051,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -44.0177,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -33.3081,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -44.7971,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -32.7106,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -39.9439,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -35.0436, -61.8926,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -36.6447,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -19.4428,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -31.1613,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -34.0547,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -37.2592, -64.7431,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -36.7279,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -20.1071,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -35.1474,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -37.3701,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -36.2431,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf, -33.7223,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -33.7223,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -33.7223,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf, -35.3661, -62.3617,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf, -25.6777,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -42.8935,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -40.5001,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -42.8935,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf, -41.7211,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -35.1384, -61.6411,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf, -41.7211,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -27.9596,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf, -38.1445, -64.8947,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf, -28.2162,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         ...,\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf, -35.0019, -63.1922,  ...,     -inf,     -inf,     -inf]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simpl.',\n",
       " 'destruct (a? 5).',\n",
       " 'induction a.',\n",
       " 'destruct a.',\n",
       " 'lia.',\n",
       " 'case a.',\n",
       " 'destruct (a? 5); simpl.',\n",
       " 'destruct (a? 5); auto.',\n",
       " 'destruct (a? 5); trivial.',\n",
       " 'destruct (a? 5); reflexivity.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', '', '<', '=', '', '', 'there']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hi <=  there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleResult:\n",
    "    def __init__(\n",
    "        self, tactics: list[str], scores: list[float], num_tokens: list[int]\n",
    "    ) -> None:\n",
    "        self.tactics = tactics\n",
    "        self.scores = scores\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "    def to_json(self) -> Any:\n",
    "        return {\n",
    "            \"tactics\": self.tactics,\n",
    "            \"scores\": self.scores,\n",
    "            \"num_tokens\": self.num_tokens,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_data: Any) -> SampleResult:\n",
    "        tactics = json_data[\"tactics\"]\n",
    "        scores = json_data[\"scores\"]\n",
    "        num_tokens = json_data[\"num_tokens\"]\n",
    "        return cls(tactics, scores, num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fuzzy_starts_with(s1: str, s2: str) -> bool:\n",
    "    \"\"\"some nonempty prefix of s1 matches some nonempty suffix of s2\"\"\"\n",
    "    if len(s2) == 0:\n",
    "        return False\n",
    "    if s1.startswith(s2):\n",
    "        return True\n",
    "    return fuzzy_starts_with(s1, s2[1:])\n",
    "\n",
    "\n",
    "def should_stop_now(\n",
    "    input_ids: torch.Tensor,\n",
    "    tokenizer: CodeLlamaTokenizer | T5Tokenizer,\n",
    "    stop_strings: list[str],\n",
    ") -> bool:\n",
    "    \"\"\"input ids is a one dimensional tensor\"\"\"\n",
    "    consider_len = 1\n",
    "    cur_candidate = tokenizer.decode(input_ids[(-1 * consider_len) :])\n",
    "    any_matched = True\n",
    "    while any_matched:\n",
    "        any_matched = False\n",
    "        for stop_string in stop_strings:\n",
    "            if stop_string in cur_candidate:\n",
    "                return True\n",
    "            any_matched |= fuzzy_starts_with(cur_candidate, stop_string)\n",
    "        consider_len += 1\n",
    "        cur_candidate = tokenizer.decode(input_ids[(-1 * consider_len) :])\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches_fid(\n",
    "    input_ids: torch.LongTensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    beam_scores: torch.Tensor,\n",
    "    batch_size: int,\n",
    ") -> tuple[tuple[torch.LongTensor], tuple[torch.Tensor], tuple[torch.Tensor]]:\n",
    "    split_input_ids = torch.split(input_ids, batch_size)\n",
    "    split_attention_mask = torch.split(attention_mask, batch_size)\n",
    "    split_scores = torch.split(beam_scores, batch_size)\n",
    "    return split_input_ids, split_attention_mask, split_scores, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2590927809.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class CompletedCandidate:\n",
    "    def __init__(self, indices: torch.LongTensor, score: torch.Tensor) -> None:\n",
    "        self.indices = indices\n",
    "        self.score = score\n",
    "\n",
    "    def __lt__(self, other: CompletedCandidate) -> bool:\n",
    "        return float(self.score) < float(other.score)\n",
    "\n",
    "    def __le__(self, other: CompletedCandidate) -> bool:\n",
    "        return float(self.score) <= float(other.score)\n",
    "\n",
    "\n",
    "def fidt5_beam_sample(\n",
    "    input_ids: torch.LongTensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    tokenizer: T5Tokenizer,\n",
    "    beam_width: int,\n",
    "    n_recs: int,\n",
    "    stop_strings: list[str],\n",
    "    batch_size: int = 2,\n",
    "    max_seq_len: int = 512,\n",
    ") -> SampleResult: \n",
    "\n",
    "    beam_scores = torch.zeros((input_ids.shape[0],), dtype=torch.float32).to(\"cuda\")\n",
    "    completed_heap: list[CompletedCandidate] = []\n",
    "\n",
    "    while True:\n",
    "        batched_input_ids, batched_attention_masks, batched_scores = prepare_batches_fid(\n",
    "            input_ids,\n",
    "            attention_mask, \n",
    "            beam_scores,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        next_scores_list: list[torch.Tensor] = []\n",
    "        next_input_id_list: list[torch.Tensor] = []\n",
    "\n",
    "        for input_ids_batch, attention_mask_batch, past_batch in zip(\n",
    "            batched_input_ids, batched_attention_masks, batched_scores \n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                output_batch = model(**batch_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> simpl.</s>']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.9911, grad_fn=<NllLossBackward0>),\n",
       " tensor([[[-20.5744, -12.8595, -14.6405,  ..., -45.1446, -45.2421, -45.2430],\n",
       "          [-25.0881, -15.1740,  -5.2820,  ..., -45.1663, -45.2774, -45.3447],\n",
       "          [-21.0880,  -8.3345,  -7.0697,  ..., -37.5994, -37.6230, -37.6437],\n",
       "          ...,\n",
       "          [-14.8440,  -8.7909,  -9.5105,  ..., -37.0198, -37.2101, -37.2129],\n",
       "          [-15.4429, -10.7294, -12.1775,  ..., -38.6211, -38.7931, -38.8090],\n",
       "          [-14.3118,  -8.4187,  -9.7325,  ..., -36.4805, -36.5963, -36.5941]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " ((tensor([[[[-4.3993e-01, -2.3827e+00, -1.1007e+00,  ..., -2.4550e+00,\n",
       "              -1.7357e+00, -5.3955e-01],\n",
       "             [-5.7156e-01,  1.8579e-01, -6.1753e-03,  ..., -1.7960e+00,\n",
       "               6.6674e-01, -1.0925e+00],\n",
       "             [-4.8580e-01,  9.8874e-01, -2.8671e-01,  ..., -1.6572e+00,\n",
       "               7.7291e-01, -7.1863e-01],\n",
       "             ...,\n",
       "             [-7.9248e-01, -1.6456e+00, -6.5908e-01,  ..., -1.3585e+00,\n",
       "              -1.8117e+00, -6.4172e-01],\n",
       "             [-5.2799e-01, -2.3237e+00, -1.1708e+00,  ..., -2.5530e+00,\n",
       "              -1.7395e+00, -5.5222e-01],\n",
       "             [-4.0525e-01, -2.2686e+00, -1.0969e+00,  ..., -2.6272e+00,\n",
       "              -1.8035e+00, -5.4696e-01]],\n",
       "   \n",
       "            [[-1.0347e+00, -2.2722e+00, -7.4094e-02,  ...,  1.2300e+00,\n",
       "               1.0671e+00,  8.3047e-01],\n",
       "             [-1.3792e+00, -1.1468e+00, -2.7094e+00,  ..., -4.3801e-01,\n",
       "               3.0756e+00,  1.2451e+00],\n",
       "             [ 4.3018e-01,  1.1818e+00, -1.1249e+00,  ...,  1.7069e+00,\n",
       "               1.0711e+00, -8.8170e-01],\n",
       "             ...,\n",
       "             [-7.5682e-01, -2.5797e+00, -7.3894e-01,  ...,  3.2291e-01,\n",
       "               2.5726e-01,  4.6836e-01],\n",
       "             [-9.5620e-01, -2.4234e+00, -2.4729e-01,  ...,  1.1286e+00,\n",
       "               7.8547e-01,  5.2086e-01],\n",
       "             [-9.6043e-01, -2.3805e+00, -2.5525e-01,  ...,  1.2823e+00,\n",
       "               9.7622e-01,  6.6241e-01]],\n",
       "   \n",
       "            [[ 1.0950e+00, -2.6700e+00,  6.2325e-01,  ..., -1.2431e+00,\n",
       "              -5.3954e-02,  3.8951e-01],\n",
       "             [-3.5431e-01, -5.3400e-01, -7.6219e-01,  ...,  8.0917e-01,\n",
       "              -8.2371e-01, -4.1233e-01],\n",
       "             [ 1.0625e+00,  8.7297e-01,  1.3450e-01,  ..., -1.6754e-01,\n",
       "               1.1135e+00,  6.4575e-01],\n",
       "             ...,\n",
       "             [-6.6486e-02, -2.4088e+00,  9.5280e-01,  ..., -1.8146e+00,\n",
       "               1.4230e+00,  6.7690e-01],\n",
       "             [ 1.0623e+00, -2.5280e+00,  7.1245e-01,  ..., -1.2954e+00,\n",
       "               3.2181e-01,  5.6577e-01],\n",
       "             [ 1.1274e+00, -2.7270e+00,  7.6254e-01,  ..., -1.1242e+00,\n",
       "               1.9774e-01,  5.1422e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-7.8119e-01,  2.2756e+00,  5.3764e-02,  ...,  1.3011e+00,\n",
       "              -2.7152e+00, -1.1664e+00],\n",
       "             [-1.6403e+00,  7.0844e-01,  8.7538e-01,  ...,  9.8101e-01,\n",
       "               4.7602e-01,  1.6679e+00],\n",
       "             [ 2.4087e+00,  5.1446e-01, -1.2579e+00,  ...,  1.7151e+00,\n",
       "               1.3340e+00,  4.8882e-01],\n",
       "             ...,\n",
       "             [-9.0648e-01,  1.2566e+00,  9.5759e-01,  ...,  2.1248e+00,\n",
       "              -1.9827e+00, -1.2407e+00],\n",
       "             [-6.8211e-01,  2.2346e+00,  3.1276e-01,  ...,  1.4457e+00,\n",
       "              -2.8578e+00, -1.1000e+00],\n",
       "             [-6.7170e-01,  2.1614e+00,  1.0183e-01,  ...,  1.5502e+00,\n",
       "              -2.7089e+00, -1.1787e+00]],\n",
       "   \n",
       "            [[-5.7848e+00, -1.0737e+00, -1.5072e+00,  ..., -1.2015e+00,\n",
       "              -8.6645e-02,  1.4401e+00],\n",
       "             [ 8.4164e-01, -6.3285e-01, -1.0305e+00,  ..., -2.6591e-01,\n",
       "              -9.0910e-02,  7.6579e-01],\n",
       "             [ 4.8641e-02,  3.3415e-01, -4.1853e-01,  ...,  9.0399e-01,\n",
       "              -7.7527e-01, -4.4392e-02],\n",
       "             ...,\n",
       "             [-5.3105e+00, -9.4238e-01, -1.6052e+00,  ..., -1.4294e+00,\n",
       "              -1.7868e-01,  9.8592e-01],\n",
       "             [-6.0082e+00, -1.1409e+00, -1.5685e+00,  ..., -1.2898e+00,\n",
       "              -1.6509e-01,  1.5533e+00],\n",
       "             [-5.9075e+00, -1.0572e+00, -1.5238e+00,  ..., -1.2454e+00,\n",
       "              -8.4755e-02,  1.5680e+00]],\n",
       "   \n",
       "            [[-9.4471e-03, -4.9656e-02, -3.9344e-03,  ...,  5.4194e-01,\n",
       "              -3.8334e-01, -2.2956e-01],\n",
       "             [ 4.0504e-01,  9.5526e-01,  4.4841e-01,  ...,  4.7065e-01,\n",
       "              -5.3214e-01,  2.9510e-01],\n",
       "             [-4.1004e-01,  5.2858e-01,  3.0735e-01,  ..., -2.8318e-02,\n",
       "              -2.2012e-03, -4.8604e-01],\n",
       "             ...,\n",
       "             [ 2.1674e-01, -1.0688e-01, -1.0327e-01,  ...,  3.5480e-01,\n",
       "              -6.1828e-01, -1.5273e-01],\n",
       "             [ 5.0619e-02, -1.1777e-02, -7.7086e-02,  ...,  5.8741e-01,\n",
       "              -2.2539e-01, -2.8961e-01],\n",
       "             [ 6.4583e-02, -6.5286e-02,  1.6266e-01,  ...,  4.7257e-01,\n",
       "              -2.8228e-01, -1.4799e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 2.0808e-01,  3.0029e-01, -2.3169e-01,  ...,  6.7718e-01,\n",
       "               2.3529e-01,  2.1900e-01],\n",
       "             [-1.3309e-01, -1.7859e+00, -6.3948e-01,  ..., -1.1751e+00,\n",
       "              -2.9532e-01, -9.7070e-01],\n",
       "             [ 5.9561e-01,  1.6989e+00, -4.7130e-02,  ...,  9.0293e-01,\n",
       "              -1.0600e+00, -1.1387e+00],\n",
       "             ...,\n",
       "             [ 1.6538e-01,  3.1896e-01, -2.5002e-01,  ...,  7.7725e-01,\n",
       "               2.9124e-01,  1.1452e-01],\n",
       "             [ 2.8783e-01,  3.1430e-01, -2.1499e-01,  ...,  6.9323e-01,\n",
       "               2.8521e-01,  1.9262e-01],\n",
       "             [ 2.3433e-01,  2.6641e-01, -1.9527e-01,  ...,  7.6450e-01,\n",
       "               2.8081e-01,  2.2145e-01]],\n",
       "   \n",
       "            [[-1.1231e-01,  4.9267e-01, -3.8044e-01,  ..., -1.2093e-01,\n",
       "              -1.3651e-01, -1.4823e-01],\n",
       "             [-1.1986e+00, -2.0982e+00,  1.9135e+00,  ...,  7.3296e-01,\n",
       "               2.8865e+00, -1.3488e+00],\n",
       "             [-1.0762e+00,  1.0437e+00, -9.6782e-02,  ...,  5.4945e-01,\n",
       "              -5.0929e-01, -1.1599e+00],\n",
       "             ...,\n",
       "             [-5.1158e-01,  4.7385e-01, -3.3201e-01,  ..., -2.3391e-03,\n",
       "               8.2552e-02, -7.9465e-02],\n",
       "             [-1.5809e-01,  5.7168e-01, -4.3307e-01,  ..., -9.4709e-02,\n",
       "              -1.2812e-01, -2.4031e-01],\n",
       "             [-3.7926e-02,  4.9121e-01, -3.7598e-01,  ..., -1.2438e-01,\n",
       "               4.2744e-02, -1.8268e-01]],\n",
       "   \n",
       "            [[ 5.7477e-02,  4.2808e-01, -5.1128e-02,  ..., -9.4580e-02,\n",
       "              -7.8489e-01,  1.9498e-01],\n",
       "             [ 2.5072e-01,  6.9108e-01, -1.1682e+00,  ...,  7.4724e-01,\n",
       "              -1.3527e+00, -1.0643e+00],\n",
       "             [ 2.1448e+00, -5.8652e-01,  3.2684e-01,  ..., -2.5433e+00,\n",
       "              -2.0729e+00, -2.2001e+00],\n",
       "             ...,\n",
       "             [ 4.3904e-01,  2.0714e-01, -1.0977e-01,  ..., -1.0236e-01,\n",
       "              -8.6410e-01, -2.3586e-01],\n",
       "             [ 1.4187e-01,  2.8190e-01, -1.8067e-02,  ..., -1.1943e-01,\n",
       "              -8.8886e-01,  1.2090e-01],\n",
       "             [-2.5554e-02,  3.3319e-01, -1.4441e-01,  ..., -9.9820e-02,\n",
       "              -8.8920e-01,  1.5443e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 3.0246e-01, -1.0832e-01,  1.4359e-02,  ..., -2.1479e-01,\n",
       "               1.7320e-01, -8.4624e-02],\n",
       "             [ 9.9657e-01, -1.8311e+00, -6.1883e-01,  ..., -1.7403e+00,\n",
       "              -3.6744e-01, -1.5563e+00],\n",
       "             [-1.0217e+00, -7.8948e-01, -2.1977e-01,  ...,  1.7009e-01,\n",
       "               2.1275e+00,  1.2601e-01],\n",
       "             ...,\n",
       "             [ 4.7624e-01, -1.5219e-01,  5.0571e-02,  ..., -9.6592e-02,\n",
       "               1.2423e-01, -8.3805e-02],\n",
       "             [ 3.3818e-01, -1.4870e-01, -6.5046e-02,  ..., -2.5195e-01,\n",
       "               1.1213e-01, -9.4159e-02],\n",
       "             [ 3.0213e-01, -1.4067e-01, -4.3470e-02,  ..., -1.5503e-01,\n",
       "               9.0069e-02, -9.1760e-02]],\n",
       "   \n",
       "            [[-1.7914e-01, -6.1502e-02,  4.1183e-02,  ...,  1.0820e-01,\n",
       "               8.2243e-02, -2.1782e-01],\n",
       "             [ 1.0093e+00, -3.3821e-01,  6.2999e-01,  ...,  1.6961e-01,\n",
       "              -4.1008e-01,  3.9852e-01],\n",
       "             [ 1.3715e+00,  5.2147e-01, -2.1562e-01,  ...,  9.4257e-02,\n",
       "               8.6718e-01,  3.9229e-01],\n",
       "             ...,\n",
       "             [-4.1928e-01,  1.4540e-02,  1.6207e-03,  ..., -5.3498e-03,\n",
       "               2.9283e-02, -7.5678e-03],\n",
       "             [-2.3361e-01, -4.1677e-02, -1.1601e-02,  ...,  5.8201e-02,\n",
       "              -3.4574e-02, -1.1323e-01],\n",
       "             [-2.2811e-01, -1.0293e-01,  3.0929e-02,  ...,  7.6858e-02,\n",
       "               8.8505e-02, -1.9740e-01]],\n",
       "   \n",
       "            [[ 1.3817e-01, -6.8270e-01,  1.9922e-01,  ...,  2.9687e-01,\n",
       "              -2.1874e-01,  1.7396e-01],\n",
       "             [ 1.8453e+00,  2.9704e+00, -1.7181e-01,  ...,  7.1929e-01,\n",
       "               1.6988e+00, -6.6906e-01],\n",
       "             [-3.3473e-01, -1.3302e+00,  1.2554e+00,  ...,  1.0468e+00,\n",
       "              -1.8503e+00, -9.0832e-01],\n",
       "             ...,\n",
       "             [ 5.7183e-02, -5.7348e-01,  3.0851e-01,  ...,  8.7351e-02,\n",
       "              -3.0660e-01,  2.2907e-01],\n",
       "             [ 3.3348e-02, -7.7439e-01,  2.9103e-01,  ...,  2.0323e-01,\n",
       "              -1.4942e-01,  6.1751e-02],\n",
       "             [ 1.8247e-01, -6.3159e-01,  2.9303e-01,  ...,  1.7037e-01,\n",
       "              -1.4800e-01,  1.5716e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[-2.7492e+00, -2.4527e+00,  1.1641e-01,  ...,  1.2145e+00,\n",
       "               3.2294e+00,  7.9752e-02],\n",
       "             [-2.9343e+00, -2.0907e+00,  4.0521e-01,  ...,  6.6373e-01,\n",
       "               1.3482e+00, -1.0495e+00],\n",
       "             [-2.5858e-01, -3.7390e+00, -2.0389e-02,  ..., -1.7022e+00,\n",
       "               1.3786e+00, -3.3095e-01],\n",
       "             ...,\n",
       "             [-4.9745e-01, -2.4160e+00,  2.5551e-01,  ..., -2.1454e+00,\n",
       "               1.1428e+00, -2.8913e+00],\n",
       "             [-5.9939e-02, -3.0380e+00,  2.6413e-02,  ..., -2.9340e+00,\n",
       "               1.0230e+00, -3.5205e+00],\n",
       "             [-6.2265e-02, -3.7440e+00,  1.3191e+00,  ..., -2.7555e+00,\n",
       "               2.5605e+00, -3.8742e+00]],\n",
       "   \n",
       "            [[-2.5278e+00, -1.7892e+00,  3.5738e+00,  ..., -4.8588e+00,\n",
       "              -2.9897e+00,  1.6031e+00],\n",
       "             [-3.6496e+00,  1.1897e+00, -2.5139e+00,  ..., -5.4261e+00,\n",
       "               2.4832e+00,  1.5180e+00],\n",
       "             [-3.4425e+00,  2.6117e+00, -3.9864e+00,  ..., -3.8781e+00,\n",
       "               4.1538e+00, -6.4036e+00],\n",
       "             ...,\n",
       "             [-2.6684e+00,  1.9102e+00,  4.3505e+00,  ..., -2.6478e+00,\n",
       "               3.5441e+00, -9.3877e-01],\n",
       "             [-3.3335e+00,  1.9587e+00,  4.9966e+00,  ..., -4.3164e+00,\n",
       "               4.0519e+00, -8.6039e-01],\n",
       "             [-2.8520e+00,  3.1457e+00,  3.9151e+00,  ..., -2.2123e+00,\n",
       "               5.9944e+00, -7.8366e-01]],\n",
       "   \n",
       "            [[-1.4337e+00,  2.8262e+00,  1.6937e+00,  ..., -2.3277e-01,\n",
       "              -1.4307e+00,  1.6082e+00],\n",
       "             [-1.7036e-02,  1.9884e+00, -4.3073e-01,  ..., -1.7266e+00,\n",
       "               1.0278e-01,  4.5986e-01],\n",
       "             [-2.6405e-01,  2.7895e-04, -5.4009e-02,  ..., -7.5585e-01,\n",
       "               1.0147e-01, -3.2390e-01],\n",
       "             ...,\n",
       "             [-3.1885e-02,  8.2228e-01,  1.4675e+00,  ...,  3.1592e-01,\n",
       "              -2.0407e+00, -4.3606e-01],\n",
       "             [-7.4340e-01, -4.2656e-01,  1.0283e+00,  ..., -1.4745e+00,\n",
       "              -1.2169e+00, -2.0306e+00],\n",
       "             [ 6.1788e-01,  1.0704e+00,  6.8679e-01,  ...,  8.8508e-01,\n",
       "              -1.9230e+00, -1.3005e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-6.7657e-01, -2.3231e+00,  5.1635e-01,  ...,  2.6513e+00,\n",
       "              -4.4725e-01, -1.9232e+00],\n",
       "             [ 4.1731e+00,  4.4842e-01, -1.4361e+00,  ..., -2.2963e+00,\n",
       "              -8.5012e-01, -2.4300e+00],\n",
       "             [ 4.9769e+00,  6.8797e-01, -1.8315e+00,  ..., -1.4610e+00,\n",
       "              -2.6644e-01, -5.3832e-02],\n",
       "             ...,\n",
       "             [-8.3335e-01,  3.5696e-01, -7.0557e-01,  ...,  8.3059e-02,\n",
       "              -2.3201e+00, -2.4816e+00],\n",
       "             [-3.9519e+00,  2.1165e+00,  1.6266e-01,  ...,  1.6192e+00,\n",
       "              -7.3554e-01, -2.3329e+00],\n",
       "             [-1.8852e+00,  1.8795e-01,  1.1901e-01,  ...,  1.4650e+00,\n",
       "              -2.3894e+00, -1.9275e+00]],\n",
       "   \n",
       "            [[-6.1202e-01, -2.6322e+00, -1.3480e+00,  ...,  2.7658e+00,\n",
       "              -2.5905e+00,  1.2443e+00],\n",
       "             [ 4.5689e-02, -5.5551e-01, -2.2760e+00,  ...,  4.4327e+00,\n",
       "              -3.1289e+00, -2.1089e+00],\n",
       "             [ 1.1302e+00,  5.9393e-01, -1.4787e+00,  ...,  3.8980e+00,\n",
       "              -2.8820e+00, -1.6410e+00],\n",
       "             ...,\n",
       "             [ 2.6383e+00, -1.4552e+00, -2.3794e+00,  ...,  2.7727e+00,\n",
       "              -5.7256e-01,  5.7582e-01],\n",
       "             [ 4.2507e+00,  1.2277e+00, -1.6263e+00,  ...,  4.0160e+00,\n",
       "              -1.6978e+00,  4.3667e-01],\n",
       "             [ 2.5757e+00,  7.5200e-02, -2.6368e+00,  ...,  5.0184e+00,\n",
       "              -2.9194e+00,  2.0907e+00]],\n",
       "   \n",
       "            [[-1.4081e+00,  4.7565e-02, -2.3790e+00,  ..., -2.7263e+00,\n",
       "               9.4892e-01,  1.6487e+00],\n",
       "             [ 1.7506e+00, -9.3897e-01,  8.9680e-01,  ..., -1.7849e+00,\n",
       "              -2.6653e-01,  3.7928e-01],\n",
       "             [ 1.4521e+00, -6.2681e-01,  2.0959e+00,  ..., -2.1381e+00,\n",
       "               1.0869e+00,  1.2849e+00],\n",
       "             ...,\n",
       "             [-1.5534e+00, -2.2511e-01, -3.8780e-01,  ...,  1.3001e+00,\n",
       "               1.1234e+00, -3.1797e-02],\n",
       "             [-1.1816e-01,  6.9296e-02, -4.1586e-01,  ...,  8.4626e-01,\n",
       "               2.0891e+00, -7.4198e-01],\n",
       "             [-7.6154e-01,  2.1120e+00,  7.1292e-01,  ...,  1.9013e+00,\n",
       "               2.3953e+00,  3.4911e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 3.2034,  4.0293,  2.5341,  ..., -1.4382,  1.7511, -3.1560],\n",
       "             [ 4.3192, -0.2403,  1.3852,  ..., -4.0672,  0.5094,  0.3623],\n",
       "             [ 1.3660,  2.5109,  0.1170,  ..., -2.9476, -1.1381,  1.2077],\n",
       "             ...,\n",
       "             [ 0.4635,  2.2860,  2.5714,  ..., -1.3553, -1.9630,  2.1259],\n",
       "             [ 0.6526,  0.6886,  2.0283,  ..., -1.7945, -0.7670,  0.5163],\n",
       "             [ 1.4081,  1.4263,  2.4024,  ..., -2.6536,  0.0501,  2.0801]],\n",
       "   \n",
       "            [[-3.7018,  6.0605,  2.9865,  ..., -0.2395, -2.9474,  3.6510],\n",
       "             [-2.2798,  6.8615,  3.7230,  ...,  1.9884,  2.3103,  2.4486],\n",
       "             [ 0.1369,  7.2452,  4.2422,  ...,  2.7864, -1.6275, -2.5140],\n",
       "             ...,\n",
       "             [-4.7381, -4.1144,  0.6580,  ...,  4.2016, -1.1721,  0.2233],\n",
       "             [-4.6341, -3.1553,  0.2541,  ...,  3.0227, -0.9658,  0.9851],\n",
       "             [-4.2033, -4.3726,  2.3447,  ...,  4.7250, -1.2571,  2.4128]],\n",
       "   \n",
       "            [[ 2.3621, -4.8756,  1.6692,  ...,  0.2795,  0.2308,  0.1996],\n",
       "             [ 0.3138, -7.0327,  2.8503,  ..., -1.9483,  2.0386, -0.3263],\n",
       "             [ 4.9601, -1.8324,  5.4527,  ..., -1.4349,  3.3655,  0.6472],\n",
       "             ...,\n",
       "             [ 1.2156, -0.0176, -0.5280,  ..., -2.4297, -3.3262, -0.6356],\n",
       "             [ 0.5142,  1.6547,  0.6473,  ..., -0.2027, -3.7235, -0.1595],\n",
       "             [ 2.1094, -0.5377, -0.0867,  ..., -2.3098, -2.7790, -0.1349]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 1.0940,  1.1825,  0.7185,  ...,  0.0838, -1.1442,  1.2332],\n",
       "             [ 2.6744,  0.5277,  0.6121,  ..., -0.2865, -2.4579,  1.8243],\n",
       "             [ 0.9975, -1.0057,  2.5509,  ..., -1.8727, -1.4553,  1.0510],\n",
       "             ...,\n",
       "             [ 0.2566,  3.2484,  2.8460,  ...,  1.5563,  0.5660, -0.5461],\n",
       "             [ 0.3339,  0.3055,  3.4952,  ...,  0.8084,  0.5250, -1.2369],\n",
       "             [ 0.3574,  0.6276,  2.9602,  ...,  1.7370,  1.9120,  0.4481]],\n",
       "   \n",
       "            [[-0.2150, -2.1361,  3.7906,  ...,  3.9324, -0.8253, -2.6685],\n",
       "             [ 1.0441,  2.6569,  4.5827,  ...,  2.5945,  0.9773, -1.3328],\n",
       "             [-1.9689,  0.1548,  3.3044,  ..., -1.9877,  3.7324, -2.3951],\n",
       "             ...,\n",
       "             [ 2.2108,  0.6829, -0.2957,  ...,  2.9920,  2.1764, -0.8460],\n",
       "             [-0.5917,  1.4529,  0.9617,  ...,  5.4204,  2.5116, -0.6177],\n",
       "             [ 0.6121,  3.6659,  0.0460,  ...,  3.7559,  1.3202, -2.1977]],\n",
       "   \n",
       "            [[ 0.7159,  0.0087,  0.4550,  ...,  0.8025, -3.7073, -0.7816],\n",
       "             [-0.7525, -0.2362,  1.0123,  ..., -0.1189, -2.8739,  4.9573],\n",
       "             [-1.1941, -2.6423, -0.6320,  ...,  0.5683, -5.4890,  2.8630],\n",
       "             ...,\n",
       "             [ 0.9158, -1.6584,  0.8575,  ...,  0.9132, -0.2003, -1.1430],\n",
       "             [ 1.2344, -1.0783,  2.1640,  ..., -2.1095,  2.8516,  0.8240],\n",
       "             [-0.4703, -0.1720,  1.7463,  ..., -1.1069,  0.3704, -1.8659]]]],\n",
       "          grad_fn=<TransposeBackward0>)),\n",
       "  (tensor([[[[ 1.0844,  0.8047,  1.9535,  ...,  1.5715, -1.4746,  1.4788],\n",
       "             [-0.4363, -0.4213, -1.0526,  ...,  0.4068, -0.9468, -0.4502],\n",
       "             [ 2.1100, -0.5198,  0.9254,  ...,  1.2135,  0.9378, -1.6591],\n",
       "             ...,\n",
       "             [ 1.0877,  0.9857,  1.9372,  ...,  1.5007, -1.6302,  1.4870],\n",
       "             [ 1.0925,  1.0334,  1.8838,  ...,  1.4281, -1.5891,  1.4846],\n",
       "             [ 1.0712,  0.9030,  1.8227,  ...,  1.4333, -1.4659,  1.4216]],\n",
       "   \n",
       "            [[-0.6188, -1.1196, -4.2329,  ...,  3.3670,  0.6830, -0.3126],\n",
       "             [-0.3609, -0.1164,  0.7088,  ..., -0.9960,  0.0818, -0.1770],\n",
       "             [ 1.0444, -1.2910,  1.4021,  ..., -0.6435,  0.5818,  0.5379],\n",
       "             ...,\n",
       "             [-0.5607, -1.1743, -4.2360,  ...,  3.3549,  0.8008, -0.1863],\n",
       "             [-0.5354, -1.2360, -4.2235,  ...,  3.2946,  0.8518, -0.1811],\n",
       "             [-0.5926, -1.1826, -4.0895,  ...,  3.2194,  0.7874, -0.3148]],\n",
       "   \n",
       "            [[-3.0185, -0.5292, -0.2980,  ..., -0.6334,  0.9416,  1.5385],\n",
       "             [-0.4193,  0.6790,  0.8217,  ..., -1.7419,  0.8894,  1.1135],\n",
       "             [ 0.6882, -0.7920, -1.3949,  ...,  1.3285, -0.6810, -0.3191],\n",
       "             ...,\n",
       "             [-2.8808, -0.5249, -0.1022,  ..., -0.5833,  0.8456,  1.5390],\n",
       "             [-2.8974, -0.4908, -0.0381,  ..., -0.6379,  0.7761,  1.5529],\n",
       "             [-2.7192, -0.6817, -0.0864,  ..., -0.7357,  0.9056,  1.4398]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 0.9560,  0.5188, -0.2345,  ..., -0.2945,  1.5611, -1.2081],\n",
       "             [ 0.5182, -1.3292,  0.7065,  ..., -0.3035, -0.1717, -0.5647],\n",
       "             [ 1.3884,  0.2622, -0.6928,  ...,  1.0238,  2.1969,  0.0142],\n",
       "             ...,\n",
       "             [ 0.9167,  0.5162, -0.2267,  ..., -0.3344,  1.4651, -1.3245],\n",
       "             [ 0.9264,  0.5359, -0.1864,  ..., -0.3550,  1.4241, -1.3613],\n",
       "             [ 0.8809,  0.4146, -0.2413,  ..., -0.4345,  1.5098, -1.2360]],\n",
       "   \n",
       "            [[-0.1508,  1.5447, -0.4799,  ..., -0.4712,  0.2350,  0.3763],\n",
       "             [ 0.5999, -0.2416, -1.2724,  ..., -0.7534, -1.1228,  0.7709],\n",
       "             [ 0.3682,  0.4046, -0.0801,  ..., -0.5317, -0.1738,  1.5068],\n",
       "             ...,\n",
       "             [-0.1795,  1.5387, -0.4774,  ..., -0.4717,  0.2468,  0.4464],\n",
       "             [-0.1471,  1.4422, -0.4935,  ..., -0.4636,  0.2800,  0.4523],\n",
       "             [-0.1133,  1.4965, -0.5281,  ..., -0.4588,  0.2271,  0.4060]],\n",
       "   \n",
       "            [[ 0.4977, -0.3073,  0.1401,  ..., -0.7482,  0.4436, -0.1013],\n",
       "             [ 1.0799, -0.5187,  0.6481,  ..., -0.3012,  0.2982,  0.1015],\n",
       "             [-1.3865, -0.6575,  0.4769,  ..., -0.0443,  0.5168, -1.4291],\n",
       "             ...,\n",
       "             [ 0.6334, -0.3198,  0.1164,  ..., -0.7029,  0.4357, -0.1801],\n",
       "             [ 0.7350, -0.3378,  0.0992,  ..., -0.5865,  0.4147, -0.1753],\n",
       "             [ 0.6709, -0.3260,  0.0811,  ..., -0.7301,  0.3675, -0.1641]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[-3.1129e-02,  1.8362e-01, -1.0839e-01,  ...,  1.8088e-02,\n",
       "              -2.8670e-01, -1.2283e-02],\n",
       "             [ 1.5746e+00,  1.1714e+00, -3.0633e+00,  ..., -1.6621e+00,\n",
       "               1.4475e+00, -3.9524e-01],\n",
       "             [-1.6372e+00, -7.5930e-01,  1.4453e+00,  ...,  2.1239e+00,\n",
       "               9.3865e-01, -2.0036e+00],\n",
       "             ...,\n",
       "             [ 4.4072e-02,  8.5907e-02, -1.6316e-01,  ..., -1.6958e-02,\n",
       "              -3.1059e-01,  3.4498e-02],\n",
       "             [ 8.9650e-02,  1.3519e-01, -4.3209e-02,  ...,  2.7515e-02,\n",
       "              -2.9913e-01,  6.6925e-02],\n",
       "             [ 7.4387e-02,  1.6326e-01, -5.9391e-02,  ..., -4.9084e-02,\n",
       "              -1.4344e-01,  7.6672e-02]],\n",
       "   \n",
       "            [[-8.4136e-01,  1.2535e-01,  1.1365e-02,  ..., -3.3606e-01,\n",
       "               4.6619e-01, -9.7329e-01],\n",
       "             [-6.4005e-01,  3.7693e+00, -9.1364e-01,  ..., -4.7724e-01,\n",
       "              -1.8277e-01, -7.5299e-01],\n",
       "             [-1.7505e+00,  2.5391e+00, -8.8056e-01,  ..., -4.5536e-01,\n",
       "               9.1398e-01, -3.4324e+00],\n",
       "             ...,\n",
       "             [-9.0296e-01,  2.8881e-02,  1.6966e-01,  ..., -3.8270e-01,\n",
       "               2.9346e-01, -7.9081e-01],\n",
       "             [-9.6599e-01,  4.4837e-02,  4.5816e-01,  ..., -3.2861e-01,\n",
       "               2.7646e-01, -8.4822e-01],\n",
       "             [-9.1158e-01,  1.4584e-01,  2.4558e-01,  ..., -3.5348e-01,\n",
       "               2.6139e-01, -8.0588e-01]],\n",
       "   \n",
       "            [[ 1.8565e-01, -6.4237e-02, -1.0114e-01,  ...,  2.7132e-01,\n",
       "               2.1954e-01,  2.4092e-01],\n",
       "             [ 9.1580e-01,  1.2741e+00,  6.2371e-01,  ..., -1.6762e+00,\n",
       "               1.7118e-01,  2.7571e-01],\n",
       "             [-6.1738e-01, -2.9275e-02, -1.2376e+00,  ..., -1.9926e+00,\n",
       "               1.0555e+00,  1.4076e+00],\n",
       "             ...,\n",
       "             [ 2.7466e-01, -4.0031e-02, -3.8469e-02,  ...,  2.1191e-01,\n",
       "               2.9236e-01,  2.3251e-01],\n",
       "             [ 2.2659e-01,  3.0885e-04,  2.8182e-02,  ...,  1.7508e-01,\n",
       "               2.3636e-01,  2.6772e-01],\n",
       "             [ 1.2361e-01, -3.9512e-02, -1.1499e-01,  ...,  2.3694e-01,\n",
       "               2.4237e-01,  2.4075e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 2.7969e-01,  1.5937e-01,  8.4551e-02,  ...,  4.6975e-01,\n",
       "              -3.8085e-02,  9.0838e-02],\n",
       "             [ 1.7927e-01,  8.4260e-01, -4.7084e-01,  ...,  1.8239e+00,\n",
       "              -1.3312e+00, -6.3593e-01],\n",
       "             [ 7.5087e-01,  1.0060e+00, -2.7868e+00,  ..., -1.2344e+00,\n",
       "               1.8615e+00, -2.8740e+00],\n",
       "             ...,\n",
       "             [ 1.6777e-01,  7.0115e-02, -2.4721e-02,  ...,  3.5590e-01,\n",
       "              -1.0791e-01, -1.8614e-02],\n",
       "             [ 1.3390e-01,  2.8751e-02, -6.8579e-03,  ...,  3.2452e-01,\n",
       "              -4.6828e-03,  1.0736e-01],\n",
       "             [ 2.1331e-01,  1.1033e-01,  9.5565e-02,  ...,  3.0839e-01,\n",
       "              -1.4799e-01,  2.7927e-02]],\n",
       "   \n",
       "            [[-1.5064e-01, -5.9811e-02,  2.5831e-02,  ..., -6.6321e-02,\n",
       "              -1.0308e-01, -2.3997e-01],\n",
       "             [-9.3062e-01,  5.8367e-01,  3.2987e-01,  ..., -1.7014e-01,\n",
       "              -6.5549e-01, -7.6126e-01],\n",
       "             [ 3.3555e+00, -4.5316e-01, -2.7847e-01,  ...,  1.1758e+00,\n",
       "              -1.5457e+00,  4.5928e-01],\n",
       "             ...,\n",
       "             [-1.1446e-01, -7.5981e-02,  3.7449e-02,  ..., -1.1550e-01,\n",
       "              -1.2074e-01, -2.0826e-01],\n",
       "             [-1.4033e-01, -1.3743e-01,  1.0367e-01,  ..., -1.5789e-01,\n",
       "              -1.1309e-01, -1.8051e-01],\n",
       "             [-2.0535e-01, -9.0156e-02,  1.1280e-02,  ..., -1.9345e-01,\n",
       "              -1.0557e-01, -8.2425e-02]],\n",
       "   \n",
       "            [[-5.4188e-01,  1.7126e-01,  1.5836e-01,  ..., -2.6155e-01,\n",
       "              -6.4351e-02,  3.2559e-01],\n",
       "             [-1.0580e-01,  5.0480e-01,  3.2133e-01,  ..., -2.4405e-01,\n",
       "              -7.1952e-01, -8.0198e-01],\n",
       "             [-4.7601e+00,  6.7970e-01, -2.4446e+00,  ..., -2.1745e-01,\n",
       "              -7.4574e-01, -1.1900e+00],\n",
       "             ...,\n",
       "             [-3.9139e-01,  8.5392e-02, -1.4294e-02,  ..., -3.2714e-01,\n",
       "               4.6762e-02,  1.7045e-01],\n",
       "             [-3.0731e-01,  1.2459e-01, -2.1603e-02,  ..., -2.5268e-01,\n",
       "              -2.9770e-02,  2.2374e-01],\n",
       "             [-3.4716e-01,  6.9783e-02, -6.1548e-02,  ..., -1.4231e-01,\n",
       "              -3.1922e-03,  1.8692e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[-0.9242, -1.5584,  1.5875,  ...,  0.2996,  2.0631, -0.1756],\n",
       "             [-0.2323, -1.2349,  2.4840,  ...,  2.6041,  1.1404,  1.6095],\n",
       "             [-1.5609, -2.0880,  1.9322,  ...,  2.1456,  1.1124,  0.2243],\n",
       "             ...,\n",
       "             [ 1.8723, -0.1356, -0.5387,  ...,  0.7161,  1.1965,  0.8837],\n",
       "             [ 2.1267,  0.1655,  0.7051,  ..., -0.1556,  1.9041,  2.1551],\n",
       "             [ 2.0495,  0.5191,  1.0193,  ...,  0.8224,  1.1429,  1.1472]],\n",
       "   \n",
       "            [[-1.0788, -1.9313,  0.1964,  ...,  1.1178,  0.1662, -0.5756],\n",
       "             [-3.1924, -0.8152,  1.4192,  ...,  1.3029, -0.7258,  1.4072],\n",
       "             [-1.7234, -1.7861,  0.2548,  ...,  2.3368, -0.3763,  1.1183],\n",
       "             ...,\n",
       "             [-1.9040,  1.5367, -1.1986,  ...,  1.6616,  1.2702, -2.6560],\n",
       "             [-1.9578,  0.1793,  0.9383,  ...,  0.7187,  1.8261, -3.2152],\n",
       "             [-1.7428,  1.7289, -0.0550,  ...,  1.0754,  0.3709, -3.1429]],\n",
       "   \n",
       "            [[ 0.3630, -0.9429, -0.1574,  ...,  0.7845,  0.0401, -0.4205],\n",
       "             [-2.4379, -1.0918,  1.3854,  ...,  2.0840, -0.8640, -0.2986],\n",
       "             [-2.0978, -1.3784, -0.4453,  ...,  1.3483, -0.1082,  0.1065],\n",
       "             ...,\n",
       "             [ 0.1068,  2.2970, -3.2546,  ...,  1.9928, -0.4404,  1.0940],\n",
       "             [-0.1891,  1.9834, -1.5565,  ...,  1.8935, -0.7222,  1.2389],\n",
       "             [-2.4529,  1.4291, -2.6308,  ...,  1.3875,  0.7873,  0.2892]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 2.3431,  0.7627, -1.0894,  ..., -1.5189,  2.5532,  1.5842],\n",
       "             [-0.2835,  1.8348,  1.3103,  ..., -0.8942,  2.6909,  0.5685],\n",
       "             [ 0.4260,  2.0230, -1.7553,  ..., -1.8148, -1.3829, -1.2307],\n",
       "             ...,\n",
       "             [ 0.9901, -0.0463, -1.7782,  ..., -2.6845,  0.0334, -0.6404],\n",
       "             [ 1.6212,  1.1244, -2.3985,  ..., -2.4510,  0.9278,  0.4200],\n",
       "             [-0.1446,  0.8176, -1.6081,  ..., -2.5845,  1.2438, -0.9253]],\n",
       "   \n",
       "            [[-0.5090,  0.6129,  0.2353,  ...,  0.4304,  1.8054, -0.1519],\n",
       "             [ 0.7594, -4.8686, -2.8839,  ..., -0.4128,  1.2065, -4.7080],\n",
       "             [-0.2596, -4.5857, -3.8774,  ...,  0.9141,  1.3652, -0.5059],\n",
       "             ...,\n",
       "             [ 1.9009,  0.8678, -1.8086,  ..., -0.6959,  0.7630,  1.9694],\n",
       "             [ 2.7541, -0.2982, -0.1667,  ..., -1.4313, -0.3725,  1.2313],\n",
       "             [ 2.8831,  0.3743, -1.6501,  ...,  0.1000, -0.3568,  1.7127]],\n",
       "   \n",
       "            [[ 1.0902, -0.4034,  1.1787,  ...,  0.6617, -1.7523,  1.9455],\n",
       "             [ 0.6104, -1.9727,  4.3683,  ...,  2.6247,  0.1838, -0.3237],\n",
       "             [ 0.0393, -0.3637,  5.3993,  ...,  1.1354,  1.6259,  0.1898],\n",
       "             ...,\n",
       "             [ 0.8609, -0.7327, -0.1790,  ...,  1.0727, -3.2279,  4.7540],\n",
       "             [ 1.3341,  0.7109,  0.2512,  ...,  1.4763, -2.0745,  1.9484],\n",
       "             [ 0.4144, -0.1949,  1.2973,  ...,  1.1943, -1.0308,  1.6101]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 2.2409e+00, -1.8693e-01,  5.8374e+00,  ...,  5.3880e-01,\n",
       "               3.1452e+00, -2.9054e-01],\n",
       "             [-5.2260e+00, -1.5826e+00,  3.0010e+00,  ..., -2.7860e+00,\n",
       "               1.7006e-01, -2.4071e+00],\n",
       "             [-1.1261e+00, -9.3404e-01,  3.7701e+00,  ..., -6.1719e-01,\n",
       "               2.4347e+00, -3.0551e+00],\n",
       "             ...,\n",
       "             [ 5.5591e+00, -4.0646e+00, -4.0558e+00,  ..., -3.2419e+00,\n",
       "               6.5647e+00, -3.3665e+00],\n",
       "             [ 5.3342e+00, -7.8167e+00, -4.4958e+00,  ..., -6.3300e+00,\n",
       "               5.2700e+00, -2.0648e+00],\n",
       "             [ 3.9902e+00, -6.4479e+00, -4.7070e+00,  ..., -3.8455e+00,\n",
       "               6.2755e+00, -4.1922e+00]],\n",
       "   \n",
       "            [[-6.6149e+00, -2.2897e+00, -9.2389e-01,  ...,  2.1885e-02,\n",
       "              -3.3757e+00, -1.5006e+00],\n",
       "             [ 1.9976e+00,  5.3145e+00, -5.1501e-01,  ..., -2.3613e+00,\n",
       "              -1.1325e+00, -2.2313e-01],\n",
       "             [-3.7035e-02,  1.2907e+00,  1.3600e-01,  ..., -1.3773e+00,\n",
       "               3.3616e+00, -3.6205e-01],\n",
       "             ...,\n",
       "             [-2.9623e+00, -2.5386e+00, -4.4044e+00,  ..., -5.7153e+00,\n",
       "               1.6897e-01, -8.0761e-01],\n",
       "             [-2.3211e+00, -5.6302e+00, -1.4552e+00,  ..., -5.1876e+00,\n",
       "               1.3666e+00,  9.2532e-01],\n",
       "             [ 1.2952e+00, -4.8622e+00, -6.4788e+00,  ..., -8.6969e+00,\n",
       "               4.3637e+00, -7.5825e-01]],\n",
       "   \n",
       "            [[-1.6559e+00, -2.1907e-01, -1.7074e+00,  ..., -1.0690e+00,\n",
       "              -1.5656e+00,  1.9019e+00],\n",
       "             [ 4.5165e-01, -1.6426e+00,  2.2434e+00,  ...,  1.2720e+00,\n",
       "               1.6037e+00,  2.3398e+00],\n",
       "             [-9.6671e-01,  1.7207e+00, -1.3788e+00,  ...,  1.3383e+00,\n",
       "               1.6180e+00,  3.8962e+00],\n",
       "             ...,\n",
       "             [-4.6391e+00,  5.6142e-01,  4.6564e-02,  ..., -3.2851e+00,\n",
       "              -6.4720e-01, -2.6980e+00],\n",
       "             [-3.1801e+00,  6.1702e-01,  5.2599e-01,  ..., -1.0676e+00,\n",
       "              -2.0372e+00, -2.9390e+00],\n",
       "             [-2.6029e+00, -1.6905e+00, -1.0510e+00,  ..., -2.7944e+00,\n",
       "              -3.6417e+00, -4.4956e+00]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 8.8700e-01, -1.8474e+00,  1.9970e+00,  ...,  5.8215e-01,\n",
       "               1.0382e+00,  1.4822e+00],\n",
       "             [-1.1808e+00, -5.4952e+00,  8.0266e+00,  ..., -8.6369e-01,\n",
       "              -1.3950e+00,  2.2288e+00],\n",
       "             [ 3.2656e+00, -3.3377e+00,  2.1141e-01,  ..., -1.0239e+00,\n",
       "               5.5012e+00,  9.7508e-02],\n",
       "             ...,\n",
       "             [ 2.6124e+00, -1.6026e+00,  8.9613e-01,  ...,  1.4087e+00,\n",
       "              -1.8819e+00,  2.6748e+00],\n",
       "             [-1.2983e+00, -1.2775e-01,  2.7787e+00,  ...,  1.4074e+00,\n",
       "              -1.8440e+00,  2.1215e+00],\n",
       "             [ 1.5173e+00, -1.7031e+00,  2.6124e+00,  ...,  2.1853e+00,\n",
       "              -2.5679e+00,  3.7417e+00]],\n",
       "   \n",
       "            [[-7.1352e+00, -7.7698e-01,  4.5285e+00,  ...,  2.0796e+00,\n",
       "              -5.6032e+00, -3.1046e+00],\n",
       "             [-1.1104e+00, -1.5293e+00,  4.6684e+00,  ...,  3.1622e+00,\n",
       "              -5.1089e+00,  1.1101e+00],\n",
       "             [ 2.6617e+00, -2.4957e+00, -2.2169e-01,  ...,  1.5497e+00,\n",
       "              -6.7072e+00, -7.6752e+00],\n",
       "             ...,\n",
       "             [-1.6372e+00, -2.0476e+00,  3.1045e+00,  ..., -4.5095e+00,\n",
       "              -3.4546e-01, -1.5108e+00],\n",
       "             [ 1.4823e+00,  1.1891e+00,  5.8612e+00,  ..., -2.0309e+00,\n",
       "              -6.2604e-01, -3.2507e-01],\n",
       "             [-2.6716e+00, -2.2586e+00,  7.0914e+00,  ..., -2.5131e+00,\n",
       "              -1.1780e+00,  1.8947e-01]],\n",
       "   \n",
       "            [[-1.7822e-01, -7.9440e-03,  6.1981e-01,  ..., -3.6096e+00,\n",
       "              -1.3239e+00, -3.6138e+00],\n",
       "             [-3.0445e-01, -2.3072e+00, -1.6371e+00,  ..., -6.7062e-01,\n",
       "              -1.9569e-02, -1.9539e+00],\n",
       "             [ 4.9557e-01,  4.8340e+00, -6.4820e+00,  ..., -1.6723e+00,\n",
       "              -6.3308e+00,  2.8231e-01],\n",
       "             ...,\n",
       "             [-1.3044e-01,  2.8848e-01,  1.2592e+00,  ...,  2.9716e+00,\n",
       "              -5.4888e-01,  1.4317e+00],\n",
       "             [-9.5171e-02,  6.0354e-01,  2.4222e+00,  ...,  2.2528e+00,\n",
       "               4.9682e-01, -4.9253e-01],\n",
       "             [-8.7590e-01,  1.0251e+00,  1.4829e+00,  ...,  9.4923e-01,\n",
       "              -1.8518e+00, -1.4551e-01]]]], grad_fn=<TransposeBackward0>)),\n",
       "  (tensor([[[[-1.1032, -0.9413, -0.6953,  ...,  0.0140,  0.1180, -0.7058],\n",
       "             [-0.5751, -1.4617, -0.8432,  ..., -1.5806,  2.4048,  0.1901],\n",
       "             [-1.2150, -1.8627, -0.3656,  ..., -1.5635,  2.0377, -0.3942],\n",
       "             ...,\n",
       "             [-1.1391, -0.8176, -0.5664,  ..., -0.1103,  0.0917, -0.7549],\n",
       "             [-1.0887, -0.8671, -0.5264,  ..., -0.0074,  0.0797, -0.7427],\n",
       "             [-1.2030, -0.8988, -0.5754,  ...,  0.0165,  0.1146, -0.6945]],\n",
       "   \n",
       "            [[-0.5973,  0.1822,  0.2308,  ...,  0.7788,  0.2314,  0.8428],\n",
       "             [-0.3038,  2.6154,  1.4393,  ...,  0.8729,  0.8674,  1.0016],\n",
       "             [ 1.2278,  1.0717, -0.3434,  ...,  1.1359,  1.3031, -0.5113],\n",
       "             ...,\n",
       "             [-0.5482,  0.1365,  0.0803,  ...,  0.6361,  0.2287,  0.8213],\n",
       "             [-0.5828,  0.0988,  0.0813,  ...,  0.6508,  0.2441,  0.8078],\n",
       "             [-0.5663,  0.2092,  0.1616,  ...,  0.6884,  0.2180,  0.8841]],\n",
       "   \n",
       "            [[-1.2676,  0.1238,  1.6441,  ...,  0.6552,  0.5986, -0.6159],\n",
       "             [-1.2564,  0.1682,  0.0727,  ...,  1.3396,  0.0904, -0.0703],\n",
       "             [-2.0675, -0.9651, -1.3210,  ..., -0.5665,  0.2615, -0.4879],\n",
       "             ...,\n",
       "             [-1.1885,  0.1517,  1.6193,  ...,  0.6568,  0.5997, -0.5623],\n",
       "             [-1.2557,  0.2071,  1.6382,  ...,  0.6884,  0.6656, -0.5649],\n",
       "             [-1.1847,  0.0532,  1.5700,  ...,  0.6986,  0.6015, -0.6516]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 0.2240, -1.7253, -0.4327,  ...,  1.8239,  0.4225,  1.1732],\n",
       "             [ 1.9483,  0.4489,  0.7388,  ..., -0.6625,  2.9803,  1.1268],\n",
       "             [ 1.5732,  0.5293,  0.5783,  ...,  2.6527,  1.5483, -0.4692],\n",
       "             ...,\n",
       "             [ 0.1644, -1.7309, -0.4943,  ...,  1.7271,  0.3989,  1.1526],\n",
       "             [ 0.1860, -1.6939, -0.5374,  ...,  1.7137,  0.3518,  1.1603],\n",
       "             [ 0.2073, -1.7643, -0.4666,  ...,  1.7159,  0.4534,  1.1821]],\n",
       "   \n",
       "            [[ 1.0338, -0.2888, -0.4415,  ..., -0.9880,  0.8732, -4.5699],\n",
       "             [ 1.6651, -2.1444, -1.0444,  ..., -0.3436,  1.9563,  1.2547],\n",
       "             [ 3.4344,  0.8029,  0.9264,  ...,  0.7004,  1.4906, -0.0572],\n",
       "             ...,\n",
       "             [ 0.9817, -0.2258, -0.3678,  ..., -0.9135,  0.8100, -4.3848],\n",
       "             [ 0.9253, -0.2534, -0.3689,  ..., -0.8736,  0.8527, -4.4113],\n",
       "             [ 1.0425, -0.3155, -0.4407,  ..., -0.9800,  0.8398, -4.4131]],\n",
       "   \n",
       "            [[-0.3838,  0.5943,  0.1609,  ..., -0.5675, -0.5291,  0.4640],\n",
       "             [ 0.4416, -0.6164,  0.4107,  ..., -2.6284, -1.5758,  0.1559],\n",
       "             [ 1.0891, -1.7176,  1.0288,  ..., -0.5500, -3.2686, -0.1488],\n",
       "             ...,\n",
       "             [-0.4655,  0.6676,  0.1905,  ..., -0.4133, -0.4843,  0.4620],\n",
       "             [-0.3788,  0.6793,  0.1481,  ..., -0.4630, -0.4619,  0.4729],\n",
       "             [-0.4047,  0.6310,  0.1285,  ..., -0.4636, -0.5589,  0.4677]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 3.4686e-02, -1.5651e-01, -1.8458e-02,  ...,  4.3042e-01,\n",
       "               2.1959e-02, -2.6768e-01],\n",
       "             [-4.9286e-01,  1.8281e+00, -1.8716e+00,  ..., -1.0051e+00,\n",
       "              -3.5713e+00, -2.2739e+00],\n",
       "             [ 2.7865e+00,  4.1384e-01,  1.7026e+00,  ..., -2.4084e+00,\n",
       "               4.1658e-01, -3.3715e+00],\n",
       "             ...,\n",
       "             [ 2.0311e-01, -1.7635e-01, -1.4653e-01,  ...,  3.2715e-01,\n",
       "               5.8474e-02, -2.1284e-01],\n",
       "             [ 1.1716e-01, -1.9832e-01, -1.1369e-01,  ...,  3.6320e-01,\n",
       "               7.4892e-02, -1.8311e-01],\n",
       "             [ 1.7936e-01, -2.0605e-01, -8.4444e-02,  ...,  4.1520e-01,\n",
       "               3.4130e-02, -1.8895e-01]],\n",
       "   \n",
       "            [[ 2.5455e-01,  3.3857e-01, -2.2610e-01,  ..., -1.4401e-01,\n",
       "              -8.9893e-03,  1.2588e-01],\n",
       "             [ 7.0408e-01,  5.3899e-01,  2.3221e-01,  ..., -2.5204e+00,\n",
       "               1.5602e-01,  2.1669e-01],\n",
       "             [-4.9590e-01, -9.0038e-01, -3.4354e+00,  ..., -1.9064e+00,\n",
       "               7.7523e-02,  1.5638e+00],\n",
       "             ...,\n",
       "             [ 3.1350e-01,  3.0044e-01, -1.9818e-01,  ..., -2.0013e-02,\n",
       "              -1.7540e-01,  2.7715e-01],\n",
       "             [ 2.3488e-01,  2.9283e-01, -8.7802e-02,  ...,  8.5217e-02,\n",
       "              -1.3317e-01,  2.6670e-01],\n",
       "             [ 3.8770e-01,  3.4191e-01, -1.5229e-01,  ..., -1.7957e-01,\n",
       "               9.2390e-02,  2.7312e-01]],\n",
       "   \n",
       "            [[ 5.4713e-02,  8.8697e-02,  7.0403e-04,  ...,  2.1715e-01,\n",
       "               2.7648e-02,  1.2691e-02],\n",
       "             [ 1.5253e+00,  1.9576e+00, -7.9870e-01,  ...,  2.8274e+00,\n",
       "              -8.5293e-01, -9.0470e-01],\n",
       "             [ 1.1245e+00,  1.8009e+00, -2.1133e+00,  ...,  4.1303e+00,\n",
       "              -2.7507e+00, -4.5972e-02],\n",
       "             ...,\n",
       "             [-9.0604e-02,  2.2896e-01,  6.2277e-02,  ...,  1.8244e-01,\n",
       "               9.0612e-03,  1.7588e-01],\n",
       "             [ 1.2695e-02,  1.4649e-01,  7.6540e-02,  ...,  2.0443e-01,\n",
       "               5.9164e-02,  5.6259e-02],\n",
       "             [ 3.6623e-02,  2.1575e-01,  1.0846e-01,  ...,  2.4905e-01,\n",
       "              -5.9918e-02,  1.0241e-02]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 1.5566e-01,  3.8722e-01,  3.1183e-02,  ...,  4.6096e-02,\n",
       "              -2.6916e-01,  4.6193e-03],\n",
       "             [-1.1209e+00,  1.1326e+00, -2.9464e+00,  ...,  1.4358e+00,\n",
       "              -9.2657e-01, -9.0711e-01],\n",
       "             [-1.9829e+00, -2.9599e+00, -8.5311e-01,  ...,  2.7500e-01,\n",
       "               3.8513e+00, -1.8533e-01],\n",
       "             ...,\n",
       "             [ 1.3631e-01,  3.2915e-01,  2.4928e-02,  ...,  1.1508e-01,\n",
       "              -2.1576e-01,  6.2394e-02],\n",
       "             [ 1.0216e-01,  3.8343e-01,  4.0972e-02,  ...,  9.5340e-02,\n",
       "              -3.0263e-01, -9.8470e-03],\n",
       "             [ 1.7830e-01,  4.0362e-01,  1.4370e-01,  ...,  1.7344e-02,\n",
       "              -3.2822e-01,  2.5790e-02]],\n",
       "   \n",
       "            [[ 8.5630e-02,  4.2262e-02,  5.1861e-02,  ..., -1.8302e-01,\n",
       "              -4.0989e-02, -8.1349e-02],\n",
       "             [ 1.8454e+00,  9.9345e-01, -4.2946e-01,  ...,  8.0232e-01,\n",
       "               6.3218e-01,  2.0174e+00],\n",
       "             [-3.2335e-01, -5.0547e-01, -6.4934e-01,  ..., -3.1018e-01,\n",
       "              -1.7591e+00,  1.4533e+00],\n",
       "             ...,\n",
       "             [ 5.0520e-02,  3.1989e-02,  3.7172e-02,  ..., -2.5490e-02,\n",
       "              -2.1937e-02,  3.3171e-02],\n",
       "             [ 9.4309e-02,  6.9534e-02,  6.3819e-02,  ..., -1.1394e-01,\n",
       "              -1.0740e-02,  6.0112e-03],\n",
       "             [ 1.1889e-01, -5.8567e-02,  1.0820e-01,  ..., -8.0816e-02,\n",
       "              -2.1751e-02, -1.6318e-02]],\n",
       "   \n",
       "            [[ 4.5687e-03, -3.0422e-01, -1.8099e-02,  ..., -3.3873e-01,\n",
       "              -2.6789e-01,  4.8163e-01],\n",
       "             [-8.1359e-01, -1.9162e-01, -4.1920e-02,  ..., -9.7373e-01,\n",
       "              -4.4989e-01,  1.5203e+00],\n",
       "             [-2.7455e-01, -1.4831e-01, -6.4613e-03,  ..., -4.8701e-01,\n",
       "              -3.1457e-01, -1.6953e+00],\n",
       "             ...,\n",
       "             [ 1.3462e-02, -3.1837e-01, -8.5138e-02,  ..., -4.2155e-01,\n",
       "              -2.6052e-01,  4.7293e-01],\n",
       "             [ 1.2238e-02, -3.4641e-01, -2.1015e-02,  ..., -4.2697e-01,\n",
       "              -2.9071e-01,  4.9631e-01],\n",
       "             [ 4.2348e-02, -3.0168e-01, -4.8035e-02,  ..., -4.4359e-01,\n",
       "              -1.9811e-01,  4.9293e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 4.0202e-01, -1.1367e+00, -3.0007e-01,  ..., -2.8914e+00,\n",
       "              -6.2519e-01,  5.7466e-01],\n",
       "             [-1.5474e+00, -1.2199e+00, -4.9498e-02,  ..., -1.5943e+00,\n",
       "              -1.3852e+00, -1.1007e+00],\n",
       "             [-3.6958e-01, -1.2338e+00,  2.2955e+00,  ..., -2.7398e+00,\n",
       "              -6.9207e-01, -2.1834e-01],\n",
       "             ...,\n",
       "             [-1.0799e+00,  1.5299e+00,  3.5697e+00,  ...,  1.3980e+00,\n",
       "               4.9417e-01, -1.8259e+00],\n",
       "             [ 2.5331e-01, -1.6098e-01,  1.4016e+00,  ...,  2.8422e+00,\n",
       "               5.9416e-01, -1.7231e+00],\n",
       "             [-3.0296e-01,  6.4795e-01,  3.0809e+00,  ...,  1.0669e+00,\n",
       "               1.7547e-01, -1.8873e+00]],\n",
       "   \n",
       "            [[-2.3358e-01, -1.0035e+00,  6.3752e-01,  ..., -2.1552e-01,\n",
       "              -2.5118e+00, -1.4479e+00],\n",
       "             [-2.5183e+00, -6.2644e-01,  6.2968e-01,  ..., -3.2651e+00,\n",
       "              -3.8112e+00, -1.2031e+00],\n",
       "             [-1.8275e+00,  8.8506e-04,  1.5911e+00,  ..., -1.0431e+00,\n",
       "              -4.3829e-01, -8.1671e-01],\n",
       "             ...,\n",
       "             [-6.0018e-01,  2.7340e+00,  2.4886e+00,  ...,  9.4154e-01,\n",
       "              -2.1776e+00,  2.0070e-02],\n",
       "             [-1.6337e+00,  1.6413e+00,  3.2838e+00,  ...,  5.3083e-01,\n",
       "              -3.3404e-01, -1.5280e-01],\n",
       "             [ 7.4435e-01,  2.1523e+00,  1.9480e+00,  ...,  1.2998e+00,\n",
       "              -1.2764e+00,  4.5438e-01]],\n",
       "   \n",
       "            [[ 6.1350e-01,  7.9554e-01, -1.2434e+00,  ...,  1.4802e+00,\n",
       "               2.4632e+00, -2.3551e+00],\n",
       "             [-9.9219e-01,  2.0163e-01,  5.7654e-01,  ...,  1.2714e+00,\n",
       "              -7.4928e-01, -2.1860e+00],\n",
       "             [-1.5081e-01,  1.3853e-01, -1.2854e+00,  ...,  2.4040e+00,\n",
       "               9.5782e-01, -4.3801e-01],\n",
       "             ...,\n",
       "             [-9.8851e-01, -2.8308e-01,  4.4329e-01,  ..., -8.6556e-01,\n",
       "               2.7798e+00, -6.3919e+00],\n",
       "             [-5.9265e-01, -1.7686e-01,  1.5043e+00,  ...,  4.9366e-02,\n",
       "               2.7475e+00, -3.6637e+00],\n",
       "             [ 1.1144e+00, -8.7482e-01,  7.2682e-03,  ...,  1.3038e+00,\n",
       "               2.8778e+00, -3.5574e+00]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-7.7725e-01,  1.7081e+00, -2.7742e+00,  ...,  2.5592e+00,\n",
       "              -1.5036e+00,  2.5356e+00],\n",
       "             [-3.2915e-01, -2.6492e+00,  1.0684e-01,  ...,  1.4533e+00,\n",
       "               6.5981e-01,  2.0299e+00],\n",
       "             [-3.4177e-01, -6.2891e-01, -1.9634e+00,  ...,  2.0801e+00,\n",
       "              -1.4409e+00,  4.4963e+00],\n",
       "             ...,\n",
       "             [-1.8350e+00,  3.3907e+00, -4.2449e+00,  ..., -4.3109e-01,\n",
       "               2.3920e+00,  2.7255e+00],\n",
       "             [ 9.2497e-01,  2.0091e+00, -2.7391e+00,  ..., -2.3383e-01,\n",
       "               1.6960e+00,  1.6275e+00],\n",
       "             [-6.3918e-01,  2.3178e+00, -2.4046e+00,  ...,  6.5217e-01,\n",
       "               1.6420e+00,  1.5801e+00]],\n",
       "   \n",
       "            [[ 5.0456e+00,  1.3547e+00, -7.8339e-01,  ...,  5.9111e-02,\n",
       "               1.9070e+00, -4.2823e+00],\n",
       "             [-1.7595e+00, -2.5214e+00,  1.6792e+00,  ..., -6.0374e-01,\n",
       "              -5.8351e-02,  2.1470e-01],\n",
       "             [-5.0932e+00, -3.3155e-01,  2.6941e+00,  ...,  1.9870e+00,\n",
       "              -7.3795e-02, -1.9014e-01],\n",
       "             ...,\n",
       "             [ 1.6266e-01,  5.5788e+00,  2.4312e+00,  ..., -7.8986e-01,\n",
       "               4.3730e-01, -1.9814e+00],\n",
       "             [-5.5686e-01,  4.0152e+00,  2.5931e+00,  ..., -2.3016e-01,\n",
       "               7.0101e-01, -9.6551e-01],\n",
       "             [ 2.2257e-01,  4.7028e+00,  1.9040e+00,  ...,  2.8256e-02,\n",
       "               1.2034e+00, -1.6850e+00]],\n",
       "   \n",
       "            [[ 1.9688e-01,  1.3012e+00, -5.2664e-01,  ...,  3.4039e-01,\n",
       "              -8.3445e-01,  6.5092e+00],\n",
       "             [ 1.1947e+00, -1.9945e+00, -6.4101e-01,  ...,  1.3728e+00,\n",
       "              -9.7356e-01,  2.9000e+00],\n",
       "             [ 3.0095e+00, -2.0438e+00, -1.2627e+00,  ...,  1.3146e+00,\n",
       "              -2.8218e+00,  1.7126e+00],\n",
       "             ...,\n",
       "             [-1.8261e+00,  3.0407e+00,  3.3392e-01,  ..., -1.7536e+00,\n",
       "               2.0263e+00,  1.7186e+00],\n",
       "             [-2.5286e+00,  2.3843e+00, -4.0175e-01,  ..., -1.9483e+00,\n",
       "               1.7744e+00,  1.2488e+00],\n",
       "             [-2.6583e+00,  5.8959e-01,  8.5433e-01,  ..., -9.0116e-01,\n",
       "               2.7838e+00,  2.7344e+00]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ -2.4501,  -0.1900,  -0.1288,  ...,   3.4945,   2.6651,   7.1365],\n",
       "             [ -0.9170,  -5.3574,  -2.3128,  ...,   5.3896,  -2.9625,   3.2496],\n",
       "             [ -2.6801,  -0.3109,   1.1474,  ...,   1.8378,  -3.9988,  -1.4269],\n",
       "             ...,\n",
       "             [  1.9480,  -4.7577,   9.3656,  ...,  -1.1684,  -2.0149,  -0.5683],\n",
       "             [ -1.7979,  -3.7232,   8.1681,  ...,  -0.1143,   3.5608,  -1.4132],\n",
       "             [  1.1161,  -3.4913,   3.4351,  ...,   0.8952,   0.5623,  -2.7352]],\n",
       "   \n",
       "            [[ -1.8568,  -0.7799,  -5.0955,  ...,  -3.7694,   4.2004,   1.6228],\n",
       "             [  1.8008,  -2.2895,  -3.9671,  ...,  -0.3073,   8.1520,  -1.9240],\n",
       "             [  0.5105,  -2.6629,  -1.0730,  ...,  -1.6757,   6.7981,   2.5052],\n",
       "             ...,\n",
       "             [ -1.4335,   3.6659,  -4.8339,  ...,   0.2939,   3.5270,  -1.1956],\n",
       "             [ -0.6880,   2.7351,  -5.4843,  ...,  -1.0850,   2.7022,   2.2008],\n",
       "             [ -1.7896,   3.3664,  -3.2688,  ...,  -1.7198,   1.9126,  -0.4527]],\n",
       "   \n",
       "            [[ -6.3621,  -3.6987,   2.5042,  ...,   5.1554,   0.1001,  -4.3698],\n",
       "             [ -0.5670,   1.3781,   4.4637,  ...,  -0.2258,  -0.7431, -14.5474],\n",
       "             [  0.8912,   4.5741,   2.3070,  ...,  -1.7836,   3.5117,  -9.1431],\n",
       "             ...,\n",
       "             [  2.2451,   1.4770,   3.0076,  ...,  -3.0963,   3.3201,   5.0893],\n",
       "             [ -3.0472,   0.8282,   6.1367,  ...,   0.1524,   1.8450,   3.0282],\n",
       "             [  0.2748,  -0.2589,   3.1467,  ...,  -0.0161,   2.9789,   5.3230]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ -0.7017,   0.0782,   0.0435,  ...,  -1.6847,  -1.3356,   1.6888],\n",
       "             [ -5.2296,   2.6760,   1.9637,  ...,  -0.4354,  -2.9683,  -4.5108],\n",
       "             [ -0.0861,  -2.0428,   1.8582,  ...,  -0.9038,  -6.0565,  -1.6390],\n",
       "             ...,\n",
       "             [  2.5004,   5.2748,  -2.1817,  ...,  -0.8699,  -6.2070,  -3.2049],\n",
       "             [  2.3364,   3.5305,  -2.7904,  ...,  -2.3514,  -6.9053,  -1.7005],\n",
       "             [  1.2915,   4.2676,  -1.8141,  ...,  -0.6554,  -6.6796,  -5.1856]],\n",
       "   \n",
       "            [[ -1.4809,   4.7043,  -0.5175,  ...,   0.2765,  -1.5128,   0.7506],\n",
       "             [ -2.9220,   5.8203,   1.1108,  ...,  -0.7934,  -3.0395,   5.3420],\n",
       "             [ -5.8367,  -2.8662,  -6.2247,  ...,  -4.7902,   9.0694,  -1.4402],\n",
       "             ...,\n",
       "             [  5.9083,   1.7989,  -0.2582,  ...,   4.2166,  -4.7581,  -0.0604],\n",
       "             [  6.4188,   5.6528,   0.9781,  ...,   1.4991,  -4.1062,   1.4770],\n",
       "             [  8.2726,   1.9987,  -0.7871,  ...,   2.5459,  -4.2346,   0.2432]],\n",
       "   \n",
       "            [[  3.8137,   5.5936,  -0.4736,  ...,  -1.6983,  -3.3620,  -0.2507],\n",
       "             [  1.4547,   0.9528,   1.8492,  ...,  -9.3152,  -3.4382,   5.1968],\n",
       "             [  0.7969,   4.1394,  -1.4731,  ...,   0.3963,  -1.6951,   1.1032],\n",
       "             ...,\n",
       "             [  1.6218,   2.9049,  -0.9202,  ...,   1.1138,   1.8713,   4.6222],\n",
       "             [  1.8390,   0.6277,  -1.9904,  ...,   1.0874,   4.2371,   5.0699],\n",
       "             [  3.1001,   3.3416,  -3.8469,  ...,  -1.0471,   2.1959,   1.2323]]]],\n",
       "          grad_fn=<TransposeBackward0>)),\n",
       "  (tensor([[[[ 2.1693e+00,  2.2075e+00,  2.3156e+00,  ...,  2.0419e+00,\n",
       "              -6.1075e-01,  7.5214e-02],\n",
       "             [ 2.4136e+00, -2.5052e+00, -1.0461e+00,  ..., -2.1787e+00,\n",
       "               6.8849e-01, -4.4043e-01],\n",
       "             [ 6.5473e-01, -1.1415e+00, -2.3573e-01,  ..., -2.2693e+00,\n",
       "               6.5307e-01,  5.9205e-04],\n",
       "             ...,\n",
       "             [ 2.1793e+00,  2.2828e+00,  2.1216e+00,  ...,  2.0380e+00,\n",
       "              -5.3727e-01,  2.1726e-01],\n",
       "             [ 2.1468e+00,  2.3837e+00,  2.2381e+00,  ...,  2.0154e+00,\n",
       "              -5.4110e-01,  1.5536e-01],\n",
       "             [ 2.0859e+00,  2.2451e+00,  2.0652e+00,  ...,  2.0046e+00,\n",
       "              -5.5674e-01,  1.2120e-01]],\n",
       "   \n",
       "            [[-6.7559e-01,  7.2644e-03, -6.4456e-01,  ...,  5.8635e-01,\n",
       "              -1.0231e+00,  7.2140e-01],\n",
       "             [-1.2175e+00, -6.0224e-01,  1.5900e+00,  ...,  1.1934e+00,\n",
       "              -2.7624e+00,  3.3150e+00],\n",
       "             [ 1.3209e+00, -5.1934e-01, -3.0199e-01,  ...,  1.0175e+00,\n",
       "              -1.2154e+00,  1.6625e+00],\n",
       "             ...,\n",
       "             [-7.0758e-01, -2.4820e-02, -5.9648e-01,  ...,  4.8015e-01,\n",
       "              -9.6788e-01,  6.5552e-01],\n",
       "             [-6.7943e-01, -5.0234e-02, -5.9837e-01,  ...,  6.1946e-01,\n",
       "              -9.8035e-01,  6.6357e-01],\n",
       "             [-6.2473e-01, -1.5792e-02, -5.3233e-01,  ...,  6.7164e-01,\n",
       "              -1.1738e+00,  8.0092e-01]],\n",
       "   \n",
       "            [[ 8.4005e-01, -3.4134e+00, -7.5924e-01,  ...,  1.4136e+00,\n",
       "               5.5079e-01,  1.0044e+00],\n",
       "             [-1.0778e+00,  8.1695e-01,  9.0514e-01,  ..., -3.7276e-01,\n",
       "               1.1195e+00, -8.3098e-01],\n",
       "             [-7.0490e-01, -1.9306e+00, -2.0883e-01,  ..., -1.0295e-01,\n",
       "               1.3913e+00,  8.8089e-01],\n",
       "             ...,\n",
       "             [ 9.4881e-01, -3.4145e+00, -7.9790e-01,  ...,  1.2852e+00,\n",
       "               5.9040e-01,  9.7642e-01],\n",
       "             [ 9.3856e-01, -3.3917e+00, -7.8827e-01,  ...,  1.3683e+00,\n",
       "               5.4644e-01,  9.1596e-01],\n",
       "             [ 7.6039e-01, -3.3370e+00, -7.2198e-01,  ...,  1.2251e+00,\n",
       "               6.0036e-01,  8.7313e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-1.2894e+00, -8.9995e-02, -2.4900e-01,  ..., -2.0263e+00,\n",
       "               1.3444e+00, -1.5140e+00],\n",
       "             [-2.7486e-01, -1.8941e+00, -2.9755e+00,  ..., -9.6548e-01,\n",
       "               1.0896e+00, -8.5485e-01],\n",
       "             [ 8.1652e-01, -3.1802e+00, -1.8227e+00,  ..., -4.9836e-01,\n",
       "               7.5709e-01, -2.6674e+00],\n",
       "             ...,\n",
       "             [-1.2200e+00, -1.4422e-02, -2.5594e-01,  ..., -2.1368e+00,\n",
       "               1.3275e+00, -1.5944e+00],\n",
       "             [-1.2424e+00, -4.2848e-02, -1.5975e-01,  ..., -2.1369e+00,\n",
       "               1.2941e+00, -1.6237e+00],\n",
       "             [-1.2871e+00, -5.6338e-02, -4.1311e-01,  ..., -2.0847e+00,\n",
       "               1.2903e+00, -1.5892e+00]],\n",
       "   \n",
       "            [[ 6.0004e-01, -2.3873e-01, -7.7384e-02,  ..., -5.6978e-01,\n",
       "              -1.1598e-01, -3.1452e-01],\n",
       "             [ 1.3295e+00,  2.9993e-01,  1.3555e+00,  ...,  5.1770e-01,\n",
       "               1.1443e-01,  4.1899e-01],\n",
       "             [ 1.9594e+00,  2.3428e+00,  4.9626e-01,  ...,  5.2979e-01,\n",
       "              -4.9476e-01, -9.5074e-02],\n",
       "             ...,\n",
       "             [ 5.8469e-01, -2.1811e-01, -7.4563e-02,  ..., -6.0297e-01,\n",
       "              -3.9198e-02, -3.7920e-01],\n",
       "             [ 6.2064e-01, -2.0770e-01, -1.8857e-02,  ..., -6.0823e-01,\n",
       "              -1.4256e-01, -3.4092e-01],\n",
       "             [ 6.7817e-01, -2.0804e-01, -4.7314e-02,  ..., -5.5283e-01,\n",
       "              -7.3396e-02, -3.8384e-01]],\n",
       "   \n",
       "            [[-1.1295e-01,  4.1982e-01,  2.0355e+00,  ...,  3.5716e-01,\n",
       "               8.4471e-02, -4.4297e-01],\n",
       "             [-4.8361e-01,  3.4868e-01,  2.5190e+00,  ..., -1.9750e-01,\n",
       "              -1.5965e+00, -2.1819e+00],\n",
       "             [ 2.6141e-01, -1.1451e+00,  4.4322e-01,  ...,  1.8283e-01,\n",
       "              -8.3039e-02, -2.9343e-01],\n",
       "             ...,\n",
       "             [-2.2138e-01,  3.8656e-01,  1.9703e+00,  ...,  4.3390e-01,\n",
       "               1.1178e-01, -4.2386e-01],\n",
       "             [-1.7242e-01,  3.2570e-01,  1.9833e+00,  ...,  4.2317e-01,\n",
       "               1.2122e-01, -3.6896e-01],\n",
       "             [-1.8841e-01,  4.1407e-01,  1.9684e+00,  ...,  4.3213e-01,\n",
       "              -1.4108e-01, -5.0482e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 9.7014e-02, -6.2723e-01, -7.2687e-01,  ...,  4.8874e-01,\n",
       "              -8.2000e-01,  4.6419e-01],\n",
       "             [ 3.6310e-01, -7.1054e-01,  7.2676e-01,  ..., -1.2132e-01,\n",
       "              -1.5583e+00,  2.6550e-01],\n",
       "             [-3.0518e+00, -3.6053e-01, -3.1420e-01,  ...,  1.3886e+00,\n",
       "               2.2510e-01, -2.3353e+00],\n",
       "             ...,\n",
       "             [-2.1837e-02, -7.4405e-01, -7.5339e-01,  ...,  3.1533e-01,\n",
       "              -8.8483e-01,  4.6864e-01],\n",
       "             [ 1.4515e-01, -6.2230e-01, -6.3851e-01,  ...,  5.2239e-01,\n",
       "              -7.7102e-01,  5.5315e-01],\n",
       "             [ 3.5555e-01, -7.2066e-01, -5.3937e-01,  ...,  1.4495e-01,\n",
       "              -8.5533e-01,  3.7806e-01]],\n",
       "   \n",
       "            [[-1.4970e-02, -6.8731e-02, -2.7707e-02,  ..., -1.6189e-01,\n",
       "              -4.0625e-01, -8.5000e-01],\n",
       "             [-3.0134e+00,  6.8902e-01,  9.8210e-01,  ..., -2.4353e+00,\n",
       "               1.0380e+00, -1.9877e+00],\n",
       "             [ 1.9077e+00, -8.8778e-01,  6.5942e-01,  ...,  1.7063e+00,\n",
       "              -1.8378e+00, -2.5025e+00],\n",
       "             ...,\n",
       "             [ 6.0319e-02, -1.1118e-01, -1.5438e-02,  ..., -5.0106e-02,\n",
       "              -4.5517e-01, -7.0079e-01],\n",
       "             [-1.0643e-02, -1.1685e-01, -1.2818e-01,  ..., -1.7311e-01,\n",
       "              -3.4059e-01, -9.1129e-01],\n",
       "             [-9.4895e-03, -3.3861e-01,  9.8970e-02,  ..., -3.3126e-01,\n",
       "              -2.9229e-01, -9.4882e-01]],\n",
       "   \n",
       "            [[ 1.0739e-01,  5.2771e-01, -2.3893e-01,  ..., -4.0194e-01,\n",
       "              -5.8520e-02, -2.4428e-02],\n",
       "             [ 9.0810e-01, -1.5532e+00,  7.4828e-01,  ...,  1.4392e+00,\n",
       "               2.7444e+00, -4.5653e-01],\n",
       "             [-7.5211e-01,  9.7044e-01, -2.6759e+00,  ...,  2.8532e+00,\n",
       "              -1.8242e+00, -1.0845e-01],\n",
       "             ...,\n",
       "             [ 1.0794e-01,  5.9856e-01, -1.8304e-01,  ..., -4.3588e-01,\n",
       "              -1.4401e-01,  3.2508e-02],\n",
       "             [ 5.7764e-02,  5.8629e-01, -1.0986e-01,  ..., -3.6923e-01,\n",
       "              -1.1651e-01,  1.3560e-02],\n",
       "             [ 1.5252e-01,  6.3048e-01, -1.3957e-01,  ..., -5.2492e-01,\n",
       "              -1.5990e-01,  1.0510e-03]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-2.1599e-01, -1.2769e-01,  8.4936e-02,  ..., -1.8208e-01,\n",
       "              -1.2201e-01,  3.8734e-01],\n",
       "             [ 1.9565e-01, -2.5833e+00, -7.8689e-01,  ...,  1.4263e+00,\n",
       "              -4.0279e-01,  3.6606e-01],\n",
       "             [ 3.7955e-01, -1.4630e+00,  2.5214e-01,  ..., -1.9313e+00,\n",
       "               7.0812e-02, -2.7273e+00],\n",
       "             ...,\n",
       "             [-6.4999e-02, -1.6342e-01,  4.2657e-02,  ..., -1.7541e-01,\n",
       "              -3.1182e-01,  5.1203e-01],\n",
       "             [-9.3909e-02, -4.4970e-02,  6.2569e-02,  ..., -3.1091e-01,\n",
       "              -1.6919e-01,  4.4420e-01],\n",
       "             [-1.7244e-01, -3.2637e-01, -1.1221e-02,  ..., -2.4626e-01,\n",
       "              -8.8286e-02,  5.1772e-01]],\n",
       "   \n",
       "            [[ 2.6519e-01,  1.1030e-01,  1.6253e-01,  ..., -2.4499e-01,\n",
       "              -8.3139e-02,  1.1002e-01],\n",
       "             [-8.4891e-02, -1.5335e+00, -1.8858e+00,  ...,  1.8299e+00,\n",
       "              -1.2330e+00, -2.8917e+00],\n",
       "             [-6.2550e+00,  3.5452e+00,  2.1335e+00,  ...,  2.7866e+00,\n",
       "               4.1475e+00, -7.0290e-02],\n",
       "             ...,\n",
       "             [ 1.8112e-01,  1.8555e-02,  1.9057e-01,  ..., -2.6425e-01,\n",
       "               2.9095e-02,  1.8342e-01],\n",
       "             [ 1.9629e-01,  2.6019e-02,  2.9475e-01,  ..., -2.9386e-01,\n",
       "              -9.9290e-03,  1.2498e-01],\n",
       "             [ 2.7557e-01,  1.6210e-01,  2.0694e-01,  ..., -2.2603e-01,\n",
       "               6.9253e-02,  2.1987e-01]],\n",
       "   \n",
       "            [[-1.2157e-01, -1.4800e-02,  3.8942e-02,  ..., -1.1866e-01,\n",
       "              -2.8461e-02, -2.4638e-01],\n",
       "             [ 1.9708e+00,  7.3609e-01, -5.2352e-01,  ..., -2.1339e+00,\n",
       "              -1.0580e-01, -5.3546e-01],\n",
       "             [-3.6269e+00, -2.1529e+00,  1.8467e+00,  ...,  1.4801e+00,\n",
       "              -7.2689e-02,  2.1094e+00],\n",
       "             ...,\n",
       "             [-2.7749e-01, -2.2299e-01, -4.1867e-02,  ..., -1.2551e-01,\n",
       "               6.9187e-02, -3.9241e-01],\n",
       "             [-2.3062e-01, -8.6058e-02, -1.5625e-01,  ..., -7.1403e-02,\n",
       "              -2.1217e-02, -2.2080e-01],\n",
       "             [-2.2785e-02, -6.5655e-02,  3.9476e-02,  ..., -2.2851e-01,\n",
       "              -9.2901e-03, -3.1932e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 7.4456e-01, -1.3725e+00,  1.7051e+00,  ...,  2.5035e+00,\n",
       "              -5.8251e-01, -5.2613e+00],\n",
       "             [ 3.0105e+00, -1.2394e+00,  3.6446e-01,  ...,  1.7209e+00,\n",
       "               1.6063e-01, -1.0067e+01],\n",
       "             [ 1.1298e+00, -3.0700e+00,  1.0460e+00,  ...,  1.4334e-02,\n",
       "              -2.4663e+00, -1.0742e+01],\n",
       "             ...,\n",
       "             [ 8.1460e-01, -7.7174e-01, -3.2483e-01,  ...,  4.0738e+00,\n",
       "              -5.1639e+00, -3.0068e+00],\n",
       "             [ 6.8041e-01,  2.7524e+00, -1.5184e+00,  ...,  3.2956e+00,\n",
       "              -5.4436e+00, -3.0290e+00],\n",
       "             [ 9.4466e-01,  1.0169e+00,  6.2451e-01,  ...,  4.3546e+00,\n",
       "              -4.1893e+00, -4.2098e+00]],\n",
       "   \n",
       "            [[ 1.5269e+00,  2.2308e-01,  8.8455e-01,  ..., -4.3935e-01,\n",
       "              -4.8909e+00, -2.8081e+00],\n",
       "             [-4.1782e+00,  3.4385e+00,  3.3400e-01,  ..., -3.3331e-02,\n",
       "              -5.6238e+00,  1.1559e+00],\n",
       "             [-4.6495e+00,  1.1673e+00, -1.1603e+00,  ..., -9.0060e-02,\n",
       "              -4.7976e+00, -1.2280e+00],\n",
       "             ...,\n",
       "             [ 1.1609e+00,  1.1024e+00,  4.9091e-01,  ..., -7.5765e-01,\n",
       "               2.0876e+00,  9.9423e-01],\n",
       "             [ 2.1410e-02,  1.0769e+00,  8.0745e-01,  ...,  1.5007e+00,\n",
       "               9.3417e-01,  2.9094e-02],\n",
       "             [ 5.7852e-01,  1.8835e+00,  1.2211e-01,  ...,  2.0047e-01,\n",
       "               9.0367e-01,  1.4184e-01]],\n",
       "   \n",
       "            [[-2.2541e+00, -1.9666e+00, -5.5798e-01,  ..., -2.2596e+00,\n",
       "               1.3353e+00,  9.0157e-01],\n",
       "             [-3.5833e+00,  6.0084e-01,  1.3963e+00,  ..., -1.6174e+00,\n",
       "              -1.1754e+00,  1.7366e+00],\n",
       "             [-2.3379e+00, -2.2017e-01,  1.9120e+00,  ..., -4.1501e+00,\n",
       "              -2.2657e-02,  2.4403e+00],\n",
       "             ...,\n",
       "             [-1.0335e+00,  2.4087e+00,  7.2071e-02,  ...,  8.5309e-01,\n",
       "               2.3610e+00,  1.8675e-01],\n",
       "             [ 8.3391e-01,  2.4635e+00, -4.0849e-01,  ..., -3.8396e-01,\n",
       "               9.4024e-01,  7.6240e-01],\n",
       "             [ 5.3981e-01,  3.2362e+00, -6.8760e-01,  ..., -1.0753e+00,\n",
       "              -1.9619e-01,  5.9847e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-1.2963e+00,  8.0780e-01, -3.0756e+00,  ...,  3.7476e+00,\n",
       "               1.4092e+00,  2.0107e+00],\n",
       "             [-6.3082e-01,  1.8409e+00, -4.0397e-01,  ...,  3.1717e-01,\n",
       "              -8.0275e-01,  3.1404e+00],\n",
       "             [-1.9534e+00,  3.4387e+00, -7.2212e-01,  ..., -1.1188e+00,\n",
       "              -7.0183e-01,  2.5348e+00],\n",
       "             ...,\n",
       "             [ 2.6416e+00, -4.9901e-02, -2.5138e+00,  ...,  1.8238e+00,\n",
       "              -1.3668e+00,  1.5023e-02],\n",
       "             [ 2.1803e+00, -1.2914e+00, -1.1982e+00,  ...,  2.1439e+00,\n",
       "               5.6968e-01,  9.3432e-01],\n",
       "             [ 1.9493e+00,  4.4041e-01, -3.0066e-01,  ...,  9.1540e-02,\n",
       "               2.4770e-01,  1.3024e+00]],\n",
       "   \n",
       "            [[ 9.2612e-01, -3.3718e+00, -7.2804e-01,  ...,  1.7913e+00,\n",
       "              -5.6732e-03,  1.2176e+00],\n",
       "             [-1.9583e-01, -2.0911e+00, -3.1948e-01,  ...,  1.6208e+00,\n",
       "              -2.9755e+00,  6.6129e-01],\n",
       "             [ 1.1357e+00, -3.0853e+00, -4.1376e+00,  ..., -5.4017e-02,\n",
       "              -4.3000e+00,  3.9793e+00],\n",
       "             ...,\n",
       "             [-1.4706e+00, -1.2680e+00, -2.6228e+00,  ..., -1.0759e+00,\n",
       "               3.8532e+00,  3.1176e+00],\n",
       "             [-2.2704e+00, -2.9640e-01, -1.0855e+00,  ..., -6.8797e-01,\n",
       "               3.6401e+00,  4.8099e+00],\n",
       "             [-3.9163e-01, -1.0558e+00, -2.4602e+00,  ..., -6.2571e-01,\n",
       "               2.1383e+00,  5.5132e+00]],\n",
       "   \n",
       "            [[-4.6429e+00,  2.3292e+00,  7.6553e-02,  ...,  3.1146e+00,\n",
       "              -1.8362e+00,  3.1886e+00],\n",
       "             [-2.9784e+00, -3.7620e+00,  2.9336e+00,  ...,  4.2520e+00,\n",
       "               2.0308e+00, -9.3766e-01],\n",
       "             [-3.6199e+00,  2.1072e+00,  8.2502e-01,  ...,  2.7576e+00,\n",
       "               6.1373e+00,  1.1483e+00],\n",
       "             ...,\n",
       "             [-2.5882e+00,  9.7994e-01,  6.5300e+00,  ...,  5.7658e-01,\n",
       "              -4.2307e-01,  1.0074e+00],\n",
       "             [-2.9902e+00,  2.0403e+00,  4.6812e+00,  ...,  2.6512e+00,\n",
       "              -9.4627e-01,  7.8003e-01],\n",
       "             [-3.1316e+00,  1.2029e+00,  5.8578e+00,  ...,  5.1122e-03,\n",
       "               6.8565e-02,  8.3022e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ -0.4657, -10.0814,   2.1867,  ...,   2.3004,   5.1621,  -3.1760],\n",
       "             [ -2.4434,   0.7747,   0.7054,  ...,   4.2445,   2.8795,  -2.7560],\n",
       "             [ -8.9503,   3.3147,   4.2326,  ...,  -2.5163,   0.6123,   1.7490],\n",
       "             ...,\n",
       "             [ -0.4672,   8.2211,  -4.4144,  ...,   0.7535,  11.0939,   0.7065],\n",
       "             [ -2.7234,   6.3162,  -1.4166,  ...,  -2.2440,  12.3534,   3.7737],\n",
       "             [ -0.0643,  11.5113,  -8.0783,  ...,  -3.7774,  10.7914,   0.7364]],\n",
       "   \n",
       "            [[  7.8202,   1.7609,   7.0546,  ...,   0.7622,  -1.4136,  -7.2757],\n",
       "             [  4.4843,  -3.1224,  -1.5529,  ...,   6.3087,  -1.0362,  -3.5283],\n",
       "             [ -2.7437,  -5.3436,  -2.4011,  ...,   2.8277,  -1.9702,  -4.9839],\n",
       "             ...,\n",
       "             [ -6.4272,   1.9352,   0.3565,  ...,  -2.8351,   6.7535,   0.3171],\n",
       "             [ -2.9173,   4.6736,   1.6373,  ...,  -1.0288,   5.6307,   0.6905],\n",
       "             [ -1.0635,   6.8312,   1.9954,  ...,  -1.4966,   1.2330,  -3.8742]],\n",
       "   \n",
       "            [[  2.7280,  -7.9140,  -8.7144,  ...,  -1.2453,   2.1887,   4.9016],\n",
       "             [  0.4307,  -3.8529,  -4.8522,  ...,  -2.4542,   1.6726,   6.0162],\n",
       "             [  3.7514,  -4.5382,  -5.3130,  ...,  -1.1048,   4.3268,   4.0264],\n",
       "             ...,\n",
       "             [ -7.9747,  -0.1609,  -0.7209,  ...,   0.9750,  -6.9261,  -2.2244],\n",
       "             [ -4.6454,   2.4975,  -0.2393,  ...,   0.4969,  -5.5276,  -2.9935],\n",
       "             [ -3.4982,  -0.4219,  -0.7499,  ...,  -0.9338,  -3.3310,  -1.6031]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ -2.4690,   4.0604,  -3.9459,  ...,   5.7969,  -0.2774,   3.0737],\n",
       "             [ -0.6838,   4.0008,  -0.6705,  ...,   1.9849,   1.1280,   7.5331],\n",
       "             [ -2.5174,   3.3245,  -1.3394,  ...,  -2.6141,  -3.4470,   4.3060],\n",
       "             ...,\n",
       "             [ -1.0193,   2.6181,   2.0511,  ...,  -1.7006,  -3.9424, -10.6050],\n",
       "             [ -2.2497,  -0.5144,   1.7345,  ...,  -1.1002,  -2.4441,  -7.6817],\n",
       "             [ -0.3935,  -1.3842,  -0.8768,  ...,  -2.9636,  -1.2004,  -5.7717]],\n",
       "   \n",
       "            [[  3.8587,  -2.1556,  -4.1919,  ...,  -4.5504,  -5.7377,   3.0730],\n",
       "             [ -0.0837,  -6.8324,  -5.7636,  ...,   0.8449,  -4.6280,  -4.7393],\n",
       "             [ -3.7105,   2.4849,  -3.7374,  ...,  -1.7409,   2.5950,  -3.9738],\n",
       "             ...,\n",
       "             [  7.1256,   4.7021,  -1.9384,  ...,   4.8798,  -2.6893, -11.7919],\n",
       "             [  6.5552,   4.2566,  -6.1213,  ...,   6.2858,  -1.9991, -12.1128],\n",
       "             [  9.2818,   6.6548,  -7.2664,  ...,   4.4705,   1.9984,  -9.9099]],\n",
       "   \n",
       "            [[  4.8155, -11.0692,  -6.3165,  ...,   0.0823,   2.3802,  -1.3145],\n",
       "             [-10.9064,   1.8681,  -0.8382,  ...,   6.1433, -10.7551,   3.5097],\n",
       "             [ -5.4489,   6.8231,   3.6104,  ...,  -7.9999,  -0.0251,   1.5425],\n",
       "             ...,\n",
       "             [  4.4228, -10.6443,  -4.9426,  ...,   1.6183,   2.3174,  -8.8958],\n",
       "             [  7.6620,  -6.9232,   2.9723,  ...,   5.2966,  -3.3803,  -6.6511],\n",
       "             [  2.2334,  -9.0225,   0.0501,  ...,   1.9592,  -0.0181,  -8.7348]]]],\n",
       "          grad_fn=<TransposeBackward0>)),\n",
       "  (tensor([[[[-1.6400e-01,  6.5666e-01, -1.5838e+00,  ..., -1.0361e+00,\n",
       "              -3.0485e-02,  3.4322e-01],\n",
       "             [-2.2163e+00, -1.5893e+00, -6.2293e-01,  ..., -1.3304e+00,\n",
       "               3.3767e-01, -1.4698e+00],\n",
       "             [-2.5588e+00,  4.7236e-01, -1.7529e+00,  ..., -7.1641e-01,\n",
       "              -5.2924e-01, -7.9926e-02],\n",
       "             ...,\n",
       "             [-2.6710e-01,  7.3170e-01, -1.6829e+00,  ..., -1.0269e+00,\n",
       "              -3.1642e-02,  2.1595e-01],\n",
       "             [-2.1141e-01,  6.8405e-01, -1.6886e+00,  ..., -1.0827e+00,\n",
       "              -1.3754e-01,  2.9919e-01],\n",
       "             [-2.0550e-01,  5.8679e-01, -1.6201e+00,  ..., -1.1481e+00,\n",
       "              -1.9157e-01,  1.1932e-01]],\n",
       "   \n",
       "            [[-2.8736e+00, -5.6610e-01,  1.2968e+00,  ..., -3.1666e-03,\n",
       "               1.0818e+00,  1.5947e+00],\n",
       "             [-1.9591e+00,  8.7572e-01,  4.2651e-01,  ..., -1.1002e+00,\n",
       "               8.0385e-02,  3.5003e+00],\n",
       "             [-3.9958e+00,  1.0148e+00, -1.1956e+00,  ...,  1.4985e-01,\n",
       "               2.4091e+00,  2.2418e+00],\n",
       "             ...,\n",
       "             [-2.9512e+00, -5.9915e-01,  1.4627e+00,  ..., -2.1617e-02,\n",
       "               1.2003e+00,  1.7074e+00],\n",
       "             [-2.8465e+00, -5.6270e-01,  1.3665e+00,  ..., -9.5921e-02,\n",
       "               1.1422e+00,  1.6695e+00],\n",
       "             [-2.9387e+00, -5.1218e-01,  1.4814e+00,  ..., -1.0966e-01,\n",
       "               9.0859e-01,  1.6852e+00]],\n",
       "   \n",
       "            [[ 2.3145e-01,  1.6760e+00, -1.8123e+00,  ..., -2.7461e-01,\n",
       "              -1.6018e+00, -5.6823e-01],\n",
       "             [ 1.3670e+00,  3.4024e+00, -6.2335e-01,  ..., -8.5479e-01,\n",
       "              -1.0258e+00, -3.0723e+00],\n",
       "             [-6.1684e-02,  2.7161e-01, -2.9036e+00,  ..., -9.6581e-01,\n",
       "              -7.9687e-01, -2.4732e+00],\n",
       "             ...,\n",
       "             [ 3.5901e-01,  1.6656e+00, -1.8796e+00,  ..., -3.1835e-01,\n",
       "              -1.6449e+00, -5.1893e-01],\n",
       "             [ 3.8241e-01,  1.6980e+00, -1.8116e+00,  ..., -3.3745e-01,\n",
       "              -1.5704e+00, -4.3595e-01],\n",
       "             [ 4.8755e-01,  1.7852e+00, -1.8041e+00,  ..., -4.6245e-01,\n",
       "              -1.8031e+00, -7.0927e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 1.2259e+00,  6.9687e-01, -3.2250e-01,  ...,  4.1941e-01,\n",
       "              -1.5442e-01, -2.3461e+00],\n",
       "             [ 3.4072e-01,  8.0522e-02, -4.8649e-01,  ...,  1.3499e+00,\n",
       "              -1.8134e-01, -1.6202e+00],\n",
       "             [-1.3765e-01,  4.3726e-01, -5.7689e-01,  ...,  1.5077e+00,\n",
       "               1.4312e+00, -3.1186e+00],\n",
       "             ...,\n",
       "             [ 1.2654e+00,  6.2929e-01, -3.5240e-01,  ...,  3.8324e-01,\n",
       "              -1.5012e-01, -2.3950e+00],\n",
       "             [ 1.2106e+00,  7.8787e-01, -4.1570e-01,  ...,  3.2336e-01,\n",
       "              -9.2664e-02, -2.3142e+00],\n",
       "             [ 1.2199e+00,  7.1603e-01, -3.5099e-01,  ...,  4.0846e-01,\n",
       "              -7.2422e-02, -2.3475e+00]],\n",
       "   \n",
       "            [[-1.6721e-01,  7.1132e-01,  6.1935e-01,  ..., -4.6811e-01,\n",
       "               1.4947e-01, -1.2110e+00],\n",
       "             [-3.1457e-01, -4.9010e-01,  1.2890e+00,  ..., -1.8865e+00,\n",
       "               6.1806e-01, -3.2031e+00],\n",
       "             [ 2.7679e+00, -1.4806e+00, -1.2304e-01,  ..., -2.0216e+00,\n",
       "              -1.3118e+00, -2.8566e+00],\n",
       "             ...,\n",
       "             [-1.5703e-01,  7.7038e-01,  6.3713e-01,  ..., -5.2705e-01,\n",
       "               1.1239e-01, -1.2855e+00],\n",
       "             [-1.0505e-01,  7.2803e-01,  6.7686e-01,  ..., -4.6366e-01,\n",
       "               8.5516e-02, -1.1213e+00],\n",
       "             [-4.7842e-02,  6.8785e-01,  7.8008e-01,  ..., -5.4766e-01,\n",
       "               1.0612e-01, -1.2175e+00]],\n",
       "   \n",
       "            [[-3.4885e-01,  1.1131e+00, -7.0739e-01,  ...,  3.2845e-01,\n",
       "              -6.3536e-01,  6.8813e-01],\n",
       "             [-1.6208e+00,  2.8150e-01, -2.8209e-01,  ..., -5.6545e-01,\n",
       "              -1.0176e+00, -8.7033e-01],\n",
       "             [-5.6475e-01,  2.2570e+00, -9.9804e-01,  ...,  1.4136e+00,\n",
       "              -6.1798e-02,  3.6261e-01],\n",
       "             ...,\n",
       "             [-3.2562e-01,  1.0518e+00, -6.7396e-01,  ...,  4.0366e-01,\n",
       "              -6.2865e-01,  5.2161e-01],\n",
       "             [-4.5699e-01,  1.0584e+00, -6.2897e-01,  ...,  2.6635e-01,\n",
       "              -5.6587e-01,  5.4592e-01],\n",
       "             [-4.3807e-01,  1.1461e+00, -7.3640e-01,  ...,  3.1215e-01,\n",
       "              -6.6340e-01,  4.4860e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 1.4601e-01,  3.4580e-01,  2.3280e-02,  ...,  1.6696e-01,\n",
       "               5.4597e-02,  2.4231e-01],\n",
       "             [-1.2081e+00, -3.4210e+00,  1.9226e+00,  ..., -1.1000e+00,\n",
       "              -4.2166e-01,  1.1706e+00],\n",
       "             [-1.3822e-01, -2.1185e+00,  1.3348e+00,  ..., -1.5432e+00,\n",
       "              -6.1962e-01,  1.2576e+00],\n",
       "             ...,\n",
       "             [ 1.9201e-02,  1.3412e-01,  5.9581e-02,  ...,  1.3689e-01,\n",
       "               5.0061e-02,  3.1464e-01],\n",
       "             [ 3.6419e-02,  2.4128e-01,  9.7246e-03,  ...,  2.6875e-01,\n",
       "               7.6372e-02,  3.1104e-01],\n",
       "             [ 1.8137e-01,  1.4905e-01,  1.8142e-01,  ...,  1.5113e-02,\n",
       "               1.0787e-01,  3.0314e-01]],\n",
       "   \n",
       "            [[-1.2941e-01, -2.8501e-01, -4.0101e-01,  ..., -2.3004e-01,\n",
       "              -2.8999e-01,  3.4158e-01],\n",
       "             [-3.7020e-01,  9.0673e-01, -8.8526e-01,  ...,  8.1335e-01,\n",
       "               4.2215e-01,  2.3952e+00],\n",
       "             [ 2.2975e+00, -4.2152e-01, -1.7539e+00,  ..., -3.1088e+00,\n",
       "               5.4418e+00,  4.1975e+00],\n",
       "             ...,\n",
       "             [ 8.3001e-02, -2.6543e-01, -3.0513e-01,  ..., -2.5908e-01,\n",
       "              -4.0690e-01,  1.9736e-01],\n",
       "             [-5.5282e-03, -1.4616e-01, -3.9725e-01,  ..., -2.3838e-01,\n",
       "              -3.5154e-01,  1.6558e-01],\n",
       "             [ 1.1354e-01, -2.1989e-01, -2.6072e-01,  ..., -1.0622e-01,\n",
       "              -2.5837e-01,  3.6714e-01]],\n",
       "   \n",
       "            [[-2.0226e-01, -7.6443e-02, -3.0462e-01,  ...,  1.9727e-01,\n",
       "              -1.4712e-01,  2.0343e-01],\n",
       "             [-6.1878e-01, -1.1987e+00, -6.2469e-01,  ...,  7.2678e-01,\n",
       "              -1.4973e+00,  1.5872e+00],\n",
       "             [-1.2747e+00, -3.4829e-01, -1.5747e+00,  ..., -1.9466e+00,\n",
       "               3.6876e+00,  3.9303e+00],\n",
       "             ...,\n",
       "             [-1.5589e-01, -2.1872e-02, -3.6252e-01,  ...,  8.2592e-02,\n",
       "              -3.1166e-01,  2.0226e-01],\n",
       "             [-3.0008e-01, -7.3696e-03, -1.5415e-01,  ...,  1.9818e-01,\n",
       "              -3.4104e-01,  3.0363e-01],\n",
       "             [-3.0201e-01, -2.1828e-01, -4.7813e-01,  ...,  1.2332e-01,\n",
       "              -2.9287e-01,  1.6559e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 3.8354e-01, -5.7535e-01, -2.2652e-02,  ...,  1.8769e-01,\n",
       "               1.2046e-01,  6.5139e-01],\n",
       "             [-6.5761e-01, -3.9117e-01, -2.6193e+00,  ...,  1.1500e+00,\n",
       "               9.5004e-01,  1.5812e+00],\n",
       "             [ 1.3139e+00, -5.7678e-01, -2.7814e+00,  ...,  4.8177e+00,\n",
       "              -7.0263e-01,  5.9577e-01],\n",
       "             ...,\n",
       "             [ 3.5228e-01, -4.3296e-01,  8.1299e-02,  ...,  2.7287e-01,\n",
       "              -4.6221e-03,  5.6038e-01],\n",
       "             [ 5.0680e-01, -3.5663e-01,  2.5698e-01,  ...,  9.0122e-02,\n",
       "               9.3143e-02,  7.8565e-01],\n",
       "             [ 4.0303e-01, -4.7071e-01, -8.7098e-02,  ...,  2.2138e-01,\n",
       "               1.8582e-01,  5.2884e-01]],\n",
       "   \n",
       "            [[-4.0124e-01, -2.6347e-01, -2.4592e-01,  ...,  2.4396e-01,\n",
       "               5.8965e-01, -3.5334e-02],\n",
       "             [-7.4643e-02, -1.5749e+00,  1.2487e+00,  ...,  1.0061e+00,\n",
       "               3.2731e+00, -2.0658e+00],\n",
       "             [ 5.5576e+00,  5.2225e-01,  1.7380e+00,  ..., -6.6725e-01,\n",
       "               1.7692e+00, -5.3880e-02],\n",
       "             ...,\n",
       "             [-3.0522e-01, -3.3631e-01, -2.2559e-01,  ...,  5.2252e-02,\n",
       "               5.8956e-01, -2.0707e-01],\n",
       "             [-2.0249e-01, -5.3582e-01, -3.2820e-01,  ...,  1.8046e-01,\n",
       "               7.0252e-01, -3.3149e-01],\n",
       "             [-4.4807e-01, -4.1415e-01, -4.2986e-01,  ...,  2.7753e-01,\n",
       "               7.7544e-01, -2.4592e-01]],\n",
       "   \n",
       "            [[-2.1406e-01,  1.8829e-01,  4.6588e-01,  ..., -1.6512e-01,\n",
       "              -1.8041e-01,  1.7243e-01],\n",
       "             [ 1.9050e+00,  2.7282e+00, -4.9349e-01,  ...,  1.4798e-01,\n",
       "              -6.2097e-01,  3.8847e-01],\n",
       "             [ 2.6149e+00,  4.1091e+00, -8.0715e-01,  ...,  1.4201e-01,\n",
       "              -3.2256e+00,  7.3566e-01],\n",
       "             ...,\n",
       "             [-1.6070e-01,  2.4013e-01,  6.1875e-01,  ...,  2.5114e-02,\n",
       "              -2.3442e-01,  2.4213e-01],\n",
       "             [-2.3336e-01,  1.5100e-01,  6.5270e-01,  ..., -5.7483e-02,\n",
       "              -2.1605e-01,  2.6873e-01],\n",
       "             [-2.7973e-01,  2.7147e-01,  6.8875e-01,  ..., -3.5811e-02,\n",
       "              -1.9695e-01,  2.5326e-01]]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 2.9223, -5.6694, -2.1169,  ..., -5.6447, -2.8747,  2.4010],\n",
       "             [ 0.2307, -2.6034, -3.3399,  ..., -4.0607, -1.2102,  2.6255],\n",
       "             [ 0.7661, -4.4793, -3.0850,  ..., -3.2571, -0.1923, -0.3281],\n",
       "             ...,\n",
       "             [ 5.1677,  0.9281,  1.8503,  ..., -1.4926, -6.4922,  4.6088],\n",
       "             [ 3.8343,  1.2110,  1.1935,  ..., -2.4105, -4.2264,  2.6243],\n",
       "             [ 3.2999, -1.7880,  1.2383,  ..., -4.1921, -4.2138,  2.2608]],\n",
       "   \n",
       "            [[-4.0693, -0.2314, -1.1228,  ...,  0.0343, -2.0550, -2.5080],\n",
       "             [-4.4681, -1.3400,  4.4781,  ..., -2.5773, -1.5890, -4.6557],\n",
       "             [-4.0104, -4.0349,  3.5443,  ..., -5.1023, -0.2360, -1.3455],\n",
       "             ...,\n",
       "             [ 0.1854, -6.5006,  1.0334,  ..., -0.1517, -0.3499,  1.3214],\n",
       "             [ 0.2057, -2.5597,  0.2017,  ...,  1.0933, -0.1444,  3.1338],\n",
       "             [ 0.1907, -5.4028,  0.5412,  ..., -0.1556,  0.4408,  1.3384]],\n",
       "   \n",
       "            [[-0.0209, -0.1814, -0.0394,  ...,  2.3912,  0.4967,  3.8342],\n",
       "             [ 3.1852, -1.7483,  4.8654,  ...,  4.8371,  5.9485, -0.2427],\n",
       "             [ 2.5166, -1.0093,  3.2102,  ...,  5.4019,  2.1515,  1.1291],\n",
       "             ...,\n",
       "             [ 2.6752, -3.8356,  3.8361,  ..., -3.9559, -0.6187,  1.5541],\n",
       "             [-0.0838, -2.5109,  1.2013,  ..., -3.2695, -0.5849,  1.1847],\n",
       "             [ 2.7804, -3.4214,  0.4939,  ..., -2.8612, -1.7289,  4.2670]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-3.2024, -0.2719,  1.2337,  ...,  2.6254, -0.2592,  3.1658],\n",
       "             [-0.6581,  3.3053,  3.4448,  ...,  1.3502, -2.2566,  3.6217],\n",
       "             [-1.6971,  2.5622,  1.7519,  ...,  1.2261, -0.9227,  2.7432],\n",
       "             ...,\n",
       "             [-0.2534, -1.3757, -2.0492,  ..., -0.1846,  3.1594,  3.9356],\n",
       "             [-1.2584, -2.2396, -1.8476,  ..., -0.9171,  3.3377,  1.5316],\n",
       "             [ 1.2282, -2.0989, -0.8904,  ...,  0.2895,  4.2131,  4.1074]],\n",
       "   \n",
       "            [[ 1.8549,  1.4236,  3.4602,  ...,  3.3063, -2.2640, -1.7523],\n",
       "             [ 0.3199,  0.7545,  3.0395,  ...,  0.2563, -1.4604,  0.8999],\n",
       "             [ 1.0803,  2.2198,  2.2592,  ..., -1.3920,  0.2183,  0.1961],\n",
       "             ...,\n",
       "             [-1.2067,  0.4270, -0.2962,  ...,  0.5715, -0.1181,  0.9237],\n",
       "             [ 1.0451, -0.7440,  2.2116,  ...,  0.1665, -0.7286, -0.6347],\n",
       "             [ 0.4317,  1.3914,  1.5826,  ..., -1.0654,  0.2254,  0.4731]],\n",
       "   \n",
       "            [[ 4.4542, -1.2366, -2.8277,  ...,  0.8736,  1.0566, -1.1665],\n",
       "             [-0.6482, -0.3536, -3.4285,  ...,  4.1378,  3.3748,  1.1291],\n",
       "             [ 0.5808, -2.6167, -2.2745,  ...,  1.0437,  1.4027,  1.1540],\n",
       "             ...,\n",
       "             [ 2.7895,  0.8714,  4.2399,  ..., -2.5457,  3.8553, -6.2398],\n",
       "             [ 2.2728,  1.5980,  4.5937,  ..., -1.7146,  3.6720, -3.5930],\n",
       "             [ 2.4086, -0.7858,  3.5062,  ..., -1.3338,  3.4588, -3.4644]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 6.5894e+00, -2.1288e+00,  3.8056e+00,  ...,  5.7812e-01,\n",
       "              -1.7100e+01, -6.8940e+00],\n",
       "             [ 2.5681e+00, -6.5524e+00, -3.5990e+00,  ...,  6.0454e+00,\n",
       "              -2.5663e+00, -3.6262e+00],\n",
       "             [ 2.0555e+00, -8.0969e+00, -7.2010e+00,  ..., -3.1098e+00,\n",
       "              -1.3323e-01, -2.2022e+00],\n",
       "             ...,\n",
       "             [-1.5732e+01,  5.3045e+00, -1.4008e+00,  ...,  8.4869e-01,\n",
       "               1.2756e+01,  3.6436e+00],\n",
       "             [-1.4366e+01,  1.6209e+00, -2.0114e+00,  ...,  2.7152e+00,\n",
       "               1.5923e+01,  2.5306e-01],\n",
       "             [-7.1629e+00,  2.1374e+00, -1.7910e+00,  ...,  7.3124e+00,\n",
       "               1.2843e+01,  1.5463e+00]],\n",
       "   \n",
       "            [[-8.3319e-01, -1.0251e+01, -1.6641e+01,  ..., -2.5671e+00,\n",
       "               1.2975e+01,  1.1590e+01],\n",
       "             [ 8.0065e+00,  6.7549e+00, -7.6674e+00,  ..., -2.2259e+00,\n",
       "               4.0082e+00, -6.9212e+00],\n",
       "             [ 8.6451e-01,  6.4862e+00, -1.3983e+00,  ...,  4.8838e+00,\n",
       "               4.7834e-02, -5.5663e+00],\n",
       "             ...,\n",
       "             [-1.2815e+01, -1.0399e+00, -6.3280e+00,  ...,  9.0805e-01,\n",
       "              -1.1360e+00, -9.6968e+00],\n",
       "             [-9.0293e+00,  7.4058e+00, -5.5668e-02,  ...,  5.3019e+00,\n",
       "              -4.9602e+00, -6.6669e+00],\n",
       "             [-6.5891e+00,  7.5951e-01, -5.8040e+00,  ...,  8.7694e+00,\n",
       "               2.6166e+00, -2.5941e+00]],\n",
       "   \n",
       "            [[-1.0513e+01, -3.1837e+00, -1.5043e+01,  ..., -2.5751e+00,\n",
       "              -2.4397e+01, -1.5255e+01],\n",
       "             [ 1.0857e+00, -9.7079e+00, -2.4945e+00,  ..., -2.2482e+00,\n",
       "               4.5389e+00, -1.3427e-01],\n",
       "             [ 4.4168e+00, -3.3738e+00, -2.0288e+00,  ...,  4.6802e+00,\n",
       "               6.6734e+00,  1.3889e+00],\n",
       "             ...,\n",
       "             [-2.1767e+01, -1.0358e+01, -1.4139e+01,  ...,  6.7522e+00,\n",
       "              -1.5042e+01,  7.2219e+00],\n",
       "             [-2.2552e+01, -2.0248e+01, -2.1843e+01,  ...,  1.6795e+00,\n",
       "              -1.5588e+01,  3.5082e+00],\n",
       "             [-2.1749e+01, -1.4812e+01, -1.8217e+01,  ...,  5.7144e+00,\n",
       "              -1.8786e+01,  5.4425e-01]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-1.0623e+01,  1.4828e+01,  3.7643e+00,  ...,  3.7605e+00,\n",
       "               4.0668e+00, -1.3248e+01],\n",
       "             [-4.4628e+00,  7.2021e-01,  3.4698e+00,  ..., -5.5600e+00,\n",
       "               6.8334e+00,  1.7282e+00],\n",
       "             [ 1.4687e+00,  4.1842e+00, -1.1090e-01,  ..., -4.3788e+00,\n",
       "               2.9375e+00,  3.5206e+00],\n",
       "             ...,\n",
       "             [-4.6424e+00, -1.0522e+01,  2.1473e-01,  ..., -5.7160e+00,\n",
       "               2.6860e+00,  3.9823e+00],\n",
       "             [-2.6361e+00, -9.6440e+00, -9.7646e+00,  ..., -6.0927e+00,\n",
       "               1.7801e+00,  2.6996e+00],\n",
       "             [-5.1374e+00, -8.8139e+00, -2.1914e-01,  ..., -3.3155e+00,\n",
       "               4.3397e+00,  3.5689e+00]],\n",
       "   \n",
       "            [[-5.8205e+00,  7.9611e+00,  1.4084e+01,  ..., -2.5411e+00,\n",
       "               1.0737e+01,  7.3238e+00],\n",
       "             [-2.4825e+00, -1.3306e+01, -1.8073e+00,  ...,  7.7439e-01,\n",
       "               4.7043e+00, -3.1694e+00],\n",
       "             [-8.1803e-01,  1.0709e+00,  1.2727e-02,  ...,  2.9306e+00,\n",
       "               3.4648e+00,  7.0649e-01],\n",
       "             ...,\n",
       "             [-1.0196e+00, -3.9397e+00, -7.7943e+00,  ...,  6.9828e+00,\n",
       "              -2.9506e+00,  7.8501e+00],\n",
       "             [-1.0648e-01, -1.7302e+00, -1.3172e+01,  ...,  1.2417e+01,\n",
       "              -1.9863e+00,  9.7386e+00],\n",
       "             [ 6.6550e+00, -8.0285e+00, -1.2830e+01,  ...,  1.3991e+01,\n",
       "              -5.2420e-01,  1.6712e+01]],\n",
       "   \n",
       "            [[ 1.5845e-01,  5.4515e-01, -1.4956e+01,  ...,  5.7000e+00,\n",
       "              -6.0597e+00,  2.8130e-01],\n",
       "             [ 4.3665e+00,  3.0097e+00, -8.8270e-01,  ..., -1.7265e+00,\n",
       "               5.3743e+00,  1.0109e+01],\n",
       "             [-1.4217e+00, -4.3751e-01,  3.0886e+00,  ...,  5.5403e-01,\n",
       "               1.2128e+01,  3.8914e+00],\n",
       "             ...,\n",
       "             [ 6.0694e+00, -1.3528e+01,  6.5923e-01,  ...,  5.8640e+00,\n",
       "               4.8558e-01,  7.4059e+00],\n",
       "             [ 7.0831e+00, -1.0264e+01, -5.6105e-01,  ...,  2.8886e+00,\n",
       "              -1.2326e+00,  8.3654e+00],\n",
       "             [ 7.9001e-01, -8.5908e+00,  5.4031e+00,  ...,  3.5109e+00,\n",
       "               5.6239e+00,  5.2373e+00]]]], grad_fn=<TransposeBackward0>)),\n",
       "  (tensor([[[[-0.1384,  0.0809,  3.1002,  ...,  0.7471, -0.1827, -0.3273],\n",
       "             [-1.1893,  0.0260,  3.4928,  ...,  0.0554,  1.8797, -0.3945],\n",
       "             [ 0.1140,  0.1024,  3.4499,  ..., -0.4365,  0.6731, -0.7838],\n",
       "             ...,\n",
       "             [-0.0654,  0.2791,  3.0410,  ...,  0.7382, -0.2255, -0.1854],\n",
       "             [-0.1039,  0.2426,  3.2796,  ...,  0.5899, -0.4959, -0.2667],\n",
       "             [-0.4496,  0.1685,  3.1982,  ...,  0.6489, -0.4327, -0.3335]],\n",
       "   \n",
       "            [[ 0.0182,  1.0388,  2.0941,  ..., -2.2603,  1.8895, -1.7795],\n",
       "             [-0.4060,  1.1365,  3.5723,  ..., -0.9799, -1.1594, -0.3197],\n",
       "             [ 0.4328,  0.5096,  2.3626,  ..., -1.7513,  0.7388, -0.5608],\n",
       "             ...,\n",
       "             [-0.2327,  1.1525,  2.3131,  ..., -2.4350,  2.0244, -1.8146],\n",
       "             [-0.1769,  1.0869,  2.1640,  ..., -2.0570,  1.8747, -1.6650],\n",
       "             [-0.3677,  0.9535,  2.7006,  ..., -2.1834,  1.6287, -1.8440]],\n",
       "   \n",
       "            [[-1.1453, -0.7348, -0.0768,  ...,  0.7021,  0.6305,  1.4641],\n",
       "             [-2.6577, -1.9851, -1.6070,  ...,  1.3703,  1.4007,  2.6992],\n",
       "             [-1.1341, -1.8483, -0.9619,  ...,  0.6783, -1.1579,  1.3523],\n",
       "             ...,\n",
       "             [-1.3442, -0.7551, -0.1896,  ...,  0.9063,  0.6792,  1.4328],\n",
       "             [-1.2702, -0.7663, -0.2934,  ...,  0.9038,  0.5300,  1.4007],\n",
       "             [-1.2275, -0.8462, -0.4093,  ...,  0.7014,  0.5998,  1.5696]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 1.4496, -0.4272, -0.3602,  ..., -2.0114, -0.4857,  3.5806],\n",
       "             [-0.4086, -0.7740, -2.2111,  ..., -2.6880,  0.2049,  1.8009],\n",
       "             [ 1.5639,  0.6520, -0.9609,  ..., -0.4411,  0.5151,  1.9152],\n",
       "             ...,\n",
       "             [ 1.4062, -0.2967, -0.2206,  ..., -2.2670, -0.4527,  3.5852],\n",
       "             [ 1.4481, -0.3828, -0.2944,  ..., -1.9527, -0.3725,  3.4655],\n",
       "             [ 1.2678, -0.3987, -0.4038,  ..., -1.9124, -0.4929,  3.5363]],\n",
       "   \n",
       "            [[-1.0923,  0.7916, -0.3240,  ..., -0.7149,  1.7056,  0.5163],\n",
       "             [-0.8121,  3.0327, -1.1026,  ..., -1.7240,  1.0590,  0.1454],\n",
       "             [-0.7081,  2.9687,  0.3495,  ..., -1.2941,  2.8639,  1.3947],\n",
       "             ...,\n",
       "             [-1.1513,  0.9858, -0.3795,  ..., -0.8247,  1.7250,  0.5363],\n",
       "             [-1.0956,  0.6782, -0.3674,  ..., -0.6502,  1.8026,  0.6124],\n",
       "             [-1.2140,  0.8403, -0.5459,  ..., -0.6745,  1.6742,  0.4688]],\n",
       "   \n",
       "            [[ 0.8336,  1.1616,  0.7182,  ..., -0.2054,  4.0553, -0.9189],\n",
       "             [ 0.3738,  0.4164,  0.8083,  ...,  0.5208,  4.1543, -0.2732],\n",
       "             [ 0.5351, -1.4835,  0.2011,  ..., -0.4010,  4.1303, -2.2207],\n",
       "             ...,\n",
       "             [ 0.9475,  1.0566,  0.6151,  ..., -0.3399,  4.2345, -1.0835],\n",
       "             [ 0.8722,  1.2675,  0.7109,  ..., -0.2621,  3.8743, -0.9145],\n",
       "             [ 0.7870,  1.2535,  0.8243,  ..., -0.3422,  4.3503, -0.9402]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[-0.3000, -1.8262,  0.0736,  ...,  0.0358, -0.7914,  0.5559],\n",
       "             [ 0.1765,  3.1784,  2.6673,  ..., -0.4595,  1.6188,  1.6265],\n",
       "             [ 2.1295,  4.1345, -2.0271,  ..., -2.2417, -2.5149,  1.3359],\n",
       "             ...,\n",
       "             [-0.3972, -1.7060, -0.2112,  ...,  0.1884, -0.7245,  0.3221],\n",
       "             [-0.3794, -1.3312,  0.3018,  ...,  0.2932, -0.6960,  0.5630],\n",
       "             [-0.3626, -1.4646,  0.4083,  ...,  0.0667, -0.5455,  0.5593]],\n",
       "   \n",
       "            [[-0.5038,  0.7966,  0.7860,  ...,  0.6677,  0.4394,  0.9358],\n",
       "             [ 0.6553, -3.8671, -1.3654,  ...,  0.9028,  2.4349,  0.8399],\n",
       "             [-1.9928,  4.0982,  2.8438,  ..., -2.1053,  2.4561,  1.7856],\n",
       "             ...,\n",
       "             [-0.3980,  0.9502,  0.5444,  ...,  0.3694,  0.3635,  1.0182],\n",
       "             [-0.4328,  1.0016,  0.6551,  ...,  0.5986,  0.1670,  0.6648],\n",
       "             [-0.8840,  0.8371,  0.4194,  ...,  1.1613,  0.4634,  0.7122]],\n",
       "   \n",
       "            [[-0.4188, -0.0645, -0.7910,  ...,  0.1577, -0.1546, -0.1043],\n",
       "             [-0.9975, -2.3524,  0.8868,  ...,  1.1016, -1.3092, -5.2611],\n",
       "             [ 2.8167, -1.1461, -0.4511,  ..., -1.4965,  0.9473,  3.9821],\n",
       "             ...,\n",
       "             [-0.3393, -0.3202, -0.9504,  ...,  0.4181,  0.0383, -0.6443],\n",
       "             [-0.6115, -0.3281, -1.3357,  ...,  0.5670, -0.0425, -0.7786],\n",
       "             [-0.3440, -0.1777, -1.2566,  ...,  0.3904,  0.2029, -0.2733]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[-0.1143, -1.3847, -0.3814,  ..., -0.2662, -0.3546,  0.1914],\n",
       "             [-1.1209,  1.3782, -1.8425,  ...,  1.7218,  0.8893,  1.7012],\n",
       "             [ 1.1808, -0.5956, -1.6197,  ..., -0.7352,  1.8799, -3.5361],\n",
       "             ...,\n",
       "             [-0.8144, -1.6384, -0.7246,  ..., -0.5984, -0.3285, -0.0649],\n",
       "             [-0.4292, -1.8301, -0.6456,  ..., -0.6721, -0.3754,  0.1778],\n",
       "             [-0.7938, -1.5952, -0.4474,  ..., -0.5481, -0.2657,  0.5654]],\n",
       "   \n",
       "            [[-0.3176, -0.1167, -0.9383,  ..., -0.3647, -0.2721, -0.3342],\n",
       "             [ 1.0078, -0.0764, -1.3878,  ...,  2.9295,  0.3601, -2.9505],\n",
       "             [ 3.9596, -0.1209, 11.6135,  ...,  3.4538, -2.0249,  1.7559],\n",
       "             ...,\n",
       "             [-0.4276, -0.6948, -0.6832,  ..., -0.5393, -0.1441, -0.8635],\n",
       "             [-0.7679, -0.3960, -0.5936,  ..., -0.3355, -0.1339, -0.3514],\n",
       "             [-0.7149, -0.5820, -1.0039,  ..., -0.1291, -0.2584, -0.6884]],\n",
       "   \n",
       "            [[ 0.0372, -0.2887,  0.3993,  ...,  0.0180, -0.1486,  0.4296],\n",
       "             [-0.2439, -2.4717,  5.3268,  ...,  0.0367,  2.4449, -3.4414],\n",
       "             [-3.5254, -6.1396,  2.7548,  ...,  2.2576, -0.5345, -1.6894],\n",
       "             ...,\n",
       "             [-0.3801, -0.1194,  0.3734,  ...,  0.2422, -0.4338,  0.7962],\n",
       "             [-0.0356, -0.5100,  0.6978,  ..., -0.2573, -0.3553,  0.7794],\n",
       "             [ 0.2829, -0.6583,  0.5107,  ..., -0.1474, -0.2150,  0.7196]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 1.0761, -0.8348, -2.8321,  ...,  1.3922,  0.4452,  0.1857],\n",
       "             [ 1.4688,  1.3609,  1.8207,  ...,  0.5306,  0.7696,  2.0319],\n",
       "             [-0.2750,  0.5793, -0.2905,  ...,  1.0094, -0.0093,  1.5695],\n",
       "             ...,\n",
       "             [ 1.0433, -0.7985, -1.3401,  ...,  3.3607, -2.9744,  3.4981],\n",
       "             [ 0.7504, -0.0193, -0.9533,  ...,  4.2065, -2.7723,  1.7410],\n",
       "             [ 1.3208, -0.3205, -1.1897,  ...,  3.8017, -0.3697,  2.5143]],\n",
       "   \n",
       "            [[-0.8862, -2.3036,  0.1056,  ...,  0.1434, -3.1721, -2.6156],\n",
       "             [-0.6157, -0.6794, -0.7713,  ..., -4.3253,  0.8571,  0.3732],\n",
       "             [-3.8022, -2.4712, -0.0126,  ...,  1.7497,  1.1934,  1.1117],\n",
       "             ...,\n",
       "             [-2.1775,  0.3480, -3.1184,  ...,  1.7269,  0.8826, -2.4138],\n",
       "             [-1.4362,  2.9541, -2.9045,  ...,  0.5160,  0.8494, -2.6758],\n",
       "             [-2.3969,  2.0375, -3.4976,  ...,  2.5413,  1.2548, -1.2717]],\n",
       "   \n",
       "            [[ 3.0779,  1.4596,  1.9520,  ...,  0.0693, -0.0574, -4.9111],\n",
       "             [ 1.4222,  0.5491,  0.4859,  ...,  3.2786, -0.2854, -0.1097],\n",
       "             [-0.2573,  0.6074,  1.1365,  ...,  1.4728, -1.1050, -0.5701],\n",
       "             ...,\n",
       "             [ 3.8681, -2.2015, -0.9141,  ..., -2.3064, -1.2189, -1.8909],\n",
       "             [ 2.6490, -0.3967, -0.6094,  ..., -1.5763,  1.9554, -0.5444],\n",
       "             [ 4.1932, -3.6294, -0.0750,  ..., -1.7628, -0.8682, -0.5969]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 0.2078,  1.9425,  1.2521,  ..., -2.3027, -2.0259, -2.5779],\n",
       "             [ 0.4034,  1.3866,  5.4992,  ...,  7.2692, -0.1459,  1.4164],\n",
       "             [-2.0150,  1.6496,  4.9232,  ...,  7.3722, -1.3302,  0.3423],\n",
       "             ...,\n",
       "             [ 0.3526, -1.4112,  0.8806,  ..., -2.9793,  1.4688,  2.9553],\n",
       "             [ 0.0134, -0.6910,  1.3072,  ..., -1.4732, -0.2830,  1.2073],\n",
       "             [ 0.5183, -2.0205,  2.1793,  ..., -2.1473,  0.7518,  0.3600]],\n",
       "   \n",
       "            [[ 1.5763, -1.2392,  1.9847,  ...,  0.5336, -3.3192, -1.5992],\n",
       "             [ 2.0668,  1.9566, -2.7291,  ..., -4.0236, -3.8370, -2.5710],\n",
       "             [-1.2003, -4.6210,  1.2496,  ..., -1.6430, -0.9255,  0.6154],\n",
       "             ...,\n",
       "             [-1.8541,  0.8311, -0.8012,  ..., -1.4961,  1.8497, -1.8402],\n",
       "             [-1.6347,  1.1468, -0.0251,  ...,  0.3270,  2.0047, -1.2617],\n",
       "             [-1.9597,  0.6519,  0.3782,  ..., -0.6004,  0.3810, -0.2651]],\n",
       "   \n",
       "            [[ 1.0491, -0.1856, -2.1361,  ..., -0.2877,  1.6422, -0.4558],\n",
       "             [ 3.2766,  0.1768,  1.2043,  ...,  2.9108,  3.5277, -1.8957],\n",
       "             [ 3.5434, -3.4348,  1.0722,  ...,  3.0458,  0.9812, -2.2456],\n",
       "             ...,\n",
       "             [-1.6830, -4.3082, -1.3496,  ...,  0.3760,  1.2632, -3.2674],\n",
       "             [-3.4850, -4.2832,  0.1924,  ..., -0.1247,  1.6769, -2.0360],\n",
       "             [-1.5524, -4.5825, -1.0242,  ..., -0.2648,  2.3020, -1.5995]]]],\n",
       "          grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[[ 10.2758,  -6.0227,  -3.7566,  ...,  16.8231,   6.2127,  36.2331],\n",
       "             [  0.2527,   6.7233,  12.4168,  ...,   2.6064, -12.9351,  -1.0670],\n",
       "             [ -3.7938,   1.3464,  -6.7220,  ...,   1.8183,  -2.5041,   2.9367],\n",
       "             ...,\n",
       "             [ 12.7268, -12.0933,  10.1012,  ...,  13.3136, -10.4848,  -4.5692],\n",
       "             [ 14.0124,  -7.1298,  12.8594,  ...,  15.6779, -11.7851,  -5.6435],\n",
       "             [  9.3809,  -1.2739,   4.9259,  ...,  11.4231,  -9.1258,  -4.5833]],\n",
       "   \n",
       "            [[ 11.8582,  -2.6330,  -7.6997,  ...,   1.9623,  -1.8176,   8.6299],\n",
       "             [  2.9172,   9.5300,  -8.4868,  ...,  -4.2779,   6.4976,  -5.2285],\n",
       "             [  4.9217,   6.6497,   1.2958,  ...,  -5.7855,   2.5687,  -6.6802],\n",
       "             ...,\n",
       "             [ 15.1669,   4.7883,  -3.3445,  ...,   6.2231,  11.3536,  16.3441],\n",
       "             [ 13.1280,  -0.5107, -16.8850,  ...,   8.6650,  16.5291,  26.5447],\n",
       "             [ 16.9552,   8.4602,  -5.6392,  ...,   1.1987,  17.2390,  19.8479]],\n",
       "   \n",
       "            [[  4.7962,  -8.7443,   2.4126,  ..., -14.4755, -10.7013, -11.3311],\n",
       "             [ 10.1930, -10.2674,  16.2519,  ...,  -5.4598,  -2.1156,  -7.1524],\n",
       "             [  1.4183,   4.5986,  17.2683,  ...,   8.5286,  -0.0477,  -0.5807],\n",
       "             ...,\n",
       "             [ 18.7097, -13.0983,   6.5010,  ...,  16.2948,  -7.7817,  14.9912],\n",
       "             [ 22.1152,  -8.7624,  -0.7928,  ...,   7.9594,  -3.0566,   8.9225],\n",
       "             [ 12.5766, -13.5724,   9.3526,  ...,  14.8752,  -6.6151,  16.5777]],\n",
       "   \n",
       "            ...,\n",
       "   \n",
       "            [[ 19.1705,  -1.6837,  -2.8860,  ...,   3.0343,  -9.5407,  -9.9801],\n",
       "             [ -6.4651,   3.6811,  -0.9160,  ...,  -2.4389,   2.9236,   5.5423],\n",
       "             [  4.6446,   7.1772,  -0.5542,  ...,  -2.6004,   4.8160,   5.8746],\n",
       "             ...,\n",
       "             [ 13.6600,   4.5538,   6.3332,  ...,  15.1743,   1.0730,   7.7555],\n",
       "             [  6.3862,   2.3509,   8.0057,  ...,  16.4068,  -1.8491,   8.6763],\n",
       "             [ 11.7561,   5.1029,  19.2334,  ...,  12.5956,  -5.3588,  10.6335]],\n",
       "   \n",
       "            [[ -6.2437,   5.2641,   2.9213,  ...,   3.5202,   4.3026,   8.8426],\n",
       "             [ -3.8239,   3.2539,   9.8155,  ...,   1.1220,   8.6713,   0.8628],\n",
       "             [ -0.5308,   7.7222,   4.0058,  ...,   7.3230,  -2.6521,   0.6047],\n",
       "             ...,\n",
       "             [-13.5411,   3.7871,  24.1329,  ...,  10.5860,  13.3739,  14.3581],\n",
       "             [-20.8569,  -5.3506,  19.0028,  ...,  16.3538,  12.2150,  10.7692],\n",
       "             [ -8.5066,  -1.5885,  23.9696,  ...,   2.9689,  11.0724,  21.8500]],\n",
       "   \n",
       "            [[  2.7661,   3.0760,   0.2220,  ...,   2.3300,  -4.1383,  12.5955],\n",
       "             [  7.5547,  -5.8032,   7.3931,  ...,   4.0617,  -6.9647,   4.3992],\n",
       "             [  3.3541,  -9.2430,  -2.4391,  ...,  -4.6708, -10.3813,  -9.4275],\n",
       "             ...,\n",
       "             [ 17.5349, -13.5201, -14.4552,  ..., -21.4975,   8.8157,  -5.1794],\n",
       "             [ 16.4522, -11.7991,  -6.8680,  ..., -20.5138,  11.2796,  -2.9325],\n",
       "             [ 14.4086, -20.0652, -10.0470,  ..., -19.7032,   5.1843,  -2.5498]]]],\n",
       "          grad_fn=<TransposeBackward0>))),\n",
       " tensor([[[ 0.2572,  0.4853,  0.2389,  ...,  0.0581, -0.0512, -0.0000],\n",
       "          [-0.2560,  0.3905,  0.0427,  ..., -0.1574,  0.0274,  0.1101],\n",
       "          [-0.0846,  0.1934,  0.1771,  ...,  0.0000, -0.2929,  0.1497],\n",
       "          ...,\n",
       "          [-0.0463,  0.0000,  0.3912,  ..., -0.0000, -0.0035, -0.0375],\n",
       "          [ 0.0412, -0.0389,  0.4117,  ..., -0.0147, -0.1495,  0.0671],\n",
       "          [ 0.0889,  0.2347,  0.3695,  ..., -0.2024,  0.1623, -0.1613]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_batch[\"input_ids\"].shape\n",
    "#test_batch[\"attention_mask\"]\n",
    "test_batch[\"return_dict\"] = False\n",
    "model(**test_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
