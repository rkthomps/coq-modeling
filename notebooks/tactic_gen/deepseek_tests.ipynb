{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_brun_umass_edu/kthompson/coq-modeling/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tactic_gen.tactic_data import TEST_LM_EXAMPLE, example_collator_conf_from_yaml, example_collator_from_conf, ExampleCollator, get_tokenizer\n",
    "from tactic_gen.train_decoder import get_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from util.constants import TRAINING_CONF_NAME\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(os.curdir).resolve().name == \"tactic_gen\":\n",
    "    os.chdir(\"../..\")\n",
    "elif Path(os.curdir).resolve().name == \"coq-modeling\": \n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(f\"In an unexpected directory: {os.curdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_conf(checkpoint_loc: Path) -> Any:\n",
    "    training_conf_loc = checkpoint_loc.parent / TRAINING_CONF_NAME\n",
    "    with training_conf_loc.open('r') as f:\n",
    "        training_conf = yaml.safe_load(f)\n",
    "    return training_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_collator(checkpoint_loc: Path) -> ExampleCollator:\n",
    "    training_conf = get_training_conf(checkpoint_loc)\n",
    "    example_collator_conf = example_collator_conf_from_yaml(training_conf['example_collator'])\n",
    "    example_collator = example_collator_from_conf(example_collator_conf) \n",
    "    return example_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_LOC = Path(\"models/deepseek-bm25-proof-tfidf-proj-thm-prem-final/checkpoint-54500\")\n",
    "training_conf = get_training_conf(CHECKPOINT_LOC)\n",
    "example_collator = get_example_collator(CHECKPOINT_LOC)\n",
    "tokenizer = get_tokenizer(training_conf[\"model_name\"], add_eos=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(str(CHECKPOINT_LOC))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "BEAM = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from tactic_gen.tactic_data import ProofPremiseCollator, NEWLINE_RESPONSE_TEMPLATE \n",
    "class TokenMask(Enum):\n",
    "    STATE = 0\n",
    "    SCRIPT = 1\n",
    "    PROOF = 2\n",
    "    PREMISE = 3\n",
    "\n",
    "\n",
    "def find_id_start_idx(t: torch.Tensor, s: torch.Tensor) -> Optional[int]:\n",
    "    for i in range(t.shape[0] - s.shape[0] + 1):\n",
    "        if torch.all(t[i : i + s.shape[0]] == s):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def transform_attention_mask(\n",
    "    collator: ExampleCollator,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    token_mask: Optional[TokenMask],\n",
    "    input_ids: torch.Tensor,\n",
    "    attn_mask: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    assert isinstance(collator, ProofPremiseCollator)\n",
    "    match token_mask:\n",
    "        case None:\n",
    "            return attn_mask\n",
    "        case TokenMask.STATE:\n",
    "            start_ids = tokenizer.encode(collator.STATE_SEP, add_special_tokens=False)\n",
    "            end_ids = tokenizer.encode(collator.SCRIPT_SEP, add_special_tokens=False)\n",
    "        case TokenMask.SCRIPT:\n",
    "            start_ids = tokenizer.encode(collator.SCRIPT_SEP, add_special_tokens=False)\n",
    "            end_ids = tokenizer.encode(\n",
    "                NEWLINE_RESPONSE_TEMPLATE, add_special_tokens=False\n",
    "            )\n",
    "        case TokenMask.PROOF:\n",
    "            start_ids = tokenizer.encode(collator.PROOF_SEP, add_special_tokens=False)\n",
    "            end_ids = tokenizer.encode(collator.STATE_SEP, add_special_tokens=False)\n",
    "        case TokenMask.PREMISE:\n",
    "            start_ids = tokenizer.encode(collator.PREMISE_SEP, add_special_tokens=False)\n",
    "            end_ids = tokenizer.encode(collator.PROOF_SEP, add_special_tokens=False)\n",
    "\n",
    "    changed_mask = attn_mask.clone()\n",
    "    for i, id_row in enumerate(input_ids):\n",
    "        start_idx = find_id_start_idx(id_row, torch.tensor(start_ids))\n",
    "        end_idx = find_id_start_idx(id_row, torch.tensor(end_ids))\n",
    "        changed_mask[i, start_idx:end_idx] = 0\n",
    "    return changed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[185, 58, 11787, 9572, 1871, 50, 60, 185]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise_seq = [tokenizer.encode(\"\\n[PREMISES]\\n\", add_special_tokens=False)]\n",
    "premise_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_brun_umass_edu/kthompson/coq-modeling/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:579: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_brun_umass_edu/kthompson/coq-modeling/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "collated_input = example_collator.collate_input(tokenizer, TEST_LM_EXAMPLE)\n",
    "inputs = tokenizer(collated_input, return_tensors='pt')\n",
    "attention_mask = transform_attention_mask(example_collator, tokenizer, TokenMask.PREMISE, inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=64, \n",
    "        do_sample=not BEAM, \n",
    "        temperature=None if BEAM else 1.0,\n",
    "        num_beams=N if BEAM else None,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        num_return_sequences=2, \n",
    "        attention_mask=attention_mask,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 504])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention_mask.T @ attention_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out = out.sequences[:, inputs[\"input_ids\"].shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n  simpl.', '\\n  induction l.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_num_tokens = inputs[\"input_ids\"].shape[1]\n",
    "tokenizer.batch_decode(out.sequences[:, input_num_tokens:], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_deployment.model_wrapper import DecoderLocalWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_LOC = Path(\"models/deepseek-1.3b-basic/checkpoint-48000\")\n",
    "model_wrapper = DecoderLocalWrapper.from_checkpoint(CHECKPOINT_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_brun_umass_edu/kthompson/coq-modeling/venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated:  \n",
      "[STATE]\n",
      "x: X\n",
      "l: list X\n",
      "\n",
      "rev l ++ [x] = rev (x :: l)\n",
      "[SCRIPT]\n",
      "Theorem rev_app : forall x l, rev l ++ [x] = rev (x::l).\n",
      "Proof.\n",
      "  intros.\n",
      "[TACTIC]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_brun_umass_edu/kthompson/coq-modeling/venv/lib/python3.12/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = model_wrapper.get_recs(TEST_LM_EXAMPLE, 64, \"\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n  induction l.',\n",
       " ' induction l.',\n",
       " ' induction l.',\n",
       " ' induction l as [|h t IH].',\n",
       " ' induction l.',\n",
       " \"\\n  induction l as [ | y0 l' IHl'].\",\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " ' induction l as [|h t].',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l as [|h t].',\n",
       " \"\\n  induction l as [|h t I'].\",\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l as [|m l IHl].',\n",
       " ' induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  rewrite rev_app_split.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l; simpl.',\n",
       " ' induction l.',\n",
       " ' induction l.',\n",
       " ' induction l.',\n",
       " ' unfold rev.',\n",
       " ' induction l.',\n",
       " '\\n  revert x.',\n",
       " '\\n  induction l; auto.',\n",
       " ' induction l.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' \\n  rewrite <- rev_involutive.',\n",
       " ' symmetry.',\n",
       " ' simpl.',\n",
       " ' induction l; simpl; auto.',\n",
       " '\\n  induction l.',\n",
       " ' simpl.',\n",
       " \" induction l as [| n l' IH].\",\n",
       " '\\n  induction l.',\n",
       " '\\n  elim l; intros.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " ' rewrite rev_involutive.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.',\n",
       " '\\n  induction l.',\n",
       " '\\n  induction l.',\n",
       " ' simpl.',\n",
       " ' induction l as [|y ys IH].',\n",
       " ' induction l.',\n",
       " ' simpl.',\n",
       " '\\n  induction l.',\n",
       " ' induction l.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.next_tactic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.091173529624939,\n",
       " -1.0829819440841675,\n",
       " -0.9765117168426514,\n",
       " -5.801501274108887,\n",
       " -1.3765190839767456,\n",
       " -13.580381393432617,\n",
       " -1.2010478973388672,\n",
       " -0.7581014633178711,\n",
       " -0.9752275943756104,\n",
       " -1.3491300344467163,\n",
       " -6.818093299865723,\n",
       " -0.9610227346420288,\n",
       " -1.306260347366333,\n",
       " -0.969602108001709,\n",
       " -7.681689739227295,\n",
       " -6.0971221923828125,\n",
       " -9.352446556091309,\n",
       " -0.9166039228439331,\n",
       " -0.9867836236953735,\n",
       " -10.54260540008545,\n",
       " -1.3574259281158447,\n",
       " -1.7184215784072876,\n",
       " -0.9940738677978516,\n",
       " -11.772241592407227,\n",
       " -1.07068932056427,\n",
       " -1.2064350843429565,\n",
       " -1.4584046602249146,\n",
       " -1.0888279676437378,\n",
       " -3.8191006183624268,\n",
       " -1.2318209409713745,\n",
       " -1.3284974098205566,\n",
       " -1.3770112991333008,\n",
       " -4.765250205993652,\n",
       " -1.0577272176742554,\n",
       " -8.313982009887695,\n",
       " -6.794739246368408,\n",
       " -1.26614511013031,\n",
       " -0.9316457509994507,\n",
       " -1.3570245504379272,\n",
       " -0.9584465026855469,\n",
       " -1.2043745517730713,\n",
       " -10.348194122314453,\n",
       " -7.571715354919434,\n",
       " -10.32215690612793,\n",
       " -5.461085796356201,\n",
       " -0.9433013796806335,\n",
       " -3.6149213314056396,\n",
       " -8.61722183227539,\n",
       " -0.9480211734771729,\n",
       " -10.675844192504883,\n",
       " -0.9374567866325378,\n",
       " -1.2903416156768799,\n",
       " -6.581651210784912,\n",
       " -0.9620234966278076,\n",
       " -1.1556146144866943,\n",
       " -1.3003675937652588,\n",
       " -0.9509530067443848,\n",
       " -1.3029683828353882,\n",
       " -3.599736452102661,\n",
       " -9.22715950012207,\n",
       " -1.0112096071243286,\n",
       " -3.33099365234375,\n",
       " -0.9744158983230591,\n",
       " -1.3073242902755737]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 4,\n",
       " 16,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 11,\n",
       " 12,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.num_tokens_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
